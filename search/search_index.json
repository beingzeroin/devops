{
    "docs": [
        {
            "location": "/",
            "text": "DevOps\n\n\nA collection of Docker and Vagrant images, scripts and documentation mainly related to distributed systems for local development, learning purposes and quick prototyping.\n\n\n\n\nLinux\n\n\nDocker\n\n\nAnsible\n\n\nCassandra\n\n\nZookeeper\n\n\nKafka\n\n\nHadoop\n\n\nKubernetes\n\n\nReadings\n\n\nOther",
            "title": "Home"
        },
        {
            "location": "/#devops",
            "text": "A collection of Docker and Vagrant images, scripts and documentation mainly related to distributed systems for local development, learning purposes and quick prototyping.   Linux  Docker  Ansible  Cassandra  Zookeeper  Kafka  Hadoop  Kubernetes  Readings  Other",
            "title": "DevOps"
        },
        {
            "location": "/linux/",
            "text": "Linux\n\n\nUseful commands\n\n\n# create nested directories\nmkdir -p parent/child1/child2 && cd $_\n\n# scroll file from bottom\nless +G /var/log/auth.log\n# follow also if doesn't exist\ntail -F /var/log/auth.log\n\n# find files\nfind /etc -name '*shadow'\n\n# prints lines that match regexp\n# -i case insensitive\n# -v inverts the search\n# -c count lines\ngrep -E '^root' /etc/passwd\n# password encryption\ngrep password.*unix /etc/pam.d/*\n\n# sed = stream editor\n# example substitution\necho -e \"a='1st' b='2nd' c='3rd'\\na='4th' b='5th' c='6th'\" > test.txt\ncat test.txt | sed -nE \"s/^.*a='([^']*).*c='([^']*).*$/\\2\\n\\1/gp\" | sort -r | uniq\n# delete lines three through six\nsed 3,6d /etc/passwd\n\n# pick a single field out of an input stream\nls -l | awk '{print $9}'\n# extract 2nd string\necho \"aaa bbb ccc\" > test.txt\ncat test.txt | awk '{printf(\"2nd: %s\\n\",$2)}'\n\n# pack archive\ntar cvf archive.tar file1 file2\n# table-of-content\ntar tvf archive.tar\n# unpack archive\ntar xvf archive.tar -C output\n# compress and pack archive\ntar zcvf archive.tar.gz /path/to/images/*.jpg\n# unpack compressed archive\ntar zxvf archive.tar.gz\n\n# pack archive\nzip -r backup.zip file-name directory-name\n# zip with password (prompt)\nzip -e backup.zip file-name\n# unpack jar\nunzip my-lib.jar -d /tmp/my-lib\n\n# count lines\nwc -l file\n\n# lowercase random uuid\nuuidgen | tr \"[:upper:]\" \"[:lower:]\"\n\n# number of bytes\nstat --printf=\"%s\" file\n\n# calculator\necho 1+2 | bc\n# print number in binary base 2 format\necho 'obase=2; 240' | bc\n# reverse-polish calculator\necho '1 2 + p' | dc\n# evaluate expressions\nexpr 1 + 2\n\n# unix timestamp\ndate +%s\n# timestamp in microsecond\ndate +%s%N\n\n# calendar\ncal -3\n\n# configure kernel parameters at runtime\nsysctl\n\n# test conditions ([)\ntest a = a && echo equal\n\n# create temporary file\nmktemp\n# X is a template\nmktemp /tmp/my-tmp.XXXXXX\n# signal handler to catch the signal that CTRL-C generates and remove the temporary files\nTMPFILE=$(mktemp /tmp/my-tmp.XXXXXX)\ntrap \"rm -f $TMPFILE; exit 1\" INT\n\n# compare files\ndiff FILE1 FILE2\n\n# here document\nDATE=$(date)\ncat <<EOF\nDate: $DATE\nline1\nline2\nEOF\n\n# strip full path and extension if specified e.g. mail\nbasename /var/log/mail.log .log\n\n# image conversion\ngiftopnm\npnmtopng\n\n# when operating on huge number of files to avoid buffer issues\n# e.g. verify file's type\n# INSECURE find . -name '*.md' -print | xargs file\n# change the find output separator and the xargs argument delimiter from a newline to a NULL character\n# two dashes if there is a chance that any of the target files start with a single dash\nfind . -name '*.md' -print0 | xargs -0 file --\n# supply a {} to substitute the filename and a literal ; to indicate the end of the command\nfind . -name '*.md' -exec file {} \\;\n\n# replaces current shell process with the program you name after exec system call\n# after you press CTRL-D or CTRL-C to terminate the cat program,\n# your window should disappear because its child process no longer exists\nexec cat\n\n# subshell example () e.g. path remains the same outside\n(PATH=/bad/invalid:$PATH; echo $PATH)\n# fast way to copy and preserve permissions\ntar cf - orig | (cd target; tar xvf -)\n\n# X Window System\nxwininfo\nxlsclients -l\nxev\nxinput --list\ndbus-monitor --system\ndbus-monitor --session\n\n# compile C program\ncc -o hello hello.c\n# list shared library (so)\nldd /bin/bash\n\n\n\n\nScript templates\n\n\n# shebang\n#!/bin/sh\n#!/bin/bash\n\n# unofficial bash strict mode\nset -euo pipefail\nIFS=$'\\n\\t'\n\n# run from any directory (no symlink allowed)\nCURRENT_PATH=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\"; pwd -P)\ncd ${CURRENT_PATH}\n\n# import\n. imported_file.sh\nsource imported_file.sh\n\n# read and store in a variable\nread MY_VAR\necho $MY_VAR\n# read stdin\nread -p \"Are you sure? [y/n]\" -n 1 -r\n\n\n\n\nDiagnostic\n\n\n# sysfs info\nudevadm info --query=all --name=/dev/xvda\n# monitor kernel uevents\nudevadm monitor\n\n# view kernel's boot and runtime diagnostic messages\ndmesg | less\n\n# system logs paths configuration\nvim /etc/rsyslog.conf\nvim /etc/rsyslog.d/50-default.conf\n# test system logger\nlogger -p mail.info mail-message\ntail -n 1 /var/log/syslog\n\n\n\n\nFilesystem\n\n\n# copy data in blocks of a fixed size\n# /dev/zero is a continuous stream of zero bytes\ndd if=/dev/zero of=DUMP_FILE bs=1024 count=1\n\n# view partition table\n# use (g)parted only for partioning disk (supports MBR and GPT)\nsudo parted -l\n\n# create filesystem\nmkfs -t ext4 /dev/PARTITION_NAME\nls -l /sbin/mkfs.*\n\n# list devices and corresponding filesystems UUID\nblkid\n# list attached filesystems\nmount\n# mount device on mount point\nmount -t ext4 /dev/PARTITION_NAME /MOUNT/POINT\n# mount filesystem by its UUID\nmount UUID=xxx-yyy-zzz /MOUNT/POINT\n# make changes permanent after reboot\necho \"UUID=$(sudo blkid -s UUID -o value /dev/PARTITION_NAME)      /MOUNT/POINT   ext4    defaults,nofail        0       2\" | sudo tee -a /etc/fstab\n# mount all filesystems\nmount -a\n# unmount (detach) a filesystem\numount /dev/PARTITION_NAME\n\n# view size and utilization of mounted filesystems\ndf -h\n# disk usage\ndu -sh /* | sort -g\n# disk size\nfdisk --list\n\n# check memory and swap size\nfree -h\n\n# (1) create swap file (~1GB)\ndd if=/dev/zero of=/dev/SWAP_FILE bs=1024 count=1024000\n# (2) create swap file (2GB)\nfallocate -l 2G /dev/SWAP_FILE\n# change owner and permissions\nchown root:root /dev/SWAP_FILE\nchmod 0600 /dev/SWAP_FILE\n# put swap signature on partition\nmkswap /dev/SWAP_FILE\n# register space with the kernel\nswapon /dev/SWAP_FILE\n# make changes permanent after reboot\necho \"/dev/SWAP_FILE    none    swap    sw    0   0\" | tee -a /etc/fstab\n# list swap partitions\nswapon --show\n\n# simple static server on port 8000\npython -m SimpleHTTPServer\n\n# copy directory to remote host\nscp -r directory remote_host:~/new-directory\ntar cBvf - directory | ssh remote_host tar xBvpf -\nrsync -az directory remote_host:~/new-\n# equivalent to /*\n# -nv dry run\nrsync -a directory/ remote_host:~/new-directory\n\n\n\n\nMonitoring\n\n\n# list processes\n# m show threads\nps aux\n\n# display current system status\n# Spacebar Updates the display immediately\n# M Sorts by current resident memory usage\n# T Sorts by total (cumulative) CPU usage\n# P Sorts by current CPU usage (the default)\n# u Displays only one user\u2019s processes\n# f Selects different statistics to display\n# ? Displays a usage summary for all top commands\ntop\ntop -p PID1 PID2\n# alternatives\nhtop\natop\n\n# monitor system performance\nvmstat 2\n\n# list open files and the processes using them\nlsof | less\nlsof /dev\n\n# print all the system calls that a process makes\nstrace cat /dev/null\nstrace uptime\n# track shared library calls\nltrace ls /\n\n# CPU usage\n/usr/bin/time ls\n\n# change process priority (-20 < nice value < +20)\nrenice 20 PID\n\n# load average: for the past 1 minute, 5 minutes and 15 minutes\nuptime\n\n# check memory status\nfree\ncat /proc/meminfo\n\n# check major/minor page faults\n/usr/bin/time cal > /dev/null\n\n# show statistics for machine\u2019s current uptime (install sysstat)\niostat\n# show partition information\niostat -p ALL\n\n# show I/O resources used by individual processes\niotop\n\n# see the resource consumption of a process over time\npidstat -p PID 1\n\n# reports CPU and IO stats\niostat -mt 2\n\n# system resource statistics\ndstat\n\n\n\n\nNetwork\n\n\n# active network interfaces\nifconfig\n# enable/disable network interface\nifconfig NETWORK_INTERFACE up\nifconfig NETWORK_INTERFACE down\n\n# show routing table\n# Destination: network prefix e.g. 0.0.0.0/0 matches every address (default route)\n# flag U: up\n# flag G: gateway\n# convention: the router is usually at address 1 of the subnet\nroute -n\n\n# ICMP echo request\n# icmp_req: verify order and no gap\n# time: round-trip time\nping -c 3 8.8.8.8\n\n# show path packets take to a remote host\ntraceroute 8.8.8.8\n\n# (DNS) find the IP address behind a domain name\nhost www.github.com\n\n# network manager\nnmcli\nnmcli device show\n# returns zero as its exit code if network is up\nnm-online\n# network details e.g. ssid/password\ncat /etc/NetworkManager/system-connections/NETWORK_NAME\n\n# override hostname lookups\nvim /etc/hosts\n\n# traditional configuration file for DNS servers\ncat /etc/resolv.conf\n# DNS settings\ncat /etc/nsswitch.conf\n\n# static IP\n/etc/network/interfaces\n\n# -t Prints TCP port information\n# -u Prints UDP port information\n# -l Prints listening ports\n# -a Prints every active port\n# -n Disables name lookups (speeds things up; also useful if DNS isn\u2019t working)\n# list open TCP connections\nnetstat -nt\n# print listening TCP ports\nnetstat -ntl\n# list running services\nnetstat -plunt\n\n# processes listening on open TCP ports\nlsof -i -n -P | grep TCP\nlsof -iTCP -sTCP:LISTEN\n# process running on specific port\nlsof -n -i:PORT_NUMBER\n# list unix domain socket\nlsof -U\n\n# well-known ports\ncat /etc/services\n\n# release IP with DHCP\ndhclient -r NETWORK_INTERFACE_NAME\n# renew IP\ndhclient -v NETWORK_INTERFACE_NAME\n\n# public IP via external services\nhttp ident.me\nhttp ipv4.ident.me\nhttp ipv6.ident.me\nhttp icanhazip.com\nhttp ipv4.icanhazip.com\nhttp ipv6.icanhazip.com\n\n# Linux kernel does not automatically move packets from one subnet to another\n# enable temporary IP forwarding in the router's kernel\nsysctl -w net.ipv4.ip_forward\n# change permanent configs upon reboot\nvim /etc/sysctl.conf\n\n# example NAT (IP masquerading)\nsysctl -w net.ipv4.ip_forward\niptables -P FORWARD DROP\niptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE\niptables -A FORWARD -i eth0 -o eth1 -m state --state ESTABLISHED,RELATED -j ACCEPT\niptables -A FORWARD -i eth1 -o eth0 -j ACCEPT\n\n# firewalling on individual machines is sometimes called IP filtering\n# firewall rules in series or chain make up a table\n# INPUT chain: protect individual machine\n# FORWARD chain: protect a network of machines\n# show iptable configuration\niptables -L\n# block IP\niptables -A INPUT -s BLOCKED_IP -j DROP\n# block IP/port\niptables -A INPUT -s BLOCKED_IP/CIDR -p tcp --destination-port BLOCKED_PORT -j DROP\n# allow IP (insert at the bottom)\niptables -A INPUT -s ALLOWED_IP -j ACCEPT\n# allow IP (insert at the top)\niptables -I INPUT -s ALLOWED_IP -j ACCEPT\n# allow IP (specify order)\niptables -I INPUT RULE_NUMBER -s ALLOWED_IP -j ACCEPT\n# delete rule #number in chain\niptables -D INPUT RULE_NUMBER\n\n# show ARP kernel cache\narp -n\n\n# list wireless network\niw dev NETWORK_INTERFACE scan\n# show details of connected network\niw dev NETWORK_INTERFACE link\n\n# manage both authentication and encryption for a wireless network interface\nwpa_supplicant\n\n\n\n\nApplications\n\n\n# old insecure\ntelnet www.wikipedia.org 80\n# press enter twice after\nGET / HTTP/1.0\n\n# details about communication\ncurl --trace-ascii trace_file https://www.wikipedia.org > /dev/null\nvim trace_file\n\n# sshd server configs\nvim /etc/ssh/sshd_config\n# generate key pair\nssh-keygen -t rsa -b 4096 -C KEY_NAME -N \"PASSPHRASE\" -f KEY_PATH\n\n# list network interfaces\ntcpdump -D\n# sniff hex and ascii (-A) by interface/host/port\ntcpdump -XX -n -i INTERFACE_NAME tcp and host IP_ADDRESS and port PORT_NUMBER\n\n# Swiss Army knife\n# banner grabbing\ncat <(echo HEAD / HTTP/1.0) - | netcat IP_ADDRESS PORT_NUMBER\n# install traditional (with -e option)\napt-get install netcat -y\n# choose /bin/nc.traditional\nupdate-alternatives --config nc\n# listen on server\nnetcat -l -p 6996 -e /bin/bash\n# run client\ncat <(echo ls -la) - | netcat IP_ADDRESS 6996\n\n# scan open ports\nnmap -Pn IP_ADDRESS\n\n# other\n# * ssh/scp/sftp/rsync\n# * curl/wget\n\n# system calls\nman recv\nman send\n\n\n\n\nUseful links\n\n\n\n\nSubnetting\n\n\nOpenWrt\n\n\nBusyBox\n\n\nNetfilter\n\n\nIptables Essentials\n\n\niptables vs nftables\n\n\nShorewall\n\n\nNmap\n\n\ntshark\n\n\nPostfix\n\n\nHTTPie\n\n\njq\n\n\nperf-tools\n\n\nSamba\n\n\nProgram Library HOWTO",
            "title": "Linux"
        },
        {
            "location": "/linux/#linux",
            "text": "",
            "title": "Linux"
        },
        {
            "location": "/linux/#useful-commands",
            "text": "# create nested directories\nmkdir -p parent/child1/child2 && cd $_\n\n# scroll file from bottom\nless +G /var/log/auth.log\n# follow also if doesn't exist\ntail -F /var/log/auth.log\n\n# find files\nfind /etc -name '*shadow'\n\n# prints lines that match regexp\n# -i case insensitive\n# -v inverts the search\n# -c count lines\ngrep -E '^root' /etc/passwd\n# password encryption\ngrep password.*unix /etc/pam.d/*\n\n# sed = stream editor\n# example substitution\necho -e \"a='1st' b='2nd' c='3rd'\\na='4th' b='5th' c='6th'\" > test.txt\ncat test.txt | sed -nE \"s/^.*a='([^']*).*c='([^']*).*$/\\2\\n\\1/gp\" | sort -r | uniq\n# delete lines three through six\nsed 3,6d /etc/passwd\n\n# pick a single field out of an input stream\nls -l | awk '{print $9}'\n# extract 2nd string\necho \"aaa bbb ccc\" > test.txt\ncat test.txt | awk '{printf(\"2nd: %s\\n\",$2)}'\n\n# pack archive\ntar cvf archive.tar file1 file2\n# table-of-content\ntar tvf archive.tar\n# unpack archive\ntar xvf archive.tar -C output\n# compress and pack archive\ntar zcvf archive.tar.gz /path/to/images/*.jpg\n# unpack compressed archive\ntar zxvf archive.tar.gz\n\n# pack archive\nzip -r backup.zip file-name directory-name\n# zip with password (prompt)\nzip -e backup.zip file-name\n# unpack jar\nunzip my-lib.jar -d /tmp/my-lib\n\n# count lines\nwc -l file\n\n# lowercase random uuid\nuuidgen | tr \"[:upper:]\" \"[:lower:]\"\n\n# number of bytes\nstat --printf=\"%s\" file\n\n# calculator\necho 1+2 | bc\n# print number in binary base 2 format\necho 'obase=2; 240' | bc\n# reverse-polish calculator\necho '1 2 + p' | dc\n# evaluate expressions\nexpr 1 + 2\n\n# unix timestamp\ndate +%s\n# timestamp in microsecond\ndate +%s%N\n\n# calendar\ncal -3\n\n# configure kernel parameters at runtime\nsysctl\n\n# test conditions ([)\ntest a = a && echo equal\n\n# create temporary file\nmktemp\n# X is a template\nmktemp /tmp/my-tmp.XXXXXX\n# signal handler to catch the signal that CTRL-C generates and remove the temporary files\nTMPFILE=$(mktemp /tmp/my-tmp.XXXXXX)\ntrap \"rm -f $TMPFILE; exit 1\" INT\n\n# compare files\ndiff FILE1 FILE2\n\n# here document\nDATE=$(date)\ncat <<EOF\nDate: $DATE\nline1\nline2\nEOF\n\n# strip full path and extension if specified e.g. mail\nbasename /var/log/mail.log .log\n\n# image conversion\ngiftopnm\npnmtopng\n\n# when operating on huge number of files to avoid buffer issues\n# e.g. verify file's type\n# INSECURE find . -name '*.md' -print | xargs file\n# change the find output separator and the xargs argument delimiter from a newline to a NULL character\n# two dashes if there is a chance that any of the target files start with a single dash\nfind . -name '*.md' -print0 | xargs -0 file --\n# supply a {} to substitute the filename and a literal ; to indicate the end of the command\nfind . -name '*.md' -exec file {} \\;\n\n# replaces current shell process with the program you name after exec system call\n# after you press CTRL-D or CTRL-C to terminate the cat program,\n# your window should disappear because its child process no longer exists\nexec cat\n\n# subshell example () e.g. path remains the same outside\n(PATH=/bad/invalid:$PATH; echo $PATH)\n# fast way to copy and preserve permissions\ntar cf - orig | (cd target; tar xvf -)\n\n# X Window System\nxwininfo\nxlsclients -l\nxev\nxinput --list\ndbus-monitor --system\ndbus-monitor --session\n\n# compile C program\ncc -o hello hello.c\n# list shared library (so)\nldd /bin/bash  Script templates  # shebang\n#!/bin/sh\n#!/bin/bash\n\n# unofficial bash strict mode\nset -euo pipefail\nIFS=$'\\n\\t'\n\n# run from any directory (no symlink allowed)\nCURRENT_PATH=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\"; pwd -P)\ncd ${CURRENT_PATH}\n\n# import\n. imported_file.sh\nsource imported_file.sh\n\n# read and store in a variable\nread MY_VAR\necho $MY_VAR\n# read stdin\nread -p \"Are you sure? [y/n]\" -n 1 -r",
            "title": "Useful commands"
        },
        {
            "location": "/linux/#diagnostic",
            "text": "# sysfs info\nudevadm info --query=all --name=/dev/xvda\n# monitor kernel uevents\nudevadm monitor\n\n# view kernel's boot and runtime diagnostic messages\ndmesg | less\n\n# system logs paths configuration\nvim /etc/rsyslog.conf\nvim /etc/rsyslog.d/50-default.conf\n# test system logger\nlogger -p mail.info mail-message\ntail -n 1 /var/log/syslog",
            "title": "Diagnostic"
        },
        {
            "location": "/linux/#filesystem",
            "text": "# copy data in blocks of a fixed size\n# /dev/zero is a continuous stream of zero bytes\ndd if=/dev/zero of=DUMP_FILE bs=1024 count=1\n\n# view partition table\n# use (g)parted only for partioning disk (supports MBR and GPT)\nsudo parted -l\n\n# create filesystem\nmkfs -t ext4 /dev/PARTITION_NAME\nls -l /sbin/mkfs.*\n\n# list devices and corresponding filesystems UUID\nblkid\n# list attached filesystems\nmount\n# mount device on mount point\nmount -t ext4 /dev/PARTITION_NAME /MOUNT/POINT\n# mount filesystem by its UUID\nmount UUID=xxx-yyy-zzz /MOUNT/POINT\n# make changes permanent after reboot\necho \"UUID=$(sudo blkid -s UUID -o value /dev/PARTITION_NAME)      /MOUNT/POINT   ext4    defaults,nofail        0       2\" | sudo tee -a /etc/fstab\n# mount all filesystems\nmount -a\n# unmount (detach) a filesystem\numount /dev/PARTITION_NAME\n\n# view size and utilization of mounted filesystems\ndf -h\n# disk usage\ndu -sh /* | sort -g\n# disk size\nfdisk --list\n\n# check memory and swap size\nfree -h\n\n# (1) create swap file (~1GB)\ndd if=/dev/zero of=/dev/SWAP_FILE bs=1024 count=1024000\n# (2) create swap file (2GB)\nfallocate -l 2G /dev/SWAP_FILE\n# change owner and permissions\nchown root:root /dev/SWAP_FILE\nchmod 0600 /dev/SWAP_FILE\n# put swap signature on partition\nmkswap /dev/SWAP_FILE\n# register space with the kernel\nswapon /dev/SWAP_FILE\n# make changes permanent after reboot\necho \"/dev/SWAP_FILE    none    swap    sw    0   0\" | tee -a /etc/fstab\n# list swap partitions\nswapon --show\n\n# simple static server on port 8000\npython -m SimpleHTTPServer\n\n# copy directory to remote host\nscp -r directory remote_host:~/new-directory\ntar cBvf - directory | ssh remote_host tar xBvpf -\nrsync -az directory remote_host:~/new-\n# equivalent to /*\n# -nv dry run\nrsync -a directory/ remote_host:~/new-directory",
            "title": "Filesystem"
        },
        {
            "location": "/linux/#monitoring",
            "text": "# list processes\n# m show threads\nps aux\n\n# display current system status\n# Spacebar Updates the display immediately\n# M Sorts by current resident memory usage\n# T Sorts by total (cumulative) CPU usage\n# P Sorts by current CPU usage (the default)\n# u Displays only one user\u2019s processes\n# f Selects different statistics to display\n# ? Displays a usage summary for all top commands\ntop\ntop -p PID1 PID2\n# alternatives\nhtop\natop\n\n# monitor system performance\nvmstat 2\n\n# list open files and the processes using them\nlsof | less\nlsof /dev\n\n# print all the system calls that a process makes\nstrace cat /dev/null\nstrace uptime\n# track shared library calls\nltrace ls /\n\n# CPU usage\n/usr/bin/time ls\n\n# change process priority (-20 < nice value < +20)\nrenice 20 PID\n\n# load average: for the past 1 minute, 5 minutes and 15 minutes\nuptime\n\n# check memory status\nfree\ncat /proc/meminfo\n\n# check major/minor page faults\n/usr/bin/time cal > /dev/null\n\n# show statistics for machine\u2019s current uptime (install sysstat)\niostat\n# show partition information\niostat -p ALL\n\n# show I/O resources used by individual processes\niotop\n\n# see the resource consumption of a process over time\npidstat -p PID 1\n\n# reports CPU and IO stats\niostat -mt 2\n\n# system resource statistics\ndstat",
            "title": "Monitoring"
        },
        {
            "location": "/linux/#network",
            "text": "# active network interfaces\nifconfig\n# enable/disable network interface\nifconfig NETWORK_INTERFACE up\nifconfig NETWORK_INTERFACE down\n\n# show routing table\n# Destination: network prefix e.g. 0.0.0.0/0 matches every address (default route)\n# flag U: up\n# flag G: gateway\n# convention: the router is usually at address 1 of the subnet\nroute -n\n\n# ICMP echo request\n# icmp_req: verify order and no gap\n# time: round-trip time\nping -c 3 8.8.8.8\n\n# show path packets take to a remote host\ntraceroute 8.8.8.8\n\n# (DNS) find the IP address behind a domain name\nhost www.github.com\n\n# network manager\nnmcli\nnmcli device show\n# returns zero as its exit code if network is up\nnm-online\n# network details e.g. ssid/password\ncat /etc/NetworkManager/system-connections/NETWORK_NAME\n\n# override hostname lookups\nvim /etc/hosts\n\n# traditional configuration file for DNS servers\ncat /etc/resolv.conf\n# DNS settings\ncat /etc/nsswitch.conf\n\n# static IP\n/etc/network/interfaces\n\n# -t Prints TCP port information\n# -u Prints UDP port information\n# -l Prints listening ports\n# -a Prints every active port\n# -n Disables name lookups (speeds things up; also useful if DNS isn\u2019t working)\n# list open TCP connections\nnetstat -nt\n# print listening TCP ports\nnetstat -ntl\n# list running services\nnetstat -plunt\n\n# processes listening on open TCP ports\nlsof -i -n -P | grep TCP\nlsof -iTCP -sTCP:LISTEN\n# process running on specific port\nlsof -n -i:PORT_NUMBER\n# list unix domain socket\nlsof -U\n\n# well-known ports\ncat /etc/services\n\n# release IP with DHCP\ndhclient -r NETWORK_INTERFACE_NAME\n# renew IP\ndhclient -v NETWORK_INTERFACE_NAME\n\n# public IP via external services\nhttp ident.me\nhttp ipv4.ident.me\nhttp ipv6.ident.me\nhttp icanhazip.com\nhttp ipv4.icanhazip.com\nhttp ipv6.icanhazip.com\n\n# Linux kernel does not automatically move packets from one subnet to another\n# enable temporary IP forwarding in the router's kernel\nsysctl -w net.ipv4.ip_forward\n# change permanent configs upon reboot\nvim /etc/sysctl.conf\n\n# example NAT (IP masquerading)\nsysctl -w net.ipv4.ip_forward\niptables -P FORWARD DROP\niptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE\niptables -A FORWARD -i eth0 -o eth1 -m state --state ESTABLISHED,RELATED -j ACCEPT\niptables -A FORWARD -i eth1 -o eth0 -j ACCEPT\n\n# firewalling on individual machines is sometimes called IP filtering\n# firewall rules in series or chain make up a table\n# INPUT chain: protect individual machine\n# FORWARD chain: protect a network of machines\n# show iptable configuration\niptables -L\n# block IP\niptables -A INPUT -s BLOCKED_IP -j DROP\n# block IP/port\niptables -A INPUT -s BLOCKED_IP/CIDR -p tcp --destination-port BLOCKED_PORT -j DROP\n# allow IP (insert at the bottom)\niptables -A INPUT -s ALLOWED_IP -j ACCEPT\n# allow IP (insert at the top)\niptables -I INPUT -s ALLOWED_IP -j ACCEPT\n# allow IP (specify order)\niptables -I INPUT RULE_NUMBER -s ALLOWED_IP -j ACCEPT\n# delete rule #number in chain\niptables -D INPUT RULE_NUMBER\n\n# show ARP kernel cache\narp -n\n\n# list wireless network\niw dev NETWORK_INTERFACE scan\n# show details of connected network\niw dev NETWORK_INTERFACE link\n\n# manage both authentication and encryption for a wireless network interface\nwpa_supplicant",
            "title": "Network"
        },
        {
            "location": "/linux/#applications",
            "text": "# old insecure\ntelnet www.wikipedia.org 80\n# press enter twice after\nGET / HTTP/1.0\n\n# details about communication\ncurl --trace-ascii trace_file https://www.wikipedia.org > /dev/null\nvim trace_file\n\n# sshd server configs\nvim /etc/ssh/sshd_config\n# generate key pair\nssh-keygen -t rsa -b 4096 -C KEY_NAME -N \"PASSPHRASE\" -f KEY_PATH\n\n# list network interfaces\ntcpdump -D\n# sniff hex and ascii (-A) by interface/host/port\ntcpdump -XX -n -i INTERFACE_NAME tcp and host IP_ADDRESS and port PORT_NUMBER\n\n# Swiss Army knife\n# banner grabbing\ncat <(echo HEAD / HTTP/1.0) - | netcat IP_ADDRESS PORT_NUMBER\n# install traditional (with -e option)\napt-get install netcat -y\n# choose /bin/nc.traditional\nupdate-alternatives --config nc\n# listen on server\nnetcat -l -p 6996 -e /bin/bash\n# run client\ncat <(echo ls -la) - | netcat IP_ADDRESS 6996\n\n# scan open ports\nnmap -Pn IP_ADDRESS\n\n# other\n# * ssh/scp/sftp/rsync\n# * curl/wget\n\n# system calls\nman recv\nman send",
            "title": "Applications"
        },
        {
            "location": "/linux/#useful-links",
            "text": "Subnetting  OpenWrt  BusyBox  Netfilter  Iptables Essentials  iptables vs nftables  Shorewall  Nmap  tshark  Postfix  HTTPie  jq  perf-tools  Samba  Program Library HOWTO",
            "title": "Useful links"
        },
        {
            "location": "/docker/",
            "text": "Docker\n\n\n\n\nDocker\n is an open platform for developers and sysadmins to build, ship, and run distributed applications\n\n\n\n\nDocumentation\n\n\n\n\nDocker\n\n\n\n\nHow-To\n\n\nSetup\n\n\n# install docker\ncurl -fsSL get.docker.com -o get-docker.sh && \\\n  chmod u+x $_ && \\\n  ./$_ && \\\n  sudo usermod -aG docker hadoop\n\ndocker --version\n\n# install docker-compose\nsudo curl -L https://github.com/docker/compose/releases/download/1.19.0/docker-compose-`uname -s`-`uname -m` \\\n  -o /usr/local/bin/docker-compose && \\\n  sudo chmod +x /usr/local/bin/docker-compose\n\ndocker-compose --version\n\n# install docker-machine (VirtualBox required)\ncurl -L https://github.com/docker/machine/releases/download/v0.13.0/docker-machine-`uname -s`-`uname -m` >/tmp/docker-machine && \\\n  sudo install /tmp/docker-machine /usr/local/bin/docker-machine\n\ndocker-machine --version\n\n\n\n\nUseful commands\n\n\n# list images\ndocker images\n# list containers\ndocker ps -a\n# list volumes\ndocker volume ls\n\n# run temporary container\ndocker run --rm --name phusion phusion/baseimage:latest\n# access container from another shell\ndocker exec -it phusion bash\n\n# remove container by name\ndocker ps -a -q -f name=CONTAINER_NAME | xargs --no-run-if-empty docker rm -f\n# delete dangling images <none>\ndocker images -q -f dangling=true | xargs --no-run-if-empty docker rmi\n# delete dangling volumes\ndocker volume ls -q -f dangling=true | xargs --no-run-if-empty docker volume rm\n\n\n\n\nDocker Machine\n\n\n# create local machine\ndocker-machine create --driver virtualbox default\n\n# list\ndocker-machine ls\ndocker-machine ls --filter name=default\ndocker-machine ls --filter state=Running\ndocker-machine ls --format \"{{.Name}}: {{.DriverName}} - {{.State}}\"\n\n# info\ndocker-machine inspect default\ndocker-machine inspect --format='{{.Driver.IPAddress}}' default\ndocker-machine status default\ndocker-machine ip default\n\n# management\ndocker-machine start default\ndocker-machine stop default\ndocker-machine restart default\ndocker-machine rm default\n\n# mount volume\n#https://docs.docker.com/machine/reference/mount\n\n# show command to connect to machine\ndocker-machine env default\n# check if variables are set\nenv | grep DOCKER\n\n# connect to machine\neval \"$(docker-machine env default)\"\ndocker ps -a\n\n# show command to disconnect from machine\ndocker-machine env -u\n# unset all\neval $(docker-machine env -u)\n\n# access\ndocker-machine ssh default\n# execute command and exit\ndocker-machine ssh default uptime\n# copy files from host to guest\ndocker-machine scp -r /FROM default:/TO\n\n# start nginx on default machine\ndocker run -d -p 8000:80 nginx\n# verify from host\ncurl $(docker-machine ip default):8000\n# forward to port 8080\ndocker-machine ssh default -L 8080:localhost:8000\n# verify tunnel from host\ncurl localhost:8080\n\n# disable error crash reporting\nmkdir -p ~/.docker/machine && touch ~/.docker/machine/no-error-report\n\n\n\n\nBase image\n\n\n\n\nSupervisor\n\n\n\n\nBuild \ndevops/base\n image\n\n\n# change path\ncd devops/base\n\n# build image\ndocker build -t devops/base .\n\n# temporary container\ndocker run --rm --name devops-base devops/base\n# access container\ndocker exec -it devops-base bash\n\n# configurations\n/etc/supervisor/conf.d\n\n# supervisor actions\nsupervisorctl status\nsupervisorctl start SERVICE_NAME\nsupervisorctl stop SERVICE_NAME\n\n\n\n\nDocker Hub\n\n\n\n\nniqdev/phusion-base\n\n\nniqdev/zookeeper\n\n\nniqdev/kafka\n\n\n\n\ndocker login\n\n# phusion-base\ndocker build -t devops/base:latest ./base\ndocker tag devops/base niqdev/phusion-base:1.0.0\ndocker tag devops/base niqdev/phusion-base:latest\ndocker push niqdev/phusion-base:1.0.0\ndocker push niqdev/phusion-base:latest\n\n# zookeeper\ndocker build -t devops/zookeeper:latest ./zookeeper\ndocker tag devops/zookeeper niqdev/zookeeper:1.0.0\ndocker tag devops/zookeeper niqdev/zookeeper\ndocker push niqdev/zookeeper:1.0.0\ndocker push niqdev/zookeeper:latest\n\n# kafka\ndocker build -t devops/kafka:latest ./kafka\ndocker tag devops/kafka niqdev/kafka:1.0.0\ndocker tag devops/kafka niqdev/kafka\ndocker push niqdev/kafka:1.0.0\ndocker push niqdev/kafka:latest\n\ndocker-compose -f kafka/docker-compose-hub.yml up",
            "title": "Docker"
        },
        {
            "location": "/docker/#docker",
            "text": "Docker  is an open platform for developers and sysadmins to build, ship, and run distributed applications   Documentation   Docker",
            "title": "Docker"
        },
        {
            "location": "/docker/#how-to",
            "text": "Setup  # install docker\ncurl -fsSL get.docker.com -o get-docker.sh && \\\n  chmod u+x $_ && \\\n  ./$_ && \\\n  sudo usermod -aG docker hadoop\n\ndocker --version\n\n# install docker-compose\nsudo curl -L https://github.com/docker/compose/releases/download/1.19.0/docker-compose-`uname -s`-`uname -m` \\\n  -o /usr/local/bin/docker-compose && \\\n  sudo chmod +x /usr/local/bin/docker-compose\n\ndocker-compose --version\n\n# install docker-machine (VirtualBox required)\ncurl -L https://github.com/docker/machine/releases/download/v0.13.0/docker-machine-`uname -s`-`uname -m` >/tmp/docker-machine && \\\n  sudo install /tmp/docker-machine /usr/local/bin/docker-machine\n\ndocker-machine --version  Useful commands  # list images\ndocker images\n# list containers\ndocker ps -a\n# list volumes\ndocker volume ls\n\n# run temporary container\ndocker run --rm --name phusion phusion/baseimage:latest\n# access container from another shell\ndocker exec -it phusion bash\n\n# remove container by name\ndocker ps -a -q -f name=CONTAINER_NAME | xargs --no-run-if-empty docker rm -f\n# delete dangling images <none>\ndocker images -q -f dangling=true | xargs --no-run-if-empty docker rmi\n# delete dangling volumes\ndocker volume ls -q -f dangling=true | xargs --no-run-if-empty docker volume rm  Docker Machine  # create local machine\ndocker-machine create --driver virtualbox default\n\n# list\ndocker-machine ls\ndocker-machine ls --filter name=default\ndocker-machine ls --filter state=Running\ndocker-machine ls --format \"{{.Name}}: {{.DriverName}} - {{.State}}\"\n\n# info\ndocker-machine inspect default\ndocker-machine inspect --format='{{.Driver.IPAddress}}' default\ndocker-machine status default\ndocker-machine ip default\n\n# management\ndocker-machine start default\ndocker-machine stop default\ndocker-machine restart default\ndocker-machine rm default\n\n# mount volume\n#https://docs.docker.com/machine/reference/mount\n\n# show command to connect to machine\ndocker-machine env default\n# check if variables are set\nenv | grep DOCKER\n\n# connect to machine\neval \"$(docker-machine env default)\"\ndocker ps -a\n\n# show command to disconnect from machine\ndocker-machine env -u\n# unset all\neval $(docker-machine env -u)\n\n# access\ndocker-machine ssh default\n# execute command and exit\ndocker-machine ssh default uptime\n# copy files from host to guest\ndocker-machine scp -r /FROM default:/TO\n\n# start nginx on default machine\ndocker run -d -p 8000:80 nginx\n# verify from host\ncurl $(docker-machine ip default):8000\n# forward to port 8080\ndocker-machine ssh default -L 8080:localhost:8000\n# verify tunnel from host\ncurl localhost:8080\n\n# disable error crash reporting\nmkdir -p ~/.docker/machine && touch ~/.docker/machine/no-error-report",
            "title": "How-To"
        },
        {
            "location": "/docker/#base-image",
            "text": "Supervisor   Build  devops/base  image  # change path\ncd devops/base\n\n# build image\ndocker build -t devops/base .\n\n# temporary container\ndocker run --rm --name devops-base devops/base\n# access container\ndocker exec -it devops-base bash\n\n# configurations\n/etc/supervisor/conf.d\n\n# supervisor actions\nsupervisorctl status\nsupervisorctl start SERVICE_NAME\nsupervisorctl stop SERVICE_NAME",
            "title": "Base image"
        },
        {
            "location": "/docker/#docker-hub",
            "text": "niqdev/phusion-base  niqdev/zookeeper  niqdev/kafka   docker login\n\n# phusion-base\ndocker build -t devops/base:latest ./base\ndocker tag devops/base niqdev/phusion-base:1.0.0\ndocker tag devops/base niqdev/phusion-base:latest\ndocker push niqdev/phusion-base:1.0.0\ndocker push niqdev/phusion-base:latest\n\n# zookeeper\ndocker build -t devops/zookeeper:latest ./zookeeper\ndocker tag devops/zookeeper niqdev/zookeeper:1.0.0\ndocker tag devops/zookeeper niqdev/zookeeper\ndocker push niqdev/zookeeper:1.0.0\ndocker push niqdev/zookeeper:latest\n\n# kafka\ndocker build -t devops/kafka:latest ./kafka\ndocker tag devops/kafka niqdev/kafka:1.0.0\ndocker tag devops/kafka niqdev/kafka\ndocker push niqdev/kafka:1.0.0\ndocker push niqdev/kafka:latest\n\ndocker-compose -f kafka/docker-compose-hub.yml up",
            "title": "Docker Hub"
        },
        {
            "location": "/ansible/",
            "text": "Ansible\n\n\n\n\nAnsible\n is an open source automation platform that can help with config management, deployment and task automation\n\n\n\n\nDocumentation\n\n\n\n\nAnsible\n\n\nTutorial\n\n\nPlaybook example\n\n\n\n\nThe following guide explains how to provision Ansible locally and play with it. Checkout the \nVagrantfile\n and the Vagrant \nguide\n for more details.\n\n\nSetup\n\n\nRequirements\n\n\n\n\nVagrant\n\n\nVirtualBox\n\n\n\n\nDirectory structure\n\n\ntree -a ansible/\nansible/\n\u251c\u2500\u2500 .share\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-3\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ssh\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ansible_rsa\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 ansible_rsa.pub\n\u251c\u2500\u2500 Vagrantfile\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 group_vars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 host_vars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hosts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 roles\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 common\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 defaults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 files\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 handlers\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 motd.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 package.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 templates\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 motd\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 vars\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 docker\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 site.yml\n\u251c\u2500\u2500 setup_ansible.sh\n\u2514\u2500\u2500 setup_share.sh\n\n\n\n\nThe first time \nonly\n, you have to setup the shared folders and generate the ssh key needed by ansible to access all nodes executing\n\n\n./setup_share.sh\n\n\n\n\nStart the boxes with\n\n\nvagrant up\n\n\n\n\nThe first time it could take a while\n\n\nVerify status of the boxes with\n\n\nvagrant status\n\n\n\n\nVerify access to the boxes with\n\n\nvagrant ssh ansible\nvagrant ssh node-1\n\n\n\n\nFrom inside the boxes you should be able to communicate with the others\n\n\nping ansible.local\nping ip-192-168-100-11.local\nping 192.168.100.12\n\n\n\n\nThe following paths are shared with the boxes\n\n\n\n\n/vagrant\n provision-tool\n\n\n/local\n host $HOME\n\n\n/ansible\n data \n(ansible only)\n\n\n/data\n .share \n(node only)\n\n\n\n\nAd-Hoc Commands\n\n\nAccess the ansible box with\n\n\nvagrant ssh ansible\n\n\n\n\nBelow a list of examples\n\n\n\n# ping all nodes (default inventory /etc/ansible/hosts)\nansible all -m ping\nansible ansible -m ping\nansible cluster -m ping\n\n# ping all nodes (specify inventory)\nansible all -i \"/vagrant/data/hosts\" -m ping\n\n# gathering facts\nansible all -m setup\nansible ansible -m setup\n\n# specify host and user\nansible ip-192-168-100-11.local -m ping -u vagrant\n\n# execute command\nansible all -a \"/bin/echo hello\"\nansible all -a \"uptime\"\nansible all -a \"/bin/date\"\n# do NOT reboot vagrant through ansible (use vagrant reload)\nansible cluster -a \"/sbin/reboot\" --become\n\n# shell module\nansible all -m shell -a \"pwd\"\n# be carefull to quotes\nansible all -m shell -a 'echo $HOME'\n\n# update && upgrade\nansible all -m apt -a \"update_cache=yes upgrade=dist\" --become\n# restart after upgrade\nvagrant reload\n# install package\nansible all -m apt -a \"name=tree state=present\" --become\n\n\n\n\nPlaybooks\n\n\nAccess the ansible box with\n\n\nvagrant ssh ansible\n\n\n\n\nBelow a list of examples\n\n\n# test uptime on all node\nansible-playbook /ansible/site.yml --tags=test --verbose\n\n# update & upgrade only cluster nodes\nansible-playbook /ansible/site.yml -t package --skip-tags=java --verbose\n\n# install packages on cluster nodes\nansible-playbook /ansible/site.yml -t package --verbose\n\n# run common task on cluster node\nansible-playbook /ansible/site.yml -t common\n\n# setup docker\nansible-playbook /ansible/site.yml -t docker\n# test docker\nvagrant ssh node-1\nsudo -i -u docker\ndocker ps -a\n\n# custom banner\nansible-playbook /ansible/site.yml -t motd\n\n# setup all infrastructure at once\nansible-playbook /ansible/site.yml\n\n# dry run\nansible-playbook -i /ansible/hosts /ansible/site.yml --check --diff",
            "title": "Ansible"
        },
        {
            "location": "/ansible/#ansible",
            "text": "Ansible  is an open source automation platform that can help with config management, deployment and task automation   Documentation   Ansible  Tutorial  Playbook example   The following guide explains how to provision Ansible locally and play with it. Checkout the  Vagrantfile  and the Vagrant  guide  for more details.",
            "title": "Ansible"
        },
        {
            "location": "/ansible/#setup",
            "text": "Requirements   Vagrant  VirtualBox   Directory structure  tree -a ansible/\nansible/\n\u251c\u2500\u2500 .share\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-3\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ssh\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ansible_rsa\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 ansible_rsa.pub\n\u251c\u2500\u2500 Vagrantfile\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 group_vars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 host_vars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hosts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 roles\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 common\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 defaults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 files\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 handlers\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 motd.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 package.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 templates\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 motd\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 vars\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 docker\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 site.yml\n\u251c\u2500\u2500 setup_ansible.sh\n\u2514\u2500\u2500 setup_share.sh  The first time  only , you have to setup the shared folders and generate the ssh key needed by ansible to access all nodes executing  ./setup_share.sh  Start the boxes with  vagrant up  The first time it could take a while  Verify status of the boxes with  vagrant status  Verify access to the boxes with  vagrant ssh ansible\nvagrant ssh node-1  From inside the boxes you should be able to communicate with the others  ping ansible.local\nping ip-192-168-100-11.local\nping 192.168.100.12  The following paths are shared with the boxes   /vagrant  provision-tool  /local  host $HOME  /ansible  data  (ansible only)  /data  .share  (node only)",
            "title": "Setup"
        },
        {
            "location": "/ansible/#ad-hoc-commands",
            "text": "Access the ansible box with  vagrant ssh ansible  Below a list of examples  \n# ping all nodes (default inventory /etc/ansible/hosts)\nansible all -m ping\nansible ansible -m ping\nansible cluster -m ping\n\n# ping all nodes (specify inventory)\nansible all -i \"/vagrant/data/hosts\" -m ping\n\n# gathering facts\nansible all -m setup\nansible ansible -m setup\n\n# specify host and user\nansible ip-192-168-100-11.local -m ping -u vagrant\n\n# execute command\nansible all -a \"/bin/echo hello\"\nansible all -a \"uptime\"\nansible all -a \"/bin/date\"\n# do NOT reboot vagrant through ansible (use vagrant reload)\nansible cluster -a \"/sbin/reboot\" --become\n\n# shell module\nansible all -m shell -a \"pwd\"\n# be carefull to quotes\nansible all -m shell -a 'echo $HOME'\n\n# update && upgrade\nansible all -m apt -a \"update_cache=yes upgrade=dist\" --become\n# restart after upgrade\nvagrant reload\n# install package\nansible all -m apt -a \"name=tree state=present\" --become",
            "title": "Ad-Hoc Commands"
        },
        {
            "location": "/ansible/#playbooks",
            "text": "Access the ansible box with  vagrant ssh ansible  Below a list of examples  # test uptime on all node\nansible-playbook /ansible/site.yml --tags=test --verbose\n\n# update & upgrade only cluster nodes\nansible-playbook /ansible/site.yml -t package --skip-tags=java --verbose\n\n# install packages on cluster nodes\nansible-playbook /ansible/site.yml -t package --verbose\n\n# run common task on cluster node\nansible-playbook /ansible/site.yml -t common\n\n# setup docker\nansible-playbook /ansible/site.yml -t docker\n# test docker\nvagrant ssh node-1\nsudo -i -u docker\ndocker ps -a\n\n# custom banner\nansible-playbook /ansible/site.yml -t motd\n\n# setup all infrastructure at once\nansible-playbook /ansible/site.yml\n\n# dry run\nansible-playbook -i /ansible/hosts /ansible/site.yml --check --diff",
            "title": "Playbooks"
        },
        {
            "location": "/cassandra/",
            "text": "Cassandra\n\n\n\n\nCassandra\n is a distributed database for managing large amounts of structured data across many commodity servers, while providing highly available service and no single point of failure\n\n\n\n\nDocumentation\n\n\n\n\n\n\nCassandra\n\n\n\n\n\n\nA Decentralized Structured Storage System\n (Paper)\n\n\n\n\n\n\nA Big Data Modeling Methodology for Apache Cassandra\n (Paper)\n\n\n\n\n\n\nFacebook\u2019s Cassandra paper\n\n\n\n\n\n\nCassandra Data Modeling Best Practices\n\n\n\n\n\n\nDifference between partition key, composite key and clustering key\n\n\n\n\n\n\nCassandra Cluster Manager\n\n\n\n\n\n\nNetflix Priam\n\n\n\n\n\n\ncstar_perf\n\n\n\n\n\n\nAmy's Cassandra 2.1 tuning guide\n\n\n\n\n\n\nRepair in Cassandra\n\n\n\n\n\n\n\n\n\nCassandra uses a tick-tock release model, even-numbered releases are feature releases, while odd-numbered releases are focused on bug fixes\n\n\nArchitecture\n\n\n\n\n\n\nA \nrack\n is a logical set of nodes in close proximity to each other\n\n\n\n\n\n\nA \ndata center\n is a logical set of racks\n\n\n\n\n\n\nCassandra uses a \ngossip protocol\n (called epidemic protocol) that allows each node to keep track of state information about the other nodes in the cluster implementing an algorithm called \nPhi Accrual Failure Detection\n instead of simple heartbeats\n\n\n\n\n\n\nThe job of a \nsnitch\n is to determine relative host proximity for each node in a cluster, which is used to determine which nodes to read and write from\n\n\n\n\n\n\nCassandra represents the data managed by a cluster as a \nring\n. Each node in the ring is assigned one or more ranges of data described by a \ntoken\n, which determines its position in the ring and is used to identify each partition\n\n\n\n\n\n\n\n\n\n\n\n\nvirtual nodes\n allow to break a token range and assign multiple tokens to a single physical node\n\n\n\n\n\n\nA \npartitioner\n is a hash function for computing the token of a partition key and determines how a (wide) row or partition of data is distributed within the ring\n\n\n\n\n\n\nThe \nreplication factor\n is the number of nodes in a cluster that will receive copies of the same row and the replication strategy is set independently for each keyspace\n\n\n\n\n\n\nCassandra provides tuneable \nconsistency\n levels and must be specified on each read or write\n\n\n\n\n\n\nA client may connect to any node in the cluster, named \ncoordinator node\n, to initiate a read or write query. The coordinator identifies which nodes are replicas for the data and forwards the queries to them\n\n\n\n\n\n\n\n\n\n\n\n\nWhen a write operation is performed, it's immediately written to a \ncommit log\n to ensure that data is not lost. It is a crash-recovery mechanism only, clients never read from it\n\n\n\n\n\n\nAfter it's written to the commit log, the value is written (already ordered) to a memory-resident data structure called the \nmemtable\n divided by Column Family (table)\n\n\n\n\n\n\nWhen the number of objects stored in the memtable or in the commit log reaches a threshold, the contents of the memtable are flushed (non-blocking operation) to disk in a file called \nSSTable\n and a new memtable or commit log is then created/recycled\n\n\n\n\n\n\nNo reads or seeks of any kind are required for writing a value to Cassandra because all writes are append operations to immutable SSTables. However, periodic \ncompaction\n operations in Cassandra are performed in order to support fast read performance: the keys are merged, columns are combined, tombstones are discarded, and a new index is created\n\n\n\n\n\n\nThe \nkey cache\n stores a map of partition keys to row index entries, facilitating faster read access into SSTables stored on disk. The key cache is stored on the JVM heap\n\n\n\n\n\n\nThe \nrow cache\n caches entire rows and can greatly speed up read access for frequently accessed rows, at the cost of more memory usage. The row cache is stored in off-heap memory\n\n\n\n\n\n\nThe \ncounter cache\n is used to improve counter performance by reducing lock contention for the most frequently accessed counters\n\n\n\n\n\n\nIn a scenario in which a write request is sent to Cassandra, but a replica node where the write properly belongs is not available due to network partition, hardware failure, or some other reason, to ensure general availability Cassandra implements a feature called \nhinted handoff\n. The coordinator node while store temporarily the data until it detects that the node is available again\n\n\n\n\n\n\nWrite Path\n\n\n\n\nRead Path\n\n\n\n\n\n\n\n\nTo provide \nlinearizable consistency\n e.g. read-before-write, Cassandra supports a \nlightweight transaction\n or LWT. The implementation is based on \npaxos\n and is limited to a single partition\n\n\n\n\n\n\nA \ntombstone\n is a deletion marker that is required to suppress older data in SSTables until compaction or garbage collection run. Data is not immediately deleted but it's treated as an update operation\n\n\n\n\n\n\nBloom filters\n are very fast, non-deterministic algorithms for testing whether an element is a member of a set. It is possible to get a false-positive read, but not a false-negative. When a read is performed, the filter is checked first before accessing disk, if it indicates that the element does not exist in the set, it certainly doesn't, but if the filter thinks that the element is in the set, the disk is accessed to make sure\n\n\n\n\n\n\nReplica Synchronization (1)\n Cassandra reads data from multiple replicas in order to achieve the requested consistency level and detects if any replicas have out of date values. If an insufficient number of nodes have the latest value, a \nread repair\n is performed immediately to update the out of date replicas\n\n\n\n\n\n\nReplica Synchronization (2)\n \nAnti-entropy repair\n is a manually initiated operation performed on nodes as part of a regular maintenance process executed with \nnodetool\n causing a \nmajor compaction\n during which a node exchange \nMerkle trees\n with neighboring nodes\n\n\n\n\n\n\nSetup\n\n\nSingle Node Cluster\n\n\n# change path\ncd devops/cassandra\n\n# start single node\ndocker-compose up\n\n# paths\n/etc/cassandra\n/var/lib/cassandra\n/var/log/cassandra\n\n# remove container and volume\ndocker rm -fv devops-cassandra\n\n\n\n\nMulti Node Cluster\n\n\n# change path\ncd devops/cassandra\n\n# start node\ndocker-compose -f docker-compose-cluster.yml up\n\n# optional mounted volumes\nmkdir -p \\\n  .cassandra/cassandra-seed/{data,log} \\\n  .cassandra/cassandra-node-1/{data,log} \\\n  .cassandra/cassandra-node-2/{data,log}\ntree .cassandra/\n\n# ISSUES releated to host permissions\n# > Small commitlog volume detected at /var/lib/cassandra/commitlog\n# > There is insufficient memory for the Java Runtime Environment to continue\n(cassandra) /var/lib/cassandra\n(root) /var/log/cassandra\n\n\n\n\nAccess container\n\n\n# access container\ndocker exec -it devops-cassandra bash\ndocker exec -it devops-cassandra bash -c cqlsh\ndocker exec -it devops-cassandra-seed bash\ndocker exec -it devops-cassandra-node-1 bash\n\n# execute cql script from host\n(docker exec -i devops-cassandra bash \\\n  -c \"cat > example.cql; cqlsh -f example.cql\") < cql/example_create.cql\n\n\n\n\nCQL\n\n\ncqlsh\n script \nexamples\n\n\n# connect\ncqlsh localhost 9042\ncqlsh localhost 9042 -u cassandra -p cassandra\n\n# execute cql script\ncqlsh -f cql/example_create.cql\n\n# info\nSHOW VERSION;\nDESCRIBE CLUSTER;\nDESCRIBE KEYSPACES;\nDESCRIBE KEYSPACE example;\nDESCRIBE TABLE example.messages;\n\n# nice format\nEXPAND ON;\n# trace query\nTRACING ON;\n\n# bulk loading\nCOPY example.users TO '/cql/users.csv' WITH HEADER=TRUE;\nCOPY example.users FROM '/cql/all_users.csv' WITH DELIMITER = ';';\nCOPY example.users (first_name,last_name,addresses,emails,enable) FROM '/cql/column_users.csv' WITH HEADER=TRUE;\n\n# automatic paging\nPAGING;\nPAGING ON;\nPAGING 100;\n# limit\nSELECT * FROM example.users LIMIT 1;\n\n\n\n\n\n\nBatch\n\n\nUser-Defined Type\n\n\nUser-Defined Function\n\n\nUser-Defined Aggregate Function\n\n\n\n\nOld \ncassandra-cli\n deprecated and removed in Cassandra 3.0\n\n\nUSE keyspace_name;\nLIST table_name;\nGET table_name[\"primary_key\"];\nSET table_name[\"primary_key\"][\"column_name\"];\n\n\n\n\nnodetool\n\n\n# help\nnodetool\n\n# cluster informations\nnodetool describecluster\nnodetool status\n\n# node informations\nnodetool -h xxx.xxx.xxx.xxx info\nnodetool -h xxx.xxx.xxx.xxx statusgossip|statusthrift|statusbinary|statushandoff\nnodetool gossipinfo\n\n# ring informations\nnodetool ring\nnodetool describering KEYSPACE\n\n# monitor network\nnodetool netstats\n\n# threadpool statistics\nnodetool tpstats\n\n# keyspace statistics\nnodetool tablestats KEYSPACE\n\n# dynamic logging via JMX\nnodetool getlogginglevels\n\n# force to write data from memtables to SSTables\nnodetool flush\n\n# gracefully shutdown\nnodetool drain\n\n# discards any data that is no longer owned by the node\n# e.g. after changing replication factor or token range\nnodetool cleanup\n\n# anti-entropy repair or manual repair: reconcile data exchanging Merkle trees among nodes\n# maintenance: incremental parallel repair on the primary token range (run on each node)\nnodetool repair -pr\n\n# create snapshot\nnodetool snapshot\nnodetool listsnapshots\n\n# restore snapshot (create schema or truncate table before)\n# 1) same cluster and configuration\n# copy SSTable \".db\" files into the data directory and on the running node execute refresh\nnodetool refresh\n# 2) different configuration (e.g. topology, token ranges, or replication)\nsstableloader\n\n# stress tool\ncassandra-stress write n=1000000\ncassandra-stress read n=200000",
            "title": "Cassandra"
        },
        {
            "location": "/cassandra/#cassandra",
            "text": "Cassandra  is a distributed database for managing large amounts of structured data across many commodity servers, while providing highly available service and no single point of failure   Documentation    Cassandra    A Decentralized Structured Storage System  (Paper)    A Big Data Modeling Methodology for Apache Cassandra  (Paper)    Facebook\u2019s Cassandra paper    Cassandra Data Modeling Best Practices    Difference between partition key, composite key and clustering key    Cassandra Cluster Manager    Netflix Priam    cstar_perf    Amy's Cassandra 2.1 tuning guide    Repair in Cassandra     Cassandra uses a tick-tock release model, even-numbered releases are feature releases, while odd-numbered releases are focused on bug fixes",
            "title": "Cassandra"
        },
        {
            "location": "/cassandra/#architecture",
            "text": "A  rack  is a logical set of nodes in close proximity to each other    A  data center  is a logical set of racks    Cassandra uses a  gossip protocol  (called epidemic protocol) that allows each node to keep track of state information about the other nodes in the cluster implementing an algorithm called  Phi Accrual Failure Detection  instead of simple heartbeats    The job of a  snitch  is to determine relative host proximity for each node in a cluster, which is used to determine which nodes to read and write from    Cassandra represents the data managed by a cluster as a  ring . Each node in the ring is assigned one or more ranges of data described by a  token , which determines its position in the ring and is used to identify each partition       virtual nodes  allow to break a token range and assign multiple tokens to a single physical node    A  partitioner  is a hash function for computing the token of a partition key and determines how a (wide) row or partition of data is distributed within the ring    The  replication factor  is the number of nodes in a cluster that will receive copies of the same row and the replication strategy is set independently for each keyspace    Cassandra provides tuneable  consistency  levels and must be specified on each read or write    A client may connect to any node in the cluster, named  coordinator node , to initiate a read or write query. The coordinator identifies which nodes are replicas for the data and forwards the queries to them       When a write operation is performed, it's immediately written to a  commit log  to ensure that data is not lost. It is a crash-recovery mechanism only, clients never read from it    After it's written to the commit log, the value is written (already ordered) to a memory-resident data structure called the  memtable  divided by Column Family (table)    When the number of objects stored in the memtable or in the commit log reaches a threshold, the contents of the memtable are flushed (non-blocking operation) to disk in a file called  SSTable  and a new memtable or commit log is then created/recycled    No reads or seeks of any kind are required for writing a value to Cassandra because all writes are append operations to immutable SSTables. However, periodic  compaction  operations in Cassandra are performed in order to support fast read performance: the keys are merged, columns are combined, tombstones are discarded, and a new index is created    The  key cache  stores a map of partition keys to row index entries, facilitating faster read access into SSTables stored on disk. The key cache is stored on the JVM heap    The  row cache  caches entire rows and can greatly speed up read access for frequently accessed rows, at the cost of more memory usage. The row cache is stored in off-heap memory    The  counter cache  is used to improve counter performance by reducing lock contention for the most frequently accessed counters    In a scenario in which a write request is sent to Cassandra, but a replica node where the write properly belongs is not available due to network partition, hardware failure, or some other reason, to ensure general availability Cassandra implements a feature called  hinted handoff . The coordinator node while store temporarily the data until it detects that the node is available again    Write Path   Read Path     To provide  linearizable consistency  e.g. read-before-write, Cassandra supports a  lightweight transaction  or LWT. The implementation is based on  paxos  and is limited to a single partition    A  tombstone  is a deletion marker that is required to suppress older data in SSTables until compaction or garbage collection run. Data is not immediately deleted but it's treated as an update operation    Bloom filters  are very fast, non-deterministic algorithms for testing whether an element is a member of a set. It is possible to get a false-positive read, but not a false-negative. When a read is performed, the filter is checked first before accessing disk, if it indicates that the element does not exist in the set, it certainly doesn't, but if the filter thinks that the element is in the set, the disk is accessed to make sure    Replica Synchronization (1)  Cassandra reads data from multiple replicas in order to achieve the requested consistency level and detects if any replicas have out of date values. If an insufficient number of nodes have the latest value, a  read repair  is performed immediately to update the out of date replicas    Replica Synchronization (2)   Anti-entropy repair  is a manually initiated operation performed on nodes as part of a regular maintenance process executed with  nodetool  causing a  major compaction  during which a node exchange  Merkle trees  with neighboring nodes",
            "title": "Architecture"
        },
        {
            "location": "/cassandra/#setup",
            "text": "Single Node Cluster  # change path\ncd devops/cassandra\n\n# start single node\ndocker-compose up\n\n# paths\n/etc/cassandra\n/var/lib/cassandra\n/var/log/cassandra\n\n# remove container and volume\ndocker rm -fv devops-cassandra  Multi Node Cluster  # change path\ncd devops/cassandra\n\n# start node\ndocker-compose -f docker-compose-cluster.yml up\n\n# optional mounted volumes\nmkdir -p \\\n  .cassandra/cassandra-seed/{data,log} \\\n  .cassandra/cassandra-node-1/{data,log} \\\n  .cassandra/cassandra-node-2/{data,log}\ntree .cassandra/\n\n# ISSUES releated to host permissions\n# > Small commitlog volume detected at /var/lib/cassandra/commitlog\n# > There is insufficient memory for the Java Runtime Environment to continue\n(cassandra) /var/lib/cassandra\n(root) /var/log/cassandra  Access container  # access container\ndocker exec -it devops-cassandra bash\ndocker exec -it devops-cassandra bash -c cqlsh\ndocker exec -it devops-cassandra-seed bash\ndocker exec -it devops-cassandra-node-1 bash\n\n# execute cql script from host\n(docker exec -i devops-cassandra bash \\\n  -c \"cat > example.cql; cqlsh -f example.cql\") < cql/example_create.cql",
            "title": "Setup"
        },
        {
            "location": "/cassandra/#cql",
            "text": "cqlsh  script  examples  # connect\ncqlsh localhost 9042\ncqlsh localhost 9042 -u cassandra -p cassandra\n\n# execute cql script\ncqlsh -f cql/example_create.cql\n\n# info\nSHOW VERSION;\nDESCRIBE CLUSTER;\nDESCRIBE KEYSPACES;\nDESCRIBE KEYSPACE example;\nDESCRIBE TABLE example.messages;\n\n# nice format\nEXPAND ON;\n# trace query\nTRACING ON;\n\n# bulk loading\nCOPY example.users TO '/cql/users.csv' WITH HEADER=TRUE;\nCOPY example.users FROM '/cql/all_users.csv' WITH DELIMITER = ';';\nCOPY example.users (first_name,last_name,addresses,emails,enable) FROM '/cql/column_users.csv' WITH HEADER=TRUE;\n\n# automatic paging\nPAGING;\nPAGING ON;\nPAGING 100;\n# limit\nSELECT * FROM example.users LIMIT 1;   Batch  User-Defined Type  User-Defined Function  User-Defined Aggregate Function   Old  cassandra-cli  deprecated and removed in Cassandra 3.0  USE keyspace_name;\nLIST table_name;\nGET table_name[\"primary_key\"];\nSET table_name[\"primary_key\"][\"column_name\"];",
            "title": "CQL"
        },
        {
            "location": "/cassandra/#nodetool",
            "text": "# help\nnodetool\n\n# cluster informations\nnodetool describecluster\nnodetool status\n\n# node informations\nnodetool -h xxx.xxx.xxx.xxx info\nnodetool -h xxx.xxx.xxx.xxx statusgossip|statusthrift|statusbinary|statushandoff\nnodetool gossipinfo\n\n# ring informations\nnodetool ring\nnodetool describering KEYSPACE\n\n# monitor network\nnodetool netstats\n\n# threadpool statistics\nnodetool tpstats\n\n# keyspace statistics\nnodetool tablestats KEYSPACE\n\n# dynamic logging via JMX\nnodetool getlogginglevels\n\n# force to write data from memtables to SSTables\nnodetool flush\n\n# gracefully shutdown\nnodetool drain\n\n# discards any data that is no longer owned by the node\n# e.g. after changing replication factor or token range\nnodetool cleanup\n\n# anti-entropy repair or manual repair: reconcile data exchanging Merkle trees among nodes\n# maintenance: incremental parallel repair on the primary token range (run on each node)\nnodetool repair -pr\n\n# create snapshot\nnodetool snapshot\nnodetool listsnapshots\n\n# restore snapshot (create schema or truncate table before)\n# 1) same cluster and configuration\n# copy SSTable \".db\" files into the data directory and on the running node execute refresh\nnodetool refresh\n# 2) different configuration (e.g. topology, token ranges, or replication)\nsstableloader\n\n# stress tool\ncassandra-stress write n=1000000\ncassandra-stress read n=200000",
            "title": "nodetool"
        },
        {
            "location": "/zookeeper/",
            "text": "ZooKeeper\n\n\n\n\nZooKeeper\n is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\n\n\n\n\nDocumentation\n\n\n\n\n\n\nZooKeeper\n\n\n\n\n\n\nCurator\n\n\n\n\n\n\nSetup\n\n\nRequirements\n\n\n\n\nBase\n image\n\n\n\n\nBuild \ndevops/zookeeper\n image\n\n\n# change path\ncd devops/zookeeper\n\n# build image\ndocker build -t devops/zookeeper:latest .\n# build image with specific version - see Dockerfile for version 3.5.x\ndocker build -t devops/zookeeper:3.4.10 --build-arg VERSION=3.4.10 .\n\n# temporary container [host:container]\ndocker run --rm --name zookeeper -p 12181:2181 devops/zookeeper\n# access container\ndocker exec -it zookeeper bash\n\n# paths\n/opt/zookeeper\n/var/log/zookeeper\n/var/lib/zookeeper\n/var/log/supervisord.log\n\n# logs\ntail -F /var/log/supervisord.log\n# check service status\nsupervisorctl status\nsupervisorctl restart zookeeper\n\n\n\n\nExample\n\n\ndocker exec -it zookeeper bash\n\n# (option 1) check zookeeper status\necho ruok | nc localhost 2181\n\n# (option 2) check zookeeper status\ntelnet localhost 2181\n# expect answer imok\n> ruok\n\nzkCli.sh -server 127.0.0.1:2181\nhelp\n# list znodes\nls /\n# create znode and associate value\ncreate /zk_test my_data\n# verify data\nget /zk_test\n# change value\nset /zk_test junk\n# delete znode\ndelete /zk_test\n\n\n\n\nThe four-letter words\n\n\n\n\n\n\n\n\nCategory\n\n\nCommand\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nServer status\n\n\nruok\n\n\nPrints \nimok\n if the server is running and not in an error state\n\n\n\n\n\n\n\n\nconf\n\n\nPrints the server configuration (from zoo.cfg)\n\n\n\n\n\n\n\n\nenvi\n\n\nPrints the server environment, including ZooKeeper version, Java version, and other system properties\n\n\n\n\n\n\n\n\nsrvr\n\n\nPrints server statistics, including latency statistics, the number of znodes, and the server mode (standalone, leader, or follower)\n\n\n\n\n\n\n\n\nstat\n\n\nPrints server statistics and connected clients\n\n\n\n\n\n\n\n\nsrst\n\n\nResets server statistics\n\n\n\n\n\n\n\n\nisro\n\n\nShows whether the server is in read-only ( ro ) mode (due to a network partition) or read/write mode (rw)\n\n\n\n\n\n\nClient connections\n\n\ndump\n\n\nLists all the sessions and ephemeral znodes for the ensemble. You must connect to the leader (see srvr) for this command\n\n\n\n\n\n\n\n\ncons\n\n\nLists connection statistics for all the server's clients\n\n\n\n\n\n\n\n\ncrst\n\n\nResets connection statistics\n\n\n\n\n\n\nWatches\n\n\nwchs\n\n\nLists summary information for the server's watches\n\n\n\n\n\n\n\n\nwchc\n\n\nLists all the server's watches by connection, may impact server performance for a large number of watches\n\n\n\n\n\n\n\n\nwchp\n\n\nLists all the server\u2019s watches by znode path, may impact server performance for a large number of watches\n\n\n\n\n\n\nMonitoring\n\n\nmntr\n\n\nLists server statistics in Java properties format, suitable as a source for monitoring systems such as Ganglia and Nagios",
            "title": "ZooKeeper"
        },
        {
            "location": "/zookeeper/#zookeeper",
            "text": "ZooKeeper  is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services   Documentation    ZooKeeper    Curator",
            "title": "ZooKeeper"
        },
        {
            "location": "/zookeeper/#setup",
            "text": "Requirements   Base  image   Build  devops/zookeeper  image  # change path\ncd devops/zookeeper\n\n# build image\ndocker build -t devops/zookeeper:latest .\n# build image with specific version - see Dockerfile for version 3.5.x\ndocker build -t devops/zookeeper:3.4.10 --build-arg VERSION=3.4.10 .\n\n# temporary container [host:container]\ndocker run --rm --name zookeeper -p 12181:2181 devops/zookeeper\n# access container\ndocker exec -it zookeeper bash\n\n# paths\n/opt/zookeeper\n/var/log/zookeeper\n/var/lib/zookeeper\n/var/log/supervisord.log\n\n# logs\ntail -F /var/log/supervisord.log\n# check service status\nsupervisorctl status\nsupervisorctl restart zookeeper  Example  docker exec -it zookeeper bash\n\n# (option 1) check zookeeper status\necho ruok | nc localhost 2181\n\n# (option 2) check zookeeper status\ntelnet localhost 2181\n# expect answer imok\n> ruok\n\nzkCli.sh -server 127.0.0.1:2181\nhelp\n# list znodes\nls /\n# create znode and associate value\ncreate /zk_test my_data\n# verify data\nget /zk_test\n# change value\nset /zk_test junk\n# delete znode\ndelete /zk_test",
            "title": "Setup"
        },
        {
            "location": "/zookeeper/#the-four-letter-words",
            "text": "Category  Command  Description      Server status  ruok  Prints  imok  if the server is running and not in an error state     conf  Prints the server configuration (from zoo.cfg)     envi  Prints the server environment, including ZooKeeper version, Java version, and other system properties     srvr  Prints server statistics, including latency statistics, the number of znodes, and the server mode (standalone, leader, or follower)     stat  Prints server statistics and connected clients     srst  Resets server statistics     isro  Shows whether the server is in read-only ( ro ) mode (due to a network partition) or read/write mode (rw)    Client connections  dump  Lists all the sessions and ephemeral znodes for the ensemble. You must connect to the leader (see srvr) for this command     cons  Lists connection statistics for all the server's clients     crst  Resets connection statistics    Watches  wchs  Lists summary information for the server's watches     wchc  Lists all the server's watches by connection, may impact server performance for a large number of watches     wchp  Lists all the server\u2019s watches by znode path, may impact server performance for a large number of watches    Monitoring  mntr  Lists server statistics in Java properties format, suitable as a source for monitoring systems such as Ganglia and Nagios",
            "title": "The four-letter words"
        },
        {
            "location": "/kafka/",
            "text": "Kafka\n\n\n\n\nKafka\n is a distributed streaming platform\n\n\n\n\nDocumentation\n\n\n\n\n\n\nKafka\n\n\n\n\n\n\nKafka: a Distributed Messaging System for Log Processing\n (Paper)\n\n\n\n\n\n\nSchema Registry\n\n\n\n\n\n\nReactive Kafka\n\n\n\n\n\n\nkafkacat\n\n\n\n\n\n\nArchitecture\n\n\n\n\n\n\nKafka is a publish/subscribe messaging system often described as a \ndistributed commit log\n or \ndistributing streaming platform\n\n\n\n\n\n\nThe unit of data is called a \nmessage\n, which is simply an array of bytes and it can have a \nkey\n used to assign partitions. A \nbatch\n is a collection of messages, all of which are being produced to the same topic and partition\n\n\n\n\n\n\nMessages are categorized into \ntopics\n which are additionally broken down into a number of \npartitions\n. Each partition is splitted into \nsegments\n for storage purposes and each segment is stored in a single data file which contains messages and their offsets\n\n\n\n\n\n\nMessages are written in an append-only fashion and are read in order from beginning to end. As a topic typically has multiple partitions, there is no guarantee of message time-ordering across the entire topic, just within a single partition\n\n\n\n\n\n\nIn order to help brokers quickly locate the message for a given offset, Kafka maintains an \nindex\n for each partition. The index maps offsets to segment files and positions within the file\n\n\n\n\n\n\nA \nstream\n is considered to be a single topic of data, regardless of the number of partitions\n\n\n\n\n\n\nProducers\n, publishers or writers, create new messages to a specific topic. By default, the producer does not care what partition a specific message is written to and will balance messages over all partitions of a topic evenly\n\n\n\n\n\n\n\n\n\n\nConsumers\n, subscribers or readers, read messages. The consumer subscribes to one or more topics and reads the messages in the order in which they were produced. The consumer keeps track of which messages it has already consumed by keeping track of the \noffset\n of messages i.e. an integer value that continually\nincreases. Each message in a given partition has a unique offset stored either in Zookeeper or in Kafka itself\n\n\n\n\n\n\n\n\n\n\nConsumers work as part of a \nconsumer group\n, which is one or more consumers that work together to consume a topic. The group assures that each partition is only consumed by one member. The mapping of a consumer to a partition is often called \nownership\n of the partition by the consumer\n\n\n\n\n\n\nWhen a new consumer is added to a group, or when a consumer shuts down or crashes leaving the group, it cause reassignment of partitions to other consumers. Moving partition ownership from one consumer to another is called a \nrebalance\n which provide high availability and scalability\n\n\n\n\n\n\nConsumers maintain membership in a consumer group and ownership of the partitions assigned to them by sending \nheartbeats\n to a Kafka broker designated as the \ngroup coordinator\n\n\n\n\n\n\nYou can't have multiple consumers that belong to the same group in one thread and you can't have multiple threads safely use the same consumer\n\n\n\n\n\n\n\n\n\n\nConsumers must keep polling or they will be considered dead and the partitions they are consuming will be handed to another consumer in the group to continue consuming. Consumers \ncommit\n (track) their offset (position) in each partition to a special \n__consumer_offsets\n topic. If a consumer crashes or a new consumer joins the consumer group, this will trigger a rebalance. After a rebalance, each consumer may be assigned a new set of partitions than the one it processed before. In order to know where to pick up the work, the consumer will read the latest committed offset of each partition and continue from there\n\n\n\n\n\n\n\n\n\n\n\n\nA single Kafka server is called a \nbroker\n. The broker receives messages from producers, assigns offsets to them, and commits the messages to storage on disk. It also services consumers, responding to fetch requests for partitions and responding with the messages that have been committed to disk\n\n\n\n\n\n\nKafka brokers are designed to operate as part of a \ncluster\n. A partition is owned by a single broker in the cluster and that broker is called the \nleader\n of the partition. A partition may be assigned to multiple brokers, which will result in the partition being replicated. All events are produced to and consumed from the \nleader\n replica. Other \nfollower\n replicas just need to stay \nin-sync\n with the leader and replicate all the recent events on time\n\n\n\n\n\n\nKafka uses \nZookeeper\n to maintain the list of brokers that are currently members of a cluster. Every time a broker process starts, it registers itself with a unique identifier by creating an \nephemeral node\n. Kafka uses Zookeeper's ephemeral node feature to elect a \ncontroller\n. The controller is responsible for electing leaders among the partitions and replicas whenever it notices nodes join and leave the cluster\n\n\n\n\n\n\n\n\n\n\n\n\nData in Kafka is organized by topics. Each topic is partitioned and each partition can have multiple \nreplicas\n. Those replicas are stored on brokers and each broker stores replicas belonging to different topics and partitions\n\n\n\n\n\n\nA key feature is that of \nretention\n. Brokers are configured with a default retention setting for topics, either retaining messages for some period of \ntime\n or until the topic reaches a certain \nsize\n in bytes. Once these limits are reached, messages are expired and deleted\n\n\n\n\n\n\nMirrorMaker\n is a tool to coordinates multiple clusters or datacenters and replicate data\n\n\n\n\n\n\nSetup\n\n\nRequirements\n\n\n\n\nBase\n docker image \n\n\nZooKeeper\n docker image\n\n\n\n\nBuild \ndevops/kafka\n image\n\n\n# change path\ncd devops/kafka\n\n# build image\ndocker build -t devops/kafka .\n\n# create network\ndocker network create --driver bridge my_network\ndocker network ls\ndocker network inspect my_network\n\n# start temporary zookeeper container [host:container]\ndocker run --rm \\\n  --name zookeeper \\\n  -p 12181:2181 \\\n  --network=my_network \\\n  devops/zookeeper\n# access container\ndocker exec -it zookeeper bash\n\n# start temporary kafka container [host:container]\ndocker run --rm \\\n  --name kafka \\\n  -p 19092:9092 \\\n  --network=my_network \\\n  -e ZOOKEEPER_HOSTS=\"zookeeper:2181\" \\\n  devops/kafka\n# access container\ndocker exec -it kafka bash\n\n# paths\n/opt/kafka\n/opt/kafka/logs\n/var/lib/kafka/data\n\n# supervisor logs\n/var/log/kafka\n/var/log/connect\ntail -F /var/log/kafka/stdout\nless +G /var/log/connect/stdout\n\n\n\n\nAlternatively use \ndocker-compose\n\n\n# change path\ncd devops/kafka\n\n# build base image\ndocker build -t devops/base ../base\n# build + start zookeeper and kafka\ndocker-compose up\n\n# access container\ndocker exec -it devops-zookeeper bash\ndocker exec -it devops-kafka bash\n\n\n\n\nHow-To\n\n\nKafka\n\n\ndocker exec -it devops-kafka bash\n\n# create topic\nkafka-topics.sh --zookeeper zookeeper:2181 \\\n  --create --if-not-exists --replication-factor 1 --partitions 1 --topic test\n\n# view topic\nkafka-topics.sh --zookeeper zookeeper:2181 --list \nkafka-topics.sh --zookeeper zookeeper:2181 --describe --topic test\nkafka-topics.sh --zookeeper zookeeper:2181 --describe --under-replicated-partitions\nkafka-topics.sh --zookeeper zookeeper:2181 --describe --unavailable-partitions\n\n# produce\nkafka-console-producer.sh --broker-list kafka:9092 --topic test\n# util\nkafkacat -P -b 0 -t test\n\n# consume\nkafka-console-consumer.sh --bootstrap-server kafka:9092 --topic test --from-beginning\n# util\nkafkacat -C -b 0 -t test\n\n# list consumers\nkafka-consumer-groups.sh --bootstrap-server kafka:9092 --list\n# view lag (GROUP_NAME from previous command)\nkafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group GROUP_NAME\n\n# delete\nkafka-topics.sh --zookeeper zookeeper:2181 --delete --topic test\n\n# verify log segment and index\nkafka-run-class.sh kafka.tools.DumpLogSegments \\\n  --files /var/lib/kafka/data/test-0/00000000000000000000.log\nkafka-run-class.sh kafka.tools.DumpLogSegments \\\n  --index-sanity-check \\\n  --files /var/lib/kafka/data/test-0/00000000000000000000.index\n\n# inspect __consumer_offsets\nkafka-console-consumer.sh --bootstrap-server kafka:9092 \\\n  --topic __consumer_offsets \\\n  --formatter \"kafka.coordinator.group.GroupMetadataManager\\$OffsetsMessageFormatter\" \\\n  --max-messages 1\n\n\n\n\nConnect\n\n\ndocker exec -it devops-kafka bash\n\n# verify connect\nhttp :8083\nhttp :8083/connector-plugins\n\n# write file to topic\nhttp POST :8083/connectors \\\n  name=load-kafka-config \\\n  config:='{\"connector.class\":\"FileStreamSource\",\"file\":\"/opt/kafka/config/server.properties\",\"topic\":\"kafka-config-topic\"}'\n\n# verify topic\nkafka-console-consumer.sh --bootstrap-server=kafka:9092 \\\n  --topic kafka-config-topic --from-beginning\n\n# write topic to file\nhttp POST :8083/connectors \\\n  name=dump-kafka-config \\\n  config:='{\"connector.class\":\"FileStreamSink\",\"file\":\"/tmp/copy-of-server-properties\",\"topics\":\"kafka-config-topic\"}'\n\n# verify file\nvim /tmp/copy-of-server-properties\n\n# manage connectors\nhttp :8083/connectors\nhttp DELETE :8083/connectors/dump-kafka-config\n\n\n\n\nZooKeeper\n\n\ndocker exec -it devops-zookeeper bash\n\n# start cli\nzkCli.sh\n\n# view ephemeral nodes\nls /brokers/ids\nget /brokers/ids/0\n\n# view topics\nls /brokers/topics\nget /brokers/topics/test\n\n\n\n\nSchema Registry\n\n\n# docker-hub images\ndocker-compose -f kafka/docker-compose-hub.yml up\ndocker exec -it devops-schema-registry bash\n\n# register new schema\nhttp -v POST :8081/subjects/ExampleSchema/versions \\\n  Accept:application/vnd.schemaregistry.v1+json \\\n  schema='{\"type\":\"string\"}'\n\n# list subjects and schema\nhttp -v :8081/subjects \\\n  Accept:application/vnd.schemaregistry.v1+json\nhttp -v :8081/subjects/ExampleSchema/versions \\\n  Accept:application/vnd.schemaregistry.v1+json\nhttp -v :8081/subjects/ExampleSchema/versions/1 \\\n  Accept:application/vnd.schemaregistry.v1+json\n\n# ui [mac|linux]\n[open|xdg-open] http://localhost:8082",
            "title": "Kafka"
        },
        {
            "location": "/kafka/#kafka",
            "text": "Kafka  is a distributed streaming platform   Documentation    Kafka    Kafka: a Distributed Messaging System for Log Processing  (Paper)    Schema Registry    Reactive Kafka    kafkacat",
            "title": "Kafka"
        },
        {
            "location": "/kafka/#architecture",
            "text": "Kafka is a publish/subscribe messaging system often described as a  distributed commit log  or  distributing streaming platform    The unit of data is called a  message , which is simply an array of bytes and it can have a  key  used to assign partitions. A  batch  is a collection of messages, all of which are being produced to the same topic and partition    Messages are categorized into  topics  which are additionally broken down into a number of  partitions . Each partition is splitted into  segments  for storage purposes and each segment is stored in a single data file which contains messages and their offsets    Messages are written in an append-only fashion and are read in order from beginning to end. As a topic typically has multiple partitions, there is no guarantee of message time-ordering across the entire topic, just within a single partition    In order to help brokers quickly locate the message for a given offset, Kafka maintains an  index  for each partition. The index maps offsets to segment files and positions within the file    A  stream  is considered to be a single topic of data, regardless of the number of partitions    Producers , publishers or writers, create new messages to a specific topic. By default, the producer does not care what partition a specific message is written to and will balance messages over all partitions of a topic evenly      Consumers , subscribers or readers, read messages. The consumer subscribes to one or more topics and reads the messages in the order in which they were produced. The consumer keeps track of which messages it has already consumed by keeping track of the  offset  of messages i.e. an integer value that continually\nincreases. Each message in a given partition has a unique offset stored either in Zookeeper or in Kafka itself      Consumers work as part of a  consumer group , which is one or more consumers that work together to consume a topic. The group assures that each partition is only consumed by one member. The mapping of a consumer to a partition is often called  ownership  of the partition by the consumer    When a new consumer is added to a group, or when a consumer shuts down or crashes leaving the group, it cause reassignment of partitions to other consumers. Moving partition ownership from one consumer to another is called a  rebalance  which provide high availability and scalability    Consumers maintain membership in a consumer group and ownership of the partitions assigned to them by sending  heartbeats  to a Kafka broker designated as the  group coordinator    You can't have multiple consumers that belong to the same group in one thread and you can't have multiple threads safely use the same consumer      Consumers must keep polling or they will be considered dead and the partitions they are consuming will be handed to another consumer in the group to continue consuming. Consumers  commit  (track) their offset (position) in each partition to a special  __consumer_offsets  topic. If a consumer crashes or a new consumer joins the consumer group, this will trigger a rebalance. After a rebalance, each consumer may be assigned a new set of partitions than the one it processed before. In order to know where to pick up the work, the consumer will read the latest committed offset of each partition and continue from there       A single Kafka server is called a  broker . The broker receives messages from producers, assigns offsets to them, and commits the messages to storage on disk. It also services consumers, responding to fetch requests for partitions and responding with the messages that have been committed to disk    Kafka brokers are designed to operate as part of a  cluster . A partition is owned by a single broker in the cluster and that broker is called the  leader  of the partition. A partition may be assigned to multiple brokers, which will result in the partition being replicated. All events are produced to and consumed from the  leader  replica. Other  follower  replicas just need to stay  in-sync  with the leader and replicate all the recent events on time    Kafka uses  Zookeeper  to maintain the list of brokers that are currently members of a cluster. Every time a broker process starts, it registers itself with a unique identifier by creating an  ephemeral node . Kafka uses Zookeeper's ephemeral node feature to elect a  controller . The controller is responsible for electing leaders among the partitions and replicas whenever it notices nodes join and leave the cluster       Data in Kafka is organized by topics. Each topic is partitioned and each partition can have multiple  replicas . Those replicas are stored on brokers and each broker stores replicas belonging to different topics and partitions    A key feature is that of  retention . Brokers are configured with a default retention setting for topics, either retaining messages for some period of  time  or until the topic reaches a certain  size  in bytes. Once these limits are reached, messages are expired and deleted    MirrorMaker  is a tool to coordinates multiple clusters or datacenters and replicate data",
            "title": "Architecture"
        },
        {
            "location": "/kafka/#setup",
            "text": "Requirements   Base  docker image   ZooKeeper  docker image   Build  devops/kafka  image  # change path\ncd devops/kafka\n\n# build image\ndocker build -t devops/kafka .\n\n# create network\ndocker network create --driver bridge my_network\ndocker network ls\ndocker network inspect my_network\n\n# start temporary zookeeper container [host:container]\ndocker run --rm \\\n  --name zookeeper \\\n  -p 12181:2181 \\\n  --network=my_network \\\n  devops/zookeeper\n# access container\ndocker exec -it zookeeper bash\n\n# start temporary kafka container [host:container]\ndocker run --rm \\\n  --name kafka \\\n  -p 19092:9092 \\\n  --network=my_network \\\n  -e ZOOKEEPER_HOSTS=\"zookeeper:2181\" \\\n  devops/kafka\n# access container\ndocker exec -it kafka bash\n\n# paths\n/opt/kafka\n/opt/kafka/logs\n/var/lib/kafka/data\n\n# supervisor logs\n/var/log/kafka\n/var/log/connect\ntail -F /var/log/kafka/stdout\nless +G /var/log/connect/stdout  Alternatively use  docker-compose  # change path\ncd devops/kafka\n\n# build base image\ndocker build -t devops/base ../base\n# build + start zookeeper and kafka\ndocker-compose up\n\n# access container\ndocker exec -it devops-zookeeper bash\ndocker exec -it devops-kafka bash",
            "title": "Setup"
        },
        {
            "location": "/kafka/#how-to",
            "text": "Kafka  docker exec -it devops-kafka bash\n\n# create topic\nkafka-topics.sh --zookeeper zookeeper:2181 \\\n  --create --if-not-exists --replication-factor 1 --partitions 1 --topic test\n\n# view topic\nkafka-topics.sh --zookeeper zookeeper:2181 --list \nkafka-topics.sh --zookeeper zookeeper:2181 --describe --topic test\nkafka-topics.sh --zookeeper zookeeper:2181 --describe --under-replicated-partitions\nkafka-topics.sh --zookeeper zookeeper:2181 --describe --unavailable-partitions\n\n# produce\nkafka-console-producer.sh --broker-list kafka:9092 --topic test\n# util\nkafkacat -P -b 0 -t test\n\n# consume\nkafka-console-consumer.sh --bootstrap-server kafka:9092 --topic test --from-beginning\n# util\nkafkacat -C -b 0 -t test\n\n# list consumers\nkafka-consumer-groups.sh --bootstrap-server kafka:9092 --list\n# view lag (GROUP_NAME from previous command)\nkafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group GROUP_NAME\n\n# delete\nkafka-topics.sh --zookeeper zookeeper:2181 --delete --topic test\n\n# verify log segment and index\nkafka-run-class.sh kafka.tools.DumpLogSegments \\\n  --files /var/lib/kafka/data/test-0/00000000000000000000.log\nkafka-run-class.sh kafka.tools.DumpLogSegments \\\n  --index-sanity-check \\\n  --files /var/lib/kafka/data/test-0/00000000000000000000.index\n\n# inspect __consumer_offsets\nkafka-console-consumer.sh --bootstrap-server kafka:9092 \\\n  --topic __consumer_offsets \\\n  --formatter \"kafka.coordinator.group.GroupMetadataManager\\$OffsetsMessageFormatter\" \\\n  --max-messages 1  Connect  docker exec -it devops-kafka bash\n\n# verify connect\nhttp :8083\nhttp :8083/connector-plugins\n\n# write file to topic\nhttp POST :8083/connectors \\\n  name=load-kafka-config \\\n  config:='{\"connector.class\":\"FileStreamSource\",\"file\":\"/opt/kafka/config/server.properties\",\"topic\":\"kafka-config-topic\"}'\n\n# verify topic\nkafka-console-consumer.sh --bootstrap-server=kafka:9092 \\\n  --topic kafka-config-topic --from-beginning\n\n# write topic to file\nhttp POST :8083/connectors \\\n  name=dump-kafka-config \\\n  config:='{\"connector.class\":\"FileStreamSink\",\"file\":\"/tmp/copy-of-server-properties\",\"topics\":\"kafka-config-topic\"}'\n\n# verify file\nvim /tmp/copy-of-server-properties\n\n# manage connectors\nhttp :8083/connectors\nhttp DELETE :8083/connectors/dump-kafka-config  ZooKeeper  docker exec -it devops-zookeeper bash\n\n# start cli\nzkCli.sh\n\n# view ephemeral nodes\nls /brokers/ids\nget /brokers/ids/0\n\n# view topics\nls /brokers/topics\nget /brokers/topics/test  Schema Registry  # docker-hub images\ndocker-compose -f kafka/docker-compose-hub.yml up\ndocker exec -it devops-schema-registry bash\n\n# register new schema\nhttp -v POST :8081/subjects/ExampleSchema/versions \\\n  Accept:application/vnd.schemaregistry.v1+json \\\n  schema='{\"type\":\"string\"}'\n\n# list subjects and schema\nhttp -v :8081/subjects \\\n  Accept:application/vnd.schemaregistry.v1+json\nhttp -v :8081/subjects/ExampleSchema/versions \\\n  Accept:application/vnd.schemaregistry.v1+json\nhttp -v :8081/subjects/ExampleSchema/versions/1 \\\n  Accept:application/vnd.schemaregistry.v1+json\n\n# ui [mac|linux]\n[open|xdg-open] http://localhost:8082",
            "title": "How-To"
        },
        {
            "location": "/hadoop/",
            "text": "Hadoop\n\n\nThe following guide explains how to provision a Multi Node Hadoop Cluster locally and play with it. Checkout the \nVagrantfile\n and the Vagrant \nguide\n for more details.\n\n\nDocumentation\n\n\n\n\nHadoop\n\n\nThe Hadoop Ecosystem Table\n\n\nHadoop Internals\n\n\n\n\nSetup\n\n\nRequirements\n\n\n\n\nVagrant\n\n\nVirtualBox\n\n\n\n\nDirectory structure\n\n\ntree -a hadoop/\nhadoop/\n\u251c\u2500\u2500 .data # mounted volume\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop_rsa\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop_rsa.pub\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 master\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 mapred\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 yarn\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 namenode\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 secondary\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 spark\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 zeppelin\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 notebook\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 datanode\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 mapred\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u2514\u2500\u2500 yarn\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-3\n\u251c\u2500\u2500 example\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 map-reduce\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 spark\n\u251c\u2500\u2500 file\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 core-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fair-scheduler.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hdfs-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 mapred-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 masters\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 slaves\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 yarn-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-hadoop.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hosts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 motd\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie-env.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 oozie-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-oozie.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 spark\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 log4j.properties\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 spark-env.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-spark.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ssh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 zeppelin\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 zeppelin-env.sh\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 profile-zeppelin.sh\n\u251c\u2500\u2500 script\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bootstrap.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_hadoop.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_oozie.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_spark.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_ubuntu.sh\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 setup_zeppelin.sh\n\u251c\u2500\u2500 Vagrantfile\n\u2514\u2500\u2500 vagrant_hadoop.sh\n\n\n\n\nImport the script\n\n\nsource vagrant_hadoop.sh\n\n\n\n\nCreate and start a Multi Node Hadoop Cluster\n\n\nhadoop-start\n\n\n\n\nThe first time it might take a while\n\n\nAccess the cluster via ssh, check also the \n/etc/hosts\n file\n\n\nvagrant ssh master\nssh hadoop@172.16.0.10 -i .data/hadoop_rsa\n\n# 3 nodes\nvagrant ssh node-1\nssh hadoop@172.16.0.101 -i .data/hadoop_rsa\n\n\n\n\nDestroy the cluster\n\n\nhadoop-destroy\n\n\n\n\nFor convenience add to the host machine\n\n\ncat hadoop/file/hosts | sudo tee --append /etc/hosts\n\n\n\n\nWeb UI links\n\n\n\n\nNameNode: \nhttp://namenode.local:50070\n\n\nNameNode metrics: \nhttp://namenode.local:50070/jmx\n\n\nResourceManager: \nhttp://resource-manager.local:8088\n\n\nLog Level: \nhttp://resource-manager.local:8088/logLevel\n\n\nWeb Application Proxy Server: \nhttp://web-proxy.local:8100/proxy/application_XXX_0000\n\n\nMapReduce Job History Server: \nhttp://history.local:19888\n\n\nDataNode/NodeManager (1): \nhttp://node-1.local:8042/node\n\n\nDataNode/NodeManager (2): \nhttp://node-2.local:8042/node\n\n\nDataNode/NodeManager (3): \nhttp://node-3.local:8042/node\n\n\nSpark: \nhttp://spark.local:4040\n\n\nSpark History Server: \nhttp://spark-history.local:18080\n\n\nZeppelin (*): \nhttp://zeppelin.local:8080\n\n\nOozie (*): \nhttp://oozie.local:11000\n\n\n\n\n(*) Not installed by default\n\n\nHDFS and MapReduce\n\n\n\n\nHDFS\n is a distributed file system that provides high-throughput access to application data\n\n\nYARN\n is a framework for job scheduling and cluster resource management\n\n\nMapReduce\n is a YARN-based system for parallel processing of large data sets\n\n\n\n\nDocumentation\n\n\n\n\nHadoop v2.7.6\n\n\nUntangling Apache Hadoop YARN\n series\n\n\n\n\nAdmin\n\n\nHDFS cli\n\n\n# help\nhdfs\n\n# filesystem statistics\nhdfs dfsadmin -report\n\n# filesystem check\nhdfs fsck /\n\n\n\n\nYARN cli\n\n\n# help\nyarn\n\n# list yarn applications\nyarn application -list\n\n# list nodes\nyarn node -list\n\n# view application logs\nyarn logs -applicationId APPLICATION_ID\n\n# kill yarn application\nyarn application -kill APPLICATION_ID\n\n\n\n\nUseful paths\n\n\n# data and logs\ndevops/hadoop/.data/master/hadoop # host\n/vol/hadoop # guest\n\n# (guest) config\n/usr/local/hadoop/etc/hadoop\n\n# (hdfs) map-reduce history\n/mr-history/history/done_intermediate/hadoop\n\n# (hdfs) aggregated app logs\n/yarn/app/hadoop/logs/application_XXX\n\n\n\n\nMapReduce WordCount Job\n\n\n# build jar on the host machine\ncd devops/hadoop/example/map-reduce\n./gradlew clean build\n\ncd devops/hadoop\nvagrant ssh master\n\n# create base directory using hdfs\nhdfs dfs -mkdir -p /user/ubuntu\n\n# create example directory\nhadoop fs -mkdir -p /user/ubuntu/word-count/input\n\n# list directory\nhadoop fs -ls -h -R /\nhadoop fs -ls -h -R /user/ubuntu\n\n# create sample files\necho \"Hello World Bye World\" > file01\necho \"Hello Hadoop Goodbye Hadoop\" > file02\n\n# copy from local to hdfs\nhadoop fs -copyFromLocal file01 /user/ubuntu/word-count/input\nhadoop fs -put file02 /user/ubuntu/word-count/input\n\n# verify copied files\nhadoop fs -ls -h -R /user/ubuntu\nhadoop fs -cat /user/ubuntu/word-count/input/file01\nhadoop fs -cat /user/ubuntu/word-count/input/file02\nhadoop fs -cat /user/ubuntu/word-count/input/*\n\n# run application\nhadoop jar /vagrant/example/map-reduce/build/libs/map-reduce.jar \\\n  /user/ubuntu/word-count/input \\\n  /user/ubuntu/word-count/output\n\n# check output\nhadoop fs -cat /user/ubuntu/word-count/output/part-r-00000\n\n# delete directory to run it again\nhadoop fs -rm -R /user/ubuntu/word-count/output\n\n# run sample job in a different queue\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  wordcount \\\n  -Dmapreduce.job.queuename=root.priority_queue \\\n  /user/ubuntu/word-count/input \\\n  /user/ubuntu/word-count/output\n\n# well known WARN issue\n# https://issues.apache.org/jira/browse/HDFS-10429\n\n\n\n\nBenchmarking MapReduce with TeraSort\n\n\n# generate random data\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  teragen 1000 random-data\n\n# run terasort benchmark\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  terasort random-data sorted-data\n\n# validate data\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  teravalidate sorted-data report\n\n# useful commands\nhadoop fs -ls -h -R .\nhadoop fs -rm -r random-data\nhadoop fs -cat random-data/part-m-00000\nhadoop fs -cat sorted-data/part-r-00000\n\n\n\n\n\n\nSpark\n\n\n\n\nSpark\n is an open-source cluster-computing framework\n\n\n\n\nDocumentation\n\n\n\n\nSpark\n\n\nHow-to: Tune Your Apache Spark Jobs\n series\n\n\nUnderstanding Resource Allocation configurations for a Spark application\n\n\nMastering Apache Spark\n\n\n\n\n\n\nSpark application on YARN\n\n\n\n\n# start REPL\nspark-shell\npyspark\n\n\n\n\nInteractive Analysis example\n\n\nspark-shell\n# spark shell with yarn\nspark-shell --master yarn --deploy-mode client\n\n# view all configured parameters\nsc.getConf.getAll.foreach(x => println(s\"${x._1}: ${x._2}\"))\n\nval licenceLines = sc.textFile(\"file:/usr/local/spark/LICENSE\")\nval lineCount = licenceLines.count\nval isBsd = (line: String) => line.contains(\"BSD\")\nval bsdLines = licenceLines.filter(isBsd)\nbsdLines.count\nbsdLines.foreach(println)\n\n\n\n\nSpark Job examples\n\n\nExample local\n\n\n# run SparkPi example\nspark-submit \\\n  --class org.apache.spark.examples.SparkPi \\\n  --master local[*] \\\n  $SPARK_HOME/examples/jars/spark-examples_*.jar 10\n\n# GitHub event documentation\n# https://developer.github.com/v3/activity/events/types\n\n# build jar on the host machine\ncd devops/hadoop/example/spark\nsbt clean package\n\ncd devops/hadoop\nvagrant ssh master\n\n# sample dataset\nmkdir -p github-archive && \\\n  cd $_ && \\\n  wget http://data.githubarchive.org/2018-01-01-{0..10}.json.gz && \\\n  gunzip -k *\n# sample line\nhead -n 1 2018-01-01-0.json | jq '.'\n\n# run local job\nspark-submit \\\n  --class \"com.github.niqdev.App\" \\\n  --master local[*] \\\n  /vagrant/example/spark/target/scala-2.11/spark-github_2.11-0.1.0-SNAPSHOT.jar\n\n\n\n\nExample cluster\n\n\n# run job in YARN cluster-deploy mode\nspark-submit \\\n  --class org.apache.spark.examples.SparkPi \\\n  --master yarn \\\n  --deploy-mode cluster \\\n  --driver-memory 4g \\\n  --executor-memory 2g \\\n  --executor-cores 1 \\\n  --queue default \\\n  --conf \"spark.yarn.jars=hdfs://namenode.local:9000/user/spark/share/lib/*.jar\" \\\n  $SPARK_HOME/examples/jars/spark-examples*.jar \\\n  10\n\n\n\n\n\n\nZeppelin\n\n\n\n\nZeppelin\n is a web-based notebook that enables data-driven, interactive data analytics and collaborative documents with SQL, Scala and more\n\n\n\n\nDocumentation\n\n\n\n\nZeppelin\n\n\n\n\nSetup\n\n\nInstall and start Zeppelin\n\n\n# access master node\nvagrant ssh master\n\n# login as root\nsudo su -\n\n# install and init\n/vagrant/script/setup_zeppelin.sh\n\n# start manually (first time only)\nsu --login hadoop /vagrant/script/bootstrap.sh zeppelin\n\n\n\n\nExamples\n\n\n\n\nLearning Spark SQL with Zeppelin\n\n\n\n\n# markdown interpreter\n%md\nhello\n\n# shell interpreter\n%sh\nhadoop fs -ls -h -R /\n\n\n\n\nCluster issue: verify to have enough memory with \nfree -m\n e.g. \nError: Cannot allocate memory\n\n\n\n\nOozie\n\n\n\n\nOozie\n is a workflow scheduler system to manage Hadoop jobs\n\n\n\n\nDocumentation\n\n\n\n\nOozie\n\n\n\n\nSetup\n\n\nOptional PostgreSQL configuration\n - By default Oozie is configured to use Embedded Derby\n\n\n# access master node\nvagrant ssh master\n\n# install docker\ncurl -fsSL get.docker.com -o get-docker.sh && \\\n  chmod u+x $_ && \\\n  ./$_ && \\\n  sudo usermod -aG docker hadoop\n\n# logout and login again to verify docker installation\nexit\nvagrant ssh master\nwhoami # hadoop\ndocker ps -a\n\n# uncomment PostgreSQL configurations\nvim devops/hadoop/file/oozie/config/oozie-site.xml # from host\nvim /vagrant/file/oozie/config/oozie-site.xml # from guest\n\n# start postgres on guest machine \ndocker run \\\n  --detach \\\n  --name oozie-postgres \\\n  -p 5432:5432 \\\n  -e POSTGRES_DB=\"oozie-db\" \\\n  -e POSTGRES_USER=\"postgres\" \\\n  -e POSTGRES_PASSWORD=\"password\" \\\n  postgres\n\n# permission issue\n# https://github.com/docker-library/postgres/issues/116\n# --volume /vol/postgres:/var/lib/postgresql/data\n\n# access container\ndocker exec -it oozie-postgres bash\npsql --username=postgres\n# list all databases\n\\list\n\\connect oozie-db\n# list all tables\n\\dt\n# describe table\n\\d+ wf_jobs\n# list workflow\nselect * from wf_jobs;\n\n\n\n\nInstall and start Oozie\n\n\n# access master node\nvagrant ssh master\n\n# login as root\nsudo su -\n\n# build, install and init\n/vagrant/script/setup_oozie.sh\n\n# start oozie manually (first time only)\nsu --login hadoop /vagrant/script/bootstrap.sh oozie\n\n\n\n\nIt might take a while to build the sources\n\n\nUseful paths\n\n\n# data and logs\ndevops/hadoop/.data/master/oozie # host\n/vol/oozie # guest\n\n# (guest) config\n/usr/local/oozie/conf\n\n# (hdfs) examples\n/user/hadoop/examples\n\n\n\n\nExamples\n\n\nRun bundled examples within distribution\n\n\n# examples path\n.data/master/oozie/examples # host\n/vol/oozie/examples # guest\n\n# access master node as hadoop user\nvagrant ssh master\n\nexport OOZIE_EXAMPLE_PATH=/vol/oozie/examples\nexport OOZIE_HDFS_PATH=/user/$(whoami)/examples\n\n# open map-reduce job.properties\nvim $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties\n\n# edit the following properties\nnameNode=hdfs://namenode.local:9000 # fs.defaultFS @ core-site.xml\njobTracker=resource-manager.local:8032 # yarn.resourcemanager.address @ yarn-site.xml\nqueueName=priority_queue # or default @ fair-scheduler.xml\n\n# upload all the examples\nhadoop fs -put $OOZIE_EXAMPLE_PATH $OOZIE_HDFS_PATH\n\n# verify uploaded files\nhadoop fs -ls -h -R /user/$(whoami)\n\n# run the map-reduce workflow example\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -config $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties \\\n  -run\n\n# verify status\noozie job -oozie http://oozie.local:11000/oozie -info WORKFLOW_ID\n\n# verify result\nhadoop fs -cat $OOZIE_HDFS_PATH/output-data/map-reduce/part-00000\n\n# remove all the examples\nhadoop fs -rm -R $OOZIE_HDFS_PATH\n\n\n\n\nUseful commands\n\n\n\n\nWorkflow requires \noozie.wf.application.path\n property\n\n\nCoordinator requires \noozie.coord.application.path\n property\n\n\n\n\n# verify oozie status\noozie admin \\\n  -oozie http://oozie.local:11000/oozie \\\n  -status\n\n# verify workflow or coordinator status\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -info JOB_ID \\\n  -verbose\n\n# poll workflow or coordinator status\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -poll JOB_ID \\\n  -interval 10 \\\n  -timeout 60 \\\n  -verbose\n\n# find running coordinator\noozie jobs \\\n  -oozie http://oozie.local:11000/oozie/ \\\n  -filter status=RUNNING \\\n  -jobtype coordinator\n\n# suspend|resume|kill coordinator\noozie job \\\n  -oozie http://oozie.local:11000/oozie/ \\\n  [-suspend|-resume|-kill] \\\n  XXX-C\n\n# re-run coordinator's workflow (action)\noozie job \\\n  -oozie http://oozie.local:11000/oozie/ \\\n  -rerun XXX-C \\\n  -action 1,2,3,N\n\n# kill workflow\noozie job \\\n  -oozie http://oozie.local:11000/oozie/ \\\n  -kill \\\n  XXX-W\n\n# re-run all workflow's actions\noozie job \\\n  -oozie http://oozie.local:11000/oozie/ \\\n  -rerun \\\n  XXX-W \\\n  -Doozie.wf.rerun.failnodes=false",
            "title": "Hadoop"
        },
        {
            "location": "/hadoop/#hadoop",
            "text": "The following guide explains how to provision a Multi Node Hadoop Cluster locally and play with it. Checkout the  Vagrantfile  and the Vagrant  guide  for more details.  Documentation   Hadoop  The Hadoop Ecosystem Table  Hadoop Internals",
            "title": "Hadoop"
        },
        {
            "location": "/hadoop/#setup",
            "text": "Requirements   Vagrant  VirtualBox   Directory structure  tree -a hadoop/\nhadoop/\n\u251c\u2500\u2500 .data # mounted volume\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop_rsa\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop_rsa.pub\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 master\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 mapred\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 yarn\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 namenode\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 secondary\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 spark\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 zeppelin\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 notebook\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 datanode\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 mapred\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u2514\u2500\u2500 yarn\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-3\n\u251c\u2500\u2500 example\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 map-reduce\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 spark\n\u251c\u2500\u2500 file\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 core-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fair-scheduler.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hdfs-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 mapred-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 masters\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 slaves\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 yarn-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-hadoop.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hosts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 motd\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie-env.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 oozie-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-oozie.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 spark\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 log4j.properties\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 spark-env.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-spark.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ssh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 zeppelin\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 zeppelin-env.sh\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 profile-zeppelin.sh\n\u251c\u2500\u2500 script\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bootstrap.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_hadoop.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_oozie.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_spark.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_ubuntu.sh\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 setup_zeppelin.sh\n\u251c\u2500\u2500 Vagrantfile\n\u2514\u2500\u2500 vagrant_hadoop.sh  Import the script  source vagrant_hadoop.sh  Create and start a Multi Node Hadoop Cluster  hadoop-start  The first time it might take a while  Access the cluster via ssh, check also the  /etc/hosts  file  vagrant ssh master\nssh hadoop@172.16.0.10 -i .data/hadoop_rsa\n\n# 3 nodes\nvagrant ssh node-1\nssh hadoop@172.16.0.101 -i .data/hadoop_rsa  Destroy the cluster  hadoop-destroy  For convenience add to the host machine  cat hadoop/file/hosts | sudo tee --append /etc/hosts  Web UI links   NameNode:  http://namenode.local:50070  NameNode metrics:  http://namenode.local:50070/jmx  ResourceManager:  http://resource-manager.local:8088  Log Level:  http://resource-manager.local:8088/logLevel  Web Application Proxy Server:  http://web-proxy.local:8100/proxy/application_XXX_0000  MapReduce Job History Server:  http://history.local:19888  DataNode/NodeManager (1):  http://node-1.local:8042/node  DataNode/NodeManager (2):  http://node-2.local:8042/node  DataNode/NodeManager (3):  http://node-3.local:8042/node  Spark:  http://spark.local:4040  Spark History Server:  http://spark-history.local:18080  Zeppelin (*):  http://zeppelin.local:8080  Oozie (*):  http://oozie.local:11000   (*) Not installed by default",
            "title": "Setup"
        },
        {
            "location": "/hadoop/#hdfs-and-mapreduce",
            "text": "HDFS  is a distributed file system that provides high-throughput access to application data  YARN  is a framework for job scheduling and cluster resource management  MapReduce  is a YARN-based system for parallel processing of large data sets   Documentation   Hadoop v2.7.6  Untangling Apache Hadoop YARN  series",
            "title": "HDFS and MapReduce"
        },
        {
            "location": "/hadoop/#admin",
            "text": "HDFS cli  # help\nhdfs\n\n# filesystem statistics\nhdfs dfsadmin -report\n\n# filesystem check\nhdfs fsck /  YARN cli  # help\nyarn\n\n# list yarn applications\nyarn application -list\n\n# list nodes\nyarn node -list\n\n# view application logs\nyarn logs -applicationId APPLICATION_ID\n\n# kill yarn application\nyarn application -kill APPLICATION_ID  Useful paths  # data and logs\ndevops/hadoop/.data/master/hadoop # host\n/vol/hadoop # guest\n\n# (guest) config\n/usr/local/hadoop/etc/hadoop\n\n# (hdfs) map-reduce history\n/mr-history/history/done_intermediate/hadoop\n\n# (hdfs) aggregated app logs\n/yarn/app/hadoop/logs/application_XXX",
            "title": "Admin"
        },
        {
            "location": "/hadoop/#mapreduce-wordcount-job",
            "text": "# build jar on the host machine\ncd devops/hadoop/example/map-reduce\n./gradlew clean build\n\ncd devops/hadoop\nvagrant ssh master\n\n# create base directory using hdfs\nhdfs dfs -mkdir -p /user/ubuntu\n\n# create example directory\nhadoop fs -mkdir -p /user/ubuntu/word-count/input\n\n# list directory\nhadoop fs -ls -h -R /\nhadoop fs -ls -h -R /user/ubuntu\n\n# create sample files\necho \"Hello World Bye World\" > file01\necho \"Hello Hadoop Goodbye Hadoop\" > file02\n\n# copy from local to hdfs\nhadoop fs -copyFromLocal file01 /user/ubuntu/word-count/input\nhadoop fs -put file02 /user/ubuntu/word-count/input\n\n# verify copied files\nhadoop fs -ls -h -R /user/ubuntu\nhadoop fs -cat /user/ubuntu/word-count/input/file01\nhadoop fs -cat /user/ubuntu/word-count/input/file02\nhadoop fs -cat /user/ubuntu/word-count/input/*\n\n# run application\nhadoop jar /vagrant/example/map-reduce/build/libs/map-reduce.jar \\\n  /user/ubuntu/word-count/input \\\n  /user/ubuntu/word-count/output\n\n# check output\nhadoop fs -cat /user/ubuntu/word-count/output/part-r-00000\n\n# delete directory to run it again\nhadoop fs -rm -R /user/ubuntu/word-count/output\n\n# run sample job in a different queue\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  wordcount \\\n  -Dmapreduce.job.queuename=root.priority_queue \\\n  /user/ubuntu/word-count/input \\\n  /user/ubuntu/word-count/output\n\n# well known WARN issue\n# https://issues.apache.org/jira/browse/HDFS-10429",
            "title": "MapReduce WordCount Job"
        },
        {
            "location": "/hadoop/#benchmarking-mapreduce-with-terasort",
            "text": "# generate random data\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  teragen 1000 random-data\n\n# run terasort benchmark\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  terasort random-data sorted-data\n\n# validate data\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  teravalidate sorted-data report\n\n# useful commands\nhadoop fs -ls -h -R .\nhadoop fs -rm -r random-data\nhadoop fs -cat random-data/part-m-00000\nhadoop fs -cat sorted-data/part-r-00000",
            "title": "Benchmarking MapReduce with TeraSort"
        },
        {
            "location": "/hadoop/#spark",
            "text": "Spark  is an open-source cluster-computing framework   Documentation   Spark  How-to: Tune Your Apache Spark Jobs  series  Understanding Resource Allocation configurations for a Spark application  Mastering Apache Spark    Spark application on YARN   # start REPL\nspark-shell\npyspark",
            "title": "Spark"
        },
        {
            "location": "/hadoop/#interactive-analysis-example",
            "text": "spark-shell\n# spark shell with yarn\nspark-shell --master yarn --deploy-mode client\n\n# view all configured parameters\nsc.getConf.getAll.foreach(x => println(s\"${x._1}: ${x._2}\"))\n\nval licenceLines = sc.textFile(\"file:/usr/local/spark/LICENSE\")\nval lineCount = licenceLines.count\nval isBsd = (line: String) => line.contains(\"BSD\")\nval bsdLines = licenceLines.filter(isBsd)\nbsdLines.count\nbsdLines.foreach(println)",
            "title": "Interactive Analysis example"
        },
        {
            "location": "/hadoop/#spark-job-examples",
            "text": "Example local  # run SparkPi example\nspark-submit \\\n  --class org.apache.spark.examples.SparkPi \\\n  --master local[*] \\\n  $SPARK_HOME/examples/jars/spark-examples_*.jar 10\n\n# GitHub event documentation\n# https://developer.github.com/v3/activity/events/types\n\n# build jar on the host machine\ncd devops/hadoop/example/spark\nsbt clean package\n\ncd devops/hadoop\nvagrant ssh master\n\n# sample dataset\nmkdir -p github-archive && \\\n  cd $_ && \\\n  wget http://data.githubarchive.org/2018-01-01-{0..10}.json.gz && \\\n  gunzip -k *\n# sample line\nhead -n 1 2018-01-01-0.json | jq '.'\n\n# run local job\nspark-submit \\\n  --class \"com.github.niqdev.App\" \\\n  --master local[*] \\\n  /vagrant/example/spark/target/scala-2.11/spark-github_2.11-0.1.0-SNAPSHOT.jar  Example cluster  # run job in YARN cluster-deploy mode\nspark-submit \\\n  --class org.apache.spark.examples.SparkPi \\\n  --master yarn \\\n  --deploy-mode cluster \\\n  --driver-memory 4g \\\n  --executor-memory 2g \\\n  --executor-cores 1 \\\n  --queue default \\\n  --conf \"spark.yarn.jars=hdfs://namenode.local:9000/user/spark/share/lib/*.jar\" \\\n  $SPARK_HOME/examples/jars/spark-examples*.jar \\\n  10",
            "title": "Spark Job examples"
        },
        {
            "location": "/hadoop/#zeppelin",
            "text": "Zeppelin  is a web-based notebook that enables data-driven, interactive data analytics and collaborative documents with SQL, Scala and more   Documentation   Zeppelin",
            "title": "Zeppelin"
        },
        {
            "location": "/hadoop/#setup_1",
            "text": "Install and start Zeppelin  # access master node\nvagrant ssh master\n\n# login as root\nsudo su -\n\n# install and init\n/vagrant/script/setup_zeppelin.sh\n\n# start manually (first time only)\nsu --login hadoop /vagrant/script/bootstrap.sh zeppelin",
            "title": "Setup"
        },
        {
            "location": "/hadoop/#examples",
            "text": "Learning Spark SQL with Zeppelin   # markdown interpreter\n%md\nhello\n\n# shell interpreter\n%sh\nhadoop fs -ls -h -R /  Cluster issue: verify to have enough memory with  free -m  e.g.  Error: Cannot allocate memory",
            "title": "Examples"
        },
        {
            "location": "/hadoop/#oozie",
            "text": "Oozie  is a workflow scheduler system to manage Hadoop jobs   Documentation   Oozie",
            "title": "Oozie"
        },
        {
            "location": "/hadoop/#setup_2",
            "text": "Optional PostgreSQL configuration  - By default Oozie is configured to use Embedded Derby  # access master node\nvagrant ssh master\n\n# install docker\ncurl -fsSL get.docker.com -o get-docker.sh && \\\n  chmod u+x $_ && \\\n  ./$_ && \\\n  sudo usermod -aG docker hadoop\n\n# logout and login again to verify docker installation\nexit\nvagrant ssh master\nwhoami # hadoop\ndocker ps -a\n\n# uncomment PostgreSQL configurations\nvim devops/hadoop/file/oozie/config/oozie-site.xml # from host\nvim /vagrant/file/oozie/config/oozie-site.xml # from guest\n\n# start postgres on guest machine \ndocker run \\\n  --detach \\\n  --name oozie-postgres \\\n  -p 5432:5432 \\\n  -e POSTGRES_DB=\"oozie-db\" \\\n  -e POSTGRES_USER=\"postgres\" \\\n  -e POSTGRES_PASSWORD=\"password\" \\\n  postgres\n\n# permission issue\n# https://github.com/docker-library/postgres/issues/116\n# --volume /vol/postgres:/var/lib/postgresql/data\n\n# access container\ndocker exec -it oozie-postgres bash\npsql --username=postgres\n# list all databases\n\\list\n\\connect oozie-db\n# list all tables\n\\dt\n# describe table\n\\d+ wf_jobs\n# list workflow\nselect * from wf_jobs;  Install and start Oozie  # access master node\nvagrant ssh master\n\n# login as root\nsudo su -\n\n# build, install and init\n/vagrant/script/setup_oozie.sh\n\n# start oozie manually (first time only)\nsu --login hadoop /vagrant/script/bootstrap.sh oozie  It might take a while to build the sources  Useful paths  # data and logs\ndevops/hadoop/.data/master/oozie # host\n/vol/oozie # guest\n\n# (guest) config\n/usr/local/oozie/conf\n\n# (hdfs) examples\n/user/hadoop/examples",
            "title": "Setup"
        },
        {
            "location": "/hadoop/#examples_1",
            "text": "Run bundled examples within distribution  # examples path\n.data/master/oozie/examples # host\n/vol/oozie/examples # guest\n\n# access master node as hadoop user\nvagrant ssh master\n\nexport OOZIE_EXAMPLE_PATH=/vol/oozie/examples\nexport OOZIE_HDFS_PATH=/user/$(whoami)/examples\n\n# open map-reduce job.properties\nvim $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties\n\n# edit the following properties\nnameNode=hdfs://namenode.local:9000 # fs.defaultFS @ core-site.xml\njobTracker=resource-manager.local:8032 # yarn.resourcemanager.address @ yarn-site.xml\nqueueName=priority_queue # or default @ fair-scheduler.xml\n\n# upload all the examples\nhadoop fs -put $OOZIE_EXAMPLE_PATH $OOZIE_HDFS_PATH\n\n# verify uploaded files\nhadoop fs -ls -h -R /user/$(whoami)\n\n# run the map-reduce workflow example\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -config $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties \\\n  -run\n\n# verify status\noozie job -oozie http://oozie.local:11000/oozie -info WORKFLOW_ID\n\n# verify result\nhadoop fs -cat $OOZIE_HDFS_PATH/output-data/map-reduce/part-00000\n\n# remove all the examples\nhadoop fs -rm -R $OOZIE_HDFS_PATH",
            "title": "Examples"
        },
        {
            "location": "/hadoop/#useful-commands",
            "text": "Workflow requires  oozie.wf.application.path  property  Coordinator requires  oozie.coord.application.path  property   # verify oozie status\noozie admin \\\n  -oozie http://oozie.local:11000/oozie \\\n  -status\n\n# verify workflow or coordinator status\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -info JOB_ID \\\n  -verbose\n\n# poll workflow or coordinator status\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -poll JOB_ID \\\n  -interval 10 \\\n  -timeout 60 \\\n  -verbose\n\n# find running coordinator\noozie jobs \\\n  -oozie http://oozie.local:11000/oozie/ \\\n  -filter status=RUNNING \\\n  -jobtype coordinator\n\n# suspend|resume|kill coordinator\noozie job \\\n  -oozie http://oozie.local:11000/oozie/ \\\n  [-suspend|-resume|-kill] \\\n  XXX-C\n\n# re-run coordinator's workflow (action)\noozie job \\\n  -oozie http://oozie.local:11000/oozie/ \\\n  -rerun XXX-C \\\n  -action 1,2,3,N\n\n# kill workflow\noozie job \\\n  -oozie http://oozie.local:11000/oozie/ \\\n  -kill \\\n  XXX-W\n\n# re-run all workflow's actions\noozie job \\\n  -oozie http://oozie.local:11000/oozie/ \\\n  -rerun \\\n  XXX-W \\\n  -Doozie.wf.rerun.failnodes=false",
            "title": "Useful commands"
        },
        {
            "location": "/kubernetes/",
            "text": "Kubernetes\n\n\n\n\nKubernetes\n is a system for automating deployment, scaling, and management of containerized applications\n\n\n\n\nDocumentation\n\n\n\n\nKubernetes\n\n\nKubernetes by Example\n\n\n\n\nSetup\n\n\nRequirements\n\n\n\n\nMinikube\n\n\nVirtualBox\n\n\n\n\nLocal cluster\n\n\n# verify installation\nminikube version\n\n# lifecycle\nminikube start --vm-driver=virtualbox\nminikube stop\nminikube delete\n\n# dashboard\nexport NO_PROXY=localhost,127.0.0.1,$(minikube ip)\nminikube dashboard\n\n# access\nminikube ssh\ndocker ps -a\n\n# reuse the minikube's built-in docker daemon\neval $(minikube docker-env)\n\n\n\n\nBasic\n\n\n# verify installation\nkubectl version\n\n# cluster info\nkubectl cluster-info\nkubectl get nodes\nkubectl describe nodes\nkubectl config view\n\n# namespace\nkubectl create namespace local\nkubectl get namespaces\nkubectl config set-context $(kubectl config current-context) --namespace=local\nkubectl config view | grep namespace\nkubectl delete namespace local\n\n\n\n\nSimple deployment\n\n\n# deploy demo app\nkubectl run kubernetes-bootcamp \\\n  --image=gcr.io/google-samples/kubernetes-bootcamp:v1 \\\n  --port=8080 \\\n  --labels='app=kubernetes-bootcamp'\n\n# update app\nkubectl set image deployments/kubernetes-bootcamp \\\n  kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2\n\n# verify update\nkubectl rollout status deployments/kubernetes-bootcamp\nkubectl rollout history deployments/kubernetes-bootcamp\n\n# undo latest deployment\nkubectl rollout undo deployments/kubernetes-bootcamp\n\n# list deployments\nkubectl get deployments\n\n# list containers inside pods\nkubectl describe pods\n\n# list pods\nkubectl get pods\n\n# filter with equality-based labels\nkubectl get pods -l app=kubernetes-bootcamp\n\n# filter with set-based labels\nkubectl get pods -l 'app in (kubernetes-bootcamp)'\n\n\n\n\nPod and Container\n\n\n# proxy cluster (open in 2nd terminal)\nkubectl proxy\n\n# pod name\nexport POD_NAME=$(kubectl get pods -l app=kubernetes-bootcamp -o go-template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}')\necho POD_NAME=$POD_NAME\n\n# verify proxy\nhttp :8001/version\nhttp :8001/api/v1/proxy/namespaces/default/pods/$POD_NAME/\n\n# view logs\nkubectl logs $POD_NAME\n\n# execute command on container\nkubectl exec $POD_NAME printenv\nkubectl exec $POD_NAME ls -- -la\n\n# access container\nkubectl exec -it $POD_NAME bash\n\n# verify label\nkubectl describe pods $POD_NAME\n\n\n\n\nService\n\n\n# list services\nkubectl get services\n\n# create service\nkubectl expose deployment/kubernetes-bootcamp \\\n  --type=\"NodePort\" \\\n  --port 8080\n\n# service info\nkubectl describe services/kubernetes-bootcamp\n\n# expose service\nexport NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}')\necho NODE_PORT=$NODE_PORT\n\n# verify service\ncurl $(minikube ip):$NODE_PORT\n\n# add 4 replicas\nkubectl scale deployments/kubernetes-bootcamp --replicas=4\n\n# info\nkubectl get pod,deployment,service\nkubectl get pods -o wide\nkubectl describe deployments/kubernetes-bootcamp\n\n# cleanup\nkubectl delete deployment,service kubernetes-bootcamp",
            "title": "Kubernetes"
        },
        {
            "location": "/kubernetes/#kubernetes",
            "text": "Kubernetes  is a system for automating deployment, scaling, and management of containerized applications   Documentation   Kubernetes  Kubernetes by Example",
            "title": "Kubernetes"
        },
        {
            "location": "/kubernetes/#setup",
            "text": "Requirements   Minikube  VirtualBox   Local cluster  # verify installation\nminikube version\n\n# lifecycle\nminikube start --vm-driver=virtualbox\nminikube stop\nminikube delete\n\n# dashboard\nexport NO_PROXY=localhost,127.0.0.1,$(minikube ip)\nminikube dashboard\n\n# access\nminikube ssh\ndocker ps -a\n\n# reuse the minikube's built-in docker daemon\neval $(minikube docker-env)  Basic  # verify installation\nkubectl version\n\n# cluster info\nkubectl cluster-info\nkubectl get nodes\nkubectl describe nodes\nkubectl config view\n\n# namespace\nkubectl create namespace local\nkubectl get namespaces\nkubectl config set-context $(kubectl config current-context) --namespace=local\nkubectl config view | grep namespace\nkubectl delete namespace local  Simple deployment  # deploy demo app\nkubectl run kubernetes-bootcamp \\\n  --image=gcr.io/google-samples/kubernetes-bootcamp:v1 \\\n  --port=8080 \\\n  --labels='app=kubernetes-bootcamp'\n\n# update app\nkubectl set image deployments/kubernetes-bootcamp \\\n  kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2\n\n# verify update\nkubectl rollout status deployments/kubernetes-bootcamp\nkubectl rollout history deployments/kubernetes-bootcamp\n\n# undo latest deployment\nkubectl rollout undo deployments/kubernetes-bootcamp\n\n# list deployments\nkubectl get deployments\n\n# list containers inside pods\nkubectl describe pods\n\n# list pods\nkubectl get pods\n\n# filter with equality-based labels\nkubectl get pods -l app=kubernetes-bootcamp\n\n# filter with set-based labels\nkubectl get pods -l 'app in (kubernetes-bootcamp)'  Pod and Container  # proxy cluster (open in 2nd terminal)\nkubectl proxy\n\n# pod name\nexport POD_NAME=$(kubectl get pods -l app=kubernetes-bootcamp -o go-template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}')\necho POD_NAME=$POD_NAME\n\n# verify proxy\nhttp :8001/version\nhttp :8001/api/v1/proxy/namespaces/default/pods/$POD_NAME/\n\n# view logs\nkubectl logs $POD_NAME\n\n# execute command on container\nkubectl exec $POD_NAME printenv\nkubectl exec $POD_NAME ls -- -la\n\n# access container\nkubectl exec -it $POD_NAME bash\n\n# verify label\nkubectl describe pods $POD_NAME  Service  # list services\nkubectl get services\n\n# create service\nkubectl expose deployment/kubernetes-bootcamp \\\n  --type=\"NodePort\" \\\n  --port 8080\n\n# service info\nkubectl describe services/kubernetes-bootcamp\n\n# expose service\nexport NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}')\necho NODE_PORT=$NODE_PORT\n\n# verify service\ncurl $(minikube ip):$NODE_PORT\n\n# add 4 replicas\nkubectl scale deployments/kubernetes-bootcamp --replicas=4\n\n# info\nkubectl get pod,deployment,service\nkubectl get pods -o wide\nkubectl describe deployments/kubernetes-bootcamp\n\n# cleanup\nkubectl delete deployment,service kubernetes-bootcamp",
            "title": "Setup"
        },
        {
            "location": "/readings/",
            "text": "Papers\n\n\n\n\n\n\nThe Google File System\n\n\n\n\n\n\nMapReduce: Simplified Data Processing on Large Clusters\n\n\n\n\n\n\nRaft: In Search of an Understandable Consensus Algorithm\n\n\n\n\n\n\nPaxos Made Simple\n\n\n\n\n\n\nZab: A simple totally ordered broadcast protocol\n\n\n\n\n\n\nThe Chubby lock service for loosely-coupled distributed systems\n\n\n\n\n\n\nSpanner: Google's Globally-Distributed Database\n\n\n\n\n\n\nDynamo: Amazon\u2019s Highly Available Key-value Store\n\n\n\n\n\n\nHyperLogLog in Practice\n\n\n\n\n\n\nDapper, a Large-Scale Distributed Systems Tracing Infrastructure\n\n\n\n\n\n\nLarge-scale cluster management at Google with Borg\n\n\n\n\n\n\nLinearizability: A Correctness Condition for Concurrent Objects\n\n\n\n\n\n\nHarvest, Yield, and Scalable Tolerant Systems\n\n\n\n\n\n\nLife beyond Distributed Transactions\n\n\n\n\n\n\nThe \u03d5 Accrual Failure Detector\n\n\n\n\n\n\nConflict-free Replicated Data Types\n\n\n\n\n\n\nFLP - Impossibility of Distributed Consensus with One Faulty Process\n\n\n\n\n\n\nSEDA: An Architecture for Well-Conditioned, Scalable Internet Services\n\n\n\n\n\n\nMerkle Hash Tree based Techniques for Data Integrity of Outsourced Data\n\n\n\n\n\n\nWhat Every Programmer Should Know About Memory\n\n\n\n\n\n\nStatistically Rigorous Java Performance Evaluation\n\n\n\n\n\n\nPregel: A System for Large-Scale Graph Processing\n\n\n\n\n\n\n\n\nArticles and Resources\n\n\n\n\n\n\nThere is No Now\n\n\n\n\n\n\nJepsen\n\n\n\n\n\n\nCAP Theorem\n\n\n\n\n\n\nBrewer's CAP Theorem\n\n\n\n\n\n\nCAP Twelve Years Later: How the \"Rules\" Have Changed\n\n\n\n\n\n\n\nPlease stop calling databases CP or AP\n\n\n\n\n\n\n\n\nBooks\n\n\n\n\n\n\nHow Linux Works\n (2014)(2nd) by Brian Ward\n\n\n\n\n\n\nDocker in Action\n (2016) by Jeff Nickoloff\n\n\n\n\n\n\nDesigning Data-Intensive Applications\n (2017) by Martin Kleppmann\n\n\n\n\n\n\nHadoop: The Definitive Guide\n (2015)(4th) by Tom White\n\n\n\n\n\n\nSpark in Action\n (2016) by Petar Ze\u010devi\u0107 and Marko Bona\u0107i\n\n\n\n\n\n\nCassandra: The Definitive Guide\n (2016)(4th) by Eben Hewitt, Jeff Carpenter\n\n\n\n\n\n\nKafka: The Definitive Guide\n (2017) by Gwen Shapira, Neha Narkhede, Todd Palino\n\n\n\n\n\n\nAddison-Wesley Professional Computing Series\n\n\n\n\n\n\nScala\n\n\n\n\n\n\nProgramming in Scala\n (2016)(3rd) by Martin Odersky, Lex Spoon, and Bill Venners\n\n\n\n\n\n\nFunctional Programming in Scala\n (2014) by Paul Chiusano and Runar Bjarnason\n\n\n\n\n\n\nAkka in Action\n (2016) by Raymond Roestenburg, Rob Bakker, and Rob Williams",
            "title": "Readings"
        },
        {
            "location": "/readings/#papers",
            "text": "The Google File System    MapReduce: Simplified Data Processing on Large Clusters    Raft: In Search of an Understandable Consensus Algorithm    Paxos Made Simple    Zab: A simple totally ordered broadcast protocol    The Chubby lock service for loosely-coupled distributed systems    Spanner: Google's Globally-Distributed Database    Dynamo: Amazon\u2019s Highly Available Key-value Store    HyperLogLog in Practice    Dapper, a Large-Scale Distributed Systems Tracing Infrastructure    Large-scale cluster management at Google with Borg    Linearizability: A Correctness Condition for Concurrent Objects    Harvest, Yield, and Scalable Tolerant Systems    Life beyond Distributed Transactions    The \u03d5 Accrual Failure Detector    Conflict-free Replicated Data Types    FLP - Impossibility of Distributed Consensus with One Faulty Process    SEDA: An Architecture for Well-Conditioned, Scalable Internet Services    Merkle Hash Tree based Techniques for Data Integrity of Outsourced Data    What Every Programmer Should Know About Memory    Statistically Rigorous Java Performance Evaluation    Pregel: A System for Large-Scale Graph Processing",
            "title": "Papers"
        },
        {
            "location": "/readings/#articles-and-resources",
            "text": "There is No Now    Jepsen    CAP Theorem    Brewer's CAP Theorem    CAP Twelve Years Later: How the \"Rules\" Have Changed    Please stop calling databases CP or AP",
            "title": "Articles and Resources"
        },
        {
            "location": "/readings/#books",
            "text": "How Linux Works  (2014)(2nd) by Brian Ward    Docker in Action  (2016) by Jeff Nickoloff    Designing Data-Intensive Applications  (2017) by Martin Kleppmann    Hadoop: The Definitive Guide  (2015)(4th) by Tom White    Spark in Action  (2016) by Petar Ze\u010devi\u0107 and Marko Bona\u0107i    Cassandra: The Definitive Guide  (2016)(4th) by Eben Hewitt, Jeff Carpenter    Kafka: The Definitive Guide  (2017) by Gwen Shapira, Neha Narkhede, Todd Palino    Addison-Wesley Professional Computing Series    Scala    Programming in Scala  (2016)(3rd) by Martin Odersky, Lex Spoon, and Bill Venners    Functional Programming in Scala  (2014) by Paul Chiusano and Runar Bjarnason    Akka in Action  (2016) by Raymond Roestenburg, Rob Bakker, and Rob Williams",
            "title": "Books"
        },
        {
            "location": "/other/",
            "text": "Online tools\n\n\n\n\nRegular Expressions\n\n\nCurrent Millis\n\n\nJSFiddle\n\n\nScalaFiddle\n\n\nJSON Formatter\n\n\nBeautify JavaScript or HTML\n\n\ndevdocs\n\n\n\n\n\n\nVagrant\n\n\n\n\nVagrant\n is a tool for building and managing virtual machine environments in a single workflow\n\n\n\n\nDocumentation\n\n\n\n\nVagrant\n\n\nVirtualBox\n\n\n\n\nSetup project creating a Vagrantfile\n\n\nvagrant init\n\n\n\n\nBoot and connect to the default virtual machine\n\n\nvagrant up\nvagrant status\nvagrant ssh\n\n\n\n\nUseful commands\n\n\n# shut down gracefully\nvagrant halt\n\n# reload (halt + up) + re-provision\nvagrant reload --provision\n\n# update box\nvagrant box update\nvagrant box list\n\n# delete virtual machine without prompt\nvagrant destory -f\n\n\n\n\n\n\nMkDocs\n\n\n\n\nMkDocs\n is a static site generator\n\n\n\n\nDocumentation\n\n\n\n\nMkDocs\n\n\n\n\nInstall\n\n\npip install mkdocs\nsudo -H pip3 install mkdocs\n\n\n\n\nUseful commands\n\n\n# setup in current directory\nmkdocs new .\n\n# start dev server with hot reload @ http://127.0.0.1:8000\nmkdocs serve\n\n# build static site\nmkdocs build --clean\n\n# deploy to github\nmkdocs gh-deploy\n\n\n\n\n\n\nSDKMAN!\n\n\n\n\nSDKMAN!\n is a tool for managing parallel versions of multiple Software Development Kits on most Unix based systems\n\n\n\n\nDocumentation\n\n\n\n\nSDKMAN!\n\n\n\n\nSetup\n\n\ncurl -s \"https://get.sdkman.io\" | bash\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\nsdk version\n\n\n\n\nGradle\n\n\n# setup\nsdk list gradle\nsdk install gradle 4.4.1\ngradle -version\n\n# create Gradle project\nmkdir -p PROJECT_NAME && cd $_\ngradle init --type java-library\n\n./gradlew clean build\n\n\n\n\nScala\n\n\n# setup sbt\nsdk list sbt\nsdk install sbt\nsbt sbtVersion\nsbt about\n\n# setup scala\nsdk list scala\nsdk install scala 2.11.8\nscala -version\n\n# sample project\nsbt new sbt/scala-seed.g8\n\n\n\n\n\n\nGiter8\n\n\n\n\nGiter8\n is a command line tool to generate files and directories from templates published on GitHub or any other git repository\n\n\n\n\nDocumentation\n\n\n\n\nGiter8\n\n\nTemplates\n\n\n\n\nSetup\n\n\n# install conscript\ncurl https://raw.githubusercontent.com/foundweekends/conscript/master/setup.sh | sh\nsource ~/.bashrc\n\n# install g8\ncs foundweekends/giter8\n\n\n\n\nExample\n\n\n# interactive\ng8 sbt/scala-seed.g8\n# non-interactive\ng8 sbt/scala-seed.g8 --name=my-new-website\n\n\n\n\n\n\nSnap\n\n\nDocumentation\n\n\n\n\nSnap\n\n\n\n\nUseful commands\n\n\n# search\nsnap find gimp\n\n# info\nsnap info gimp\n\n# install\nsnap install gimp\n\n# list installed app\nsnap list\n\n\n\n\n\n\nPython\n\n\nDocumentation\n\n\n\n\npip\n\n\nvirtualenv\n\n\nWhat is the difference between virtualenv | pyenv | virtualenvwrapper | venv ?\n\n\n\n\nSetup\n\n\n# search\napt-get update && apt-cache search python | grep python2\n\n# setup python\napt-get install -y python2.7\napt-get install -y python3\n\n# install pip + setuptools\ncurl https://bootstrap.pypa.io/get-pip.py | python2.7 -\ncurl https://bootstrap.pypa.io/get-pip.py | python3 -\napt install -y python-pip\napt install -y python3-pip\n\n# upgrade pip\npip install -U pip\n\n# install virtualenv globally \npip install virtualenv\n\n\n\n\nvirtualenv\n\n\n# create virtualenv\nvirtualenv venv\nvirtualenv -p python3 venv\nvirtualenv -p $(which python3) venv\n\n# activate virtualenv\nsource venv/bin/activate\n\n# verify virtualenv\nwhich python\npython --version\n\n# deactivate virtualenv\ndeactivate\n\n\n\n\npip\n\n\n# search package\npip search <package>\n\n# install new package\npip install <package>\n\n# update requirements with new packages\npip freeze > requirements.txt\n\n# install all requirements\npip install -r requirements.txt\n\n\n\n\nOther\n\n\n# generate rc file\npylint --generate-rcfile > .pylintrc\n\n# create module\ntouch app/{__init__,main}.py\n\n\n\n\n\n\nGit\n\n\nDocumentation\n\n\n\n\ngit - the simple guide\n\n\ngit notes (1)\n\n\ngit notes (2)\n\n\n\n\n\n\nMercurial\n\n\nDocumentation\n\n\n\n\nA Guide to Branching in Mercurial\n\n\n\n\n# changes since last commit\nhg st\n\n# verify current branch\nhg branch\n\n# lists all branches\nhg branches\n\n# checkout default branch\nhg up default\n\n# pull latest changes\nhg pull -u\n\n# create new branch\nhg branch \"branch-name\"\n\n# track new file\nhg add .\n\n# track new files and untrack removed files\nhg addremove\n\n# commit all tracked files\nhg commit -m \"my-comment\"\n\n# commit specific files\nhg commit FILE_1 FILE_2 -m \"my-comment\"\n\n# commit and track/untrack files (i.e. addremove)\nhg commit -A -m \"my-comment-with-addremove\"\n\n# rename last unpushed commit message\nhg commit -m \"bad-commit-message\"\nhg commit --amend -m \"good-commit-message\"\n\n# discard untracked files\nhg purge\n\n# discard uncommitted local changes\nhg up -C\n\n# discard local uncommitted branch\nhg strip \"branch-name\"\n\n# push commits in all branches\nhg push\n\n# push commits in current branch\nhg push -b .\n\n# create a new branch and push commits in current branch (first time only)\nhg push -b . --new-branch\n\n# lists unpushed commit\nhg outgoing\n\n# change head to specific revision\nhg up -r 12345\n\n# merge default branch on current branch\nhg up default\nhg pull -u\nhg status\nhg up CURRENT-BRANCH\nhg merge default\nhg diff\n\n# remove all resolved conflicts\nrm **/*.orig\n\n# list stashes\nhg shelve --list\n\n# stash\nhg shelve -n \"my-draft\"\n\n# unstash\nhg unshelve \"my-draft\"\n\n# revert/undo last unpushed commit\nhg strip -r -1 --keep\nhg strip --keep --rev .\n\n# solve conflicts manually and then mark it as merged\nhg resolve -m FILE-NAME\n\n# lists commits\nhg log\nhg ls\n\n# pretty log\nhg history --graph --limit 10",
            "title": "Other"
        },
        {
            "location": "/other/#online-tools",
            "text": "Regular Expressions  Current Millis  JSFiddle  ScalaFiddle  JSON Formatter  Beautify JavaScript or HTML  devdocs",
            "title": "Online tools"
        },
        {
            "location": "/other/#vagrant",
            "text": "Vagrant  is a tool for building and managing virtual machine environments in a single workflow   Documentation   Vagrant  VirtualBox   Setup project creating a Vagrantfile  vagrant init  Boot and connect to the default virtual machine  vagrant up\nvagrant status\nvagrant ssh  Useful commands  # shut down gracefully\nvagrant halt\n\n# reload (halt + up) + re-provision\nvagrant reload --provision\n\n# update box\nvagrant box update\nvagrant box list\n\n# delete virtual machine without prompt\nvagrant destory -f",
            "title": "Vagrant"
        },
        {
            "location": "/other/#mkdocs",
            "text": "MkDocs  is a static site generator   Documentation   MkDocs   Install  pip install mkdocs\nsudo -H pip3 install mkdocs  Useful commands  # setup in current directory\nmkdocs new .\n\n# start dev server with hot reload @ http://127.0.0.1:8000\nmkdocs serve\n\n# build static site\nmkdocs build --clean\n\n# deploy to github\nmkdocs gh-deploy",
            "title": "MkDocs"
        },
        {
            "location": "/other/#sdkman",
            "text": "SDKMAN!  is a tool for managing parallel versions of multiple Software Development Kits on most Unix based systems   Documentation   SDKMAN!   Setup  curl -s \"https://get.sdkman.io\" | bash\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\nsdk version  Gradle  # setup\nsdk list gradle\nsdk install gradle 4.4.1\ngradle -version\n\n# create Gradle project\nmkdir -p PROJECT_NAME && cd $_\ngradle init --type java-library\n\n./gradlew clean build  Scala  # setup sbt\nsdk list sbt\nsdk install sbt\nsbt sbtVersion\nsbt about\n\n# setup scala\nsdk list scala\nsdk install scala 2.11.8\nscala -version\n\n# sample project\nsbt new sbt/scala-seed.g8",
            "title": "SDKMAN!"
        },
        {
            "location": "/other/#giter8",
            "text": "Giter8  is a command line tool to generate files and directories from templates published on GitHub or any other git repository   Documentation   Giter8  Templates   Setup  # install conscript\ncurl https://raw.githubusercontent.com/foundweekends/conscript/master/setup.sh | sh\nsource ~/.bashrc\n\n# install g8\ncs foundweekends/giter8  Example  # interactive\ng8 sbt/scala-seed.g8\n# non-interactive\ng8 sbt/scala-seed.g8 --name=my-new-website",
            "title": "Giter8"
        },
        {
            "location": "/other/#snap",
            "text": "Documentation   Snap   Useful commands  # search\nsnap find gimp\n\n# info\nsnap info gimp\n\n# install\nsnap install gimp\n\n# list installed app\nsnap list",
            "title": "Snap"
        },
        {
            "location": "/other/#python",
            "text": "Documentation   pip  virtualenv  What is the difference between virtualenv | pyenv | virtualenvwrapper | venv ?   Setup  # search\napt-get update && apt-cache search python | grep python2\n\n# setup python\napt-get install -y python2.7\napt-get install -y python3\n\n# install pip + setuptools\ncurl https://bootstrap.pypa.io/get-pip.py | python2.7 -\ncurl https://bootstrap.pypa.io/get-pip.py | python3 -\napt install -y python-pip\napt install -y python3-pip\n\n# upgrade pip\npip install -U pip\n\n# install virtualenv globally \npip install virtualenv  virtualenv  # create virtualenv\nvirtualenv venv\nvirtualenv -p python3 venv\nvirtualenv -p $(which python3) venv\n\n# activate virtualenv\nsource venv/bin/activate\n\n# verify virtualenv\nwhich python\npython --version\n\n# deactivate virtualenv\ndeactivate  pip  # search package\npip search <package>\n\n# install new package\npip install <package>\n\n# update requirements with new packages\npip freeze > requirements.txt\n\n# install all requirements\npip install -r requirements.txt  Other  # generate rc file\npylint --generate-rcfile > .pylintrc\n\n# create module\ntouch app/{__init__,main}.py",
            "title": "Python"
        },
        {
            "location": "/other/#git",
            "text": "Documentation   git - the simple guide  git notes (1)  git notes (2)",
            "title": "Git"
        },
        {
            "location": "/other/#mercurial",
            "text": "Documentation   A Guide to Branching in Mercurial   # changes since last commit\nhg st\n\n# verify current branch\nhg branch\n\n# lists all branches\nhg branches\n\n# checkout default branch\nhg up default\n\n# pull latest changes\nhg pull -u\n\n# create new branch\nhg branch \"branch-name\"\n\n# track new file\nhg add .\n\n# track new files and untrack removed files\nhg addremove\n\n# commit all tracked files\nhg commit -m \"my-comment\"\n\n# commit specific files\nhg commit FILE_1 FILE_2 -m \"my-comment\"\n\n# commit and track/untrack files (i.e. addremove)\nhg commit -A -m \"my-comment-with-addremove\"\n\n# rename last unpushed commit message\nhg commit -m \"bad-commit-message\"\nhg commit --amend -m \"good-commit-message\"\n\n# discard untracked files\nhg purge\n\n# discard uncommitted local changes\nhg up -C\n\n# discard local uncommitted branch\nhg strip \"branch-name\"\n\n# push commits in all branches\nhg push\n\n# push commits in current branch\nhg push -b .\n\n# create a new branch and push commits in current branch (first time only)\nhg push -b . --new-branch\n\n# lists unpushed commit\nhg outgoing\n\n# change head to specific revision\nhg up -r 12345\n\n# merge default branch on current branch\nhg up default\nhg pull -u\nhg status\nhg up CURRENT-BRANCH\nhg merge default\nhg diff\n\n# remove all resolved conflicts\nrm **/*.orig\n\n# list stashes\nhg shelve --list\n\n# stash\nhg shelve -n \"my-draft\"\n\n# unstash\nhg unshelve \"my-draft\"\n\n# revert/undo last unpushed commit\nhg strip -r -1 --keep\nhg strip --keep --rev .\n\n# solve conflicts manually and then mark it as merged\nhg resolve -m FILE-NAME\n\n# lists commits\nhg log\nhg ls\n\n# pretty log\nhg history --graph --limit 10",
            "title": "Mercurial"
        }
    ]
}