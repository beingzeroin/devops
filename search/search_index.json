{
    "docs": [
        {
            "location": "/", 
            "text": "DevOps\n\n\nA collection of Docker and Vagrant images, scripts and documentation mainly related to distributed systems for local development, learning purposes and quick prototyping.\n\n\n\n\nLinux\n\n\nDocker\n\n\nAnsible\n\n\nCassandra\n\n\nZookeeper\n\n\nKafka\n\n\nHadoop\n\n\nReadings\n\n\nOther", 
            "title": "Home"
        }, 
        {
            "location": "/#devops", 
            "text": "A collection of Docker and Vagrant images, scripts and documentation mainly related to distributed systems for local development, learning purposes and quick prototyping.   Linux  Docker  Ansible  Cassandra  Zookeeper  Kafka  Hadoop  Readings  Other", 
            "title": "DevOps"
        }, 
        {
            "location": "/linux/", 
            "text": "Linux\n\n\nUseful commands\n\n\n# create nested directories\nmkdir -p parent/child1/child2 \n cd $_\n\n# scroll file from bottom\nless +G /var/log/auth.log\n# follow also if doesn't exist\ntail -F /var/log/auth.log\n\n# find files\nfind /etc -name '*shadow'\n\n# prints lines that match regexp\n# -i case insensitive\n# -v inverts the search\n# -c count lines\ngrep -E '^root' /etc/passwd\n# password encryption\ngrep password.*unix /etc/pam.d/*\n\n# sed = stream editor\n# example substitution\necho -e \na='1st' b='2nd' c='3rd'\\na='4th' b='5th' c='6th'\n \n test.txt\ncat test.txt | sed -nE \ns/^.*a='([^']*).*c='([^']*).*$/\\2\\n\\1/gp\n | sort -r | uniq\n# delete lines three through six\nsed 3,6d /etc/passwd\n\n# pick a single field out of an input stream\nls -l | awk '{print $9}'\n# extract 2nd string\necho \naaa bbb ccc\n \n test.txt\ncat test.txt | awk '{printf(\n2nd: %s\\n\n,$2)}'\n\n# pack archive\ntar cvf archive.tar file1 file2\n# table-of-content\ntar tvf archive.tar\n# unpack archive\ntar xvf archive.tar -C output\n# compress and pack archive\ntar zcvf archive.tar.gz /path/to/images/*.jpg\n# unpack compressed archive\ntar zxvf archive.tar.gz\n\n# pack archive\nzip -r backup.zip file-name directory-name\n# zip with password (prompt)\nzip -e backup.zip file-name\n# unpack jar\nunzip my-lib.jar -d /tmp/my-lib\n\n# count lines\nwc -l file\n\n# lowercase random uuid\nuuidgen | tr \n[:upper:]\n \n[:lower:]\n\n\n# number of bytes\nstat --printf=\n%s\n file\n\n# calculator\necho 1+2 | bc\n# print number in binary base 2 format\necho 'obase=2; 240' | bc\n# reverse-polish calculator\necho '1 2 + p' | dc\n# evaluate expressions\nexpr 1 + 2\n\n# unix timestamp\ndate +%s\n# timestamp in microsecond\ndate +%s%N\n\n# calendar\ncal -3\n\n# configure kernel parameters at runtime\nsysctl\n\n# test conditions ([)\ntest a = a \n echo equal\n\n# create temporary file\nmktemp\n# X is a template\nmktemp /tmp/my-tmp.XXXXXX\n# signal handler to catch the signal that CTRL-C generates and remove the temporary files\nTMPFILE=$(mktemp /tmp/my-tmp.XXXXXX)\ntrap \nrm -f $TMPFILE; exit 1\n INT\n\n# compare files\ndiff FILE1 FILE2\n\n# here document\nDATE=$(date)\ncat \nEOF\nDate: $DATE\nline1\nline2\nEOF\n\n# strip full path and extension if specified e.g. mail\nbasename /var/log/mail.log .log\n\n# image conversion\ngiftopnm\npnmtopng\n\n# when operating on huge number of files to avoid buffer issues\n# e.g. verify file's type\n# INSECURE find . -name '*.md' -print | xargs file\n# change the find output separator and the xargs argument delimiter from a newline to a NULL character\n# two dashes if there is a chance that any of the target files start with a single dash\nfind . -name '*.md' -print0 | xargs -0 file --\n# supply a {} to substitute the filename and a literal ; to indicate the end of the command\nfind . -name '*.md' -exec file {} \\;\n\n# replaces current shell process with the program you name after exec system call\n# after you press CTRL-D or CTRL-C to terminate the cat program,\n# your window should disappear because its child process no longer exists\nexec cat\n\n# subshell example () e.g. path remains the same outside\n(PATH=/bad/invalid:$PATH; echo $PATH)\n# fast way to copy and preserve permissions\ntar cf - orig | (cd target; tar xvf -)\n\n# X Window System\nxwininfo\nxlsclients -l\nxev\nxinput --list\ndbus-monitor --system\ndbus-monitor --session\n\n# compile C program\ncc -o hello hello.c\n# list shared library (so)\nldd /bin/bash\n\n\n\n\nScript templates\n\n\n# shebang\n#!/bin/sh\n#!/bin/bash\n\n# unofficial bash strict mode\nset -euo pipefail\nIFS=$'\\n\\t'\n\n# run from any directory (no symlink allowed)\nCURRENT_PATH=$(cd \n$(dirname \n${BASH_SOURCE[0]}\n)\n; pwd -P)\ncd ${CURRENT_PATH}\n\n# import\n. imported_file.sh\nsource imported_file.sh\n\n# read and store in a variable\nread MY_VAR\necho $MY_VAR\n# read stdin\nread -p \nAre you sure? [y/n]\n -n 1 -r\n\n\n\n\nDiagnostic\n\n\n# sysfs info\nudevadm info --query=all --name=/dev/xvda\n# monitor kernel uevents\nudevadm monitor\n\n# view kernel's boot and runtime diagnostic messages\ndmesg | less\n\n# system logs paths configuration\nvim /etc/rsyslog.conf\nvim /etc/rsyslog.d/50-default.conf\n# test system logger\nlogger -p mail.info mail-message\ntail -n 1 /var/log/syslog\n\n\n\n\nFilesystem\n\n\n# copy data in blocks of a fixed size\n# /dev/zero is a continuous stream of zero bytes\ndd if=/dev/zero of=DUMP_FILE bs=1024 count=1\n\n# view partition table\n# use (g)parted only for partioning disk (supports MBR and GPT)\nsudo parted -l\n\n# create filesystem\nmkfs -t ext4 /dev/PARTITION_NAME\nls -l /sbin/mkfs.*\n\n# list devices and corresponding filesystems UUID\nblkid\n# list attached filesystems\nmount\n# mount device on mount point\nmount -t ext4 /dev/PARTITION_NAME /MOUNT/POINT\n# mount filesystem by its UUID\nmount UUID=xxx-yyy-zzz /MOUNT/POINT\n# make changes permanent after reboot\necho \nUUID=$(sudo blkid -s UUID -o value /dev/PARTITION_NAME)      /MOUNT/POINT   ext4    defaults,nofail        0       2\n | sudo tee -a /etc/fstab\n# mount all filesystems\nmount -a\n# unmount (detach) a filesystem\numount /dev/PARTITION_NAME\n\n# view size and utilization of mounted filesystems\ndf -h\n# disk usage\ndu -sh /*\n\n# check memory and swap size\nfree -h\n\n# (1) create swap file (~1GB)\ndd if=/dev/zero of=/dev/SWAP_FILE bs=1024 count=1024000\n# (2) create swap file (2GB)\nfallocate -l 2G /dev/SWAP_FILE\n# change owner and permissions\nchown root:root /dev/SWAP_FILE\nchmod 0600 /dev/SWAP_FILE\n# put swap signature on partition\nmkswap /dev/SWAP_FILE\n# register space with the kernel\nswapon /dev/SWAP_FILE\n# make changes permanent after reboot\necho \n/dev/SWAP_FILE    none    swap    sw    0   0\n | tee -a /etc/fstab\n# list swap partitions\nswapon --show\n\n# simple static server on port 8000\npython -m SimpleHTTPServer\n\n# copy directory to remote host\nscp -r directory remote_host:~/new-directory\ntar cBvf - directory | ssh remote_host tar xBvpf -\nrsync -az directory remote_host:~/new-\n# equivalent to /*\n# -nv dry run\nrsync -a directory/ remote_host:~/new-directory\n\n\n\n\nMonitoring\n\n\n# list processes\n# m show threads\nps aux\n\n# display current system status\n# Spacebar Updates the display immediately\n# M Sorts by current resident memory usage\n# T Sorts by total (cumulative) CPU usage\n# P Sorts by current CPU usage (the default)\n# u Displays only one user\u2019s processes\n# f Selects different statistics to display\n# ? Displays a usage summary for all top commands\ntop\ntop -p PID1 PID2\n# alternatives\nhtop\natop\n\n# monitor system performance\nvmstat 2\n\n# list open files and the processes using them\nlsof | less\nlsof /dev\n\n# print all the system calls that a process makes\nstrace cat /dev/null\nstrace uptime\n# track shared library calls\nltrace ls /\n\n# CPU usage\n/usr/bin/time ls\n\n# change process priority (-20 \n nice value \n +20)\nrenice 20 PID\n\n# load average: for the past 1 minute, 5 minutes and 15 minutes\nuptime\n\n# check memory status\nfree\ncat /proc/meminfo\n\n# check major/minor page faults\n/usr/bin/time cal \n /dev/null\n\n# show statistics for machine\u2019s current uptime (install sysstat)\niostat\n# show partition information\niostat -p ALL\n\n# show I/O resources used by individual processes\niotop\n\n# see the resource consumption of a process over time\npidstat -p PID 1\n\n# reports CPU and IO stats\niostat -mt 2\n\n# system resource statistics\ndstat\n\n\n\n\nNetwork\n\n\n# active network interfaces\nifconfig\n# enable/disable network interface\nifconfig NETWORK_INTERFACE up\nifconfig NETWORK_INTERFACE down\n\n# show routing table\n# Destination: network prefix e.g. 0.0.0.0/0 matches every address (default route)\n# flag U: up\n# flag G: gateway\n# convention: the router is usually at address 1 of the subnet\nroute -n\n\n# ICMP echo request\n# icmp_req: verify order and no gap\n# time: round-trip time\nping -c 3 8.8.8.8\n\n# show path packets take to a remote host\ntraceroute 8.8.8.8\n\n# (DNS) find the IP address behind a domain name\nhost www.github.com\n\n# network manager\nnmcli\nnmcli device show\n# returns zero as its exit code if network is up\nnm-online\n# network details e.g. ssid/password\ncat /etc/NetworkManager/system-connections/NETWORK_NAME\n\n# override hostname lookups\nvim /etc/hosts\n\n# traditional configuration file for DNS servers\ncat /etc/resolv.conf\n# DNS settings\ncat /etc/nsswitch.conf\n\n# static IP\n/etc/network/interfaces\n\n# -t Prints TCP port information\n# -u Prints UDP port information\n# -l Prints listening ports\n# -a Prints every active port\n# -n Disables name lookups (speeds things up; also useful if DNS isn\u2019t working)\n# list open TCP connections\nnetstat -nt\n# print listening TCP ports\nnetstat -ntl\n# list running services\nnetstat -plunt\n\n# processes listening on open TCP ports\nlsof -i -n -P | grep TCP\nlsof -iTCP -sTCP:LISTEN\n# process running on specific port\nlsof -n -i:PORT_NUMBER\n# list unix domain socket\nlsof -U\n\n# well-known ports\ncat /etc/services\n\n# release IP with DHCP\ndhclient -r NETWORK_INTERFACE_NAME\n# renew IP\ndhclient -v NETWORK_INTERFACE_NAME\n\n# public IP via external services\nhttp ident.me\nhttp ipv4.ident.me\nhttp ipv6.ident.me\nhttp icanhazip.com\nhttp ipv4.icanhazip.com\nhttp ipv6.icanhazip.com\n\n# Linux kernel does not automatically move packets from one subnet to another\n# enable temporary IP forwarding in the router's kernel\nsysctl -w net.ipv4.ip_forward\n# change permanent configs upon reboot\nvim /etc/sysctl.conf\n\n# example NAT (IP masquerading)\nsysctl -w net.ipv4.ip_forward\niptables -P FORWARD DROP\niptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE\niptables -A FORWARD -i eth0 -o eth1 -m state --state ESTABLISHED,RELATED -j ACCEPT\niptables -A FORWARD -i eth1 -o eth0 -j ACCEPT\n\n# firewalling on individual machines is sometimes called IP filtering\n# firewall rules in series or chain make up a table\n# INPUT chain: protect individual machine\n# FORWARD chain: protect a network of machines\n# show iptable configuration\niptables -L\n# block IP\niptables -A INPUT -s BLOCKED_IP -j DROP\n# block IP/port\niptables -A INPUT -s BLOCKED_IP/CIDR -p tcp --destination-port BLOCKED_PORT -j DROP\n# allow IP (insert at the bottom)\niptables -A INPUT -s ALLOWED_IP -j ACCEPT\n# allow IP (insert at the top)\niptables -I INPUT -s ALLOWED_IP -j ACCEPT\n# allow IP (specify order)\niptables -I INPUT RULE_NUMBER -s ALLOWED_IP -j ACCEPT\n# delete rule #number in chain\niptables -D INPUT RULE_NUMBER\n\n# show ARP kernel cache\narp -n\n\n# list wireless network\niw dev NETWORK_INTERFACE scan\n# show details of connected network\niw dev NETWORK_INTERFACE link\n\n# manage both authentication and encryption for a wireless network interface\nwpa_supplicant\n\n\n\n\nApplications\n\n\n# old insecure\ntelnet www.wikipedia.org 80\n# press enter twice after\nGET / HTTP/1.0\n\n# details about communication\ncurl --trace-ascii trace_file https://www.wikipedia.org \n /dev/null\nvim trace_file\n\n# sshd server configs\nvim /etc/ssh/sshd_config\n# generate key pair\nssh-keygen -t rsa -b 4096 -C KEY_NAME -N \nPASSPHRASE\n -f KEY_PATH\n\n# list network interfaces\ntcpdump -D\n# sniff hex and ascii (-A) by interface/host/port\ntcpdump -XX -n -i INTERFACE_NAME tcp and host IP_ADDRESS and port PORT_NUMBER\n\n# Swiss Army knife\n# banner grabbing\ncat \n(echo HEAD / HTTP/1.0) - | netcat IP_ADDRESS PORT_NUMBER\n# install traditional (with -e option)\napt-get install netcat -y\n# choose /bin/nc.traditional\nupdate-alternatives --config nc\n# listen on server\nnetcat -l -p 6996 -e /bin/bash\n# run client\ncat \n(echo ls -la) - | netcat IP_ADDRESS 6996\n\n# scan open ports\nnmap -Pn IP_ADDRESS\n\n# other\n# * ssh/scp/sftp/rsync\n# * curl/wget\n\n# system calls\nman recv\nman send\n\n\n\n\nUseful links\n\n\n\n\nSubnetting\n\n\nOpenWrt\n\n\nBusyBox\n\n\nNetfilter\n\n\nIptables Essentials\n\n\niptables vs nftables\n\n\nShorewall\n\n\nNmap\n\n\ntshark\n\n\nPostfix\n\n\nHTTPie\n\n\njq\n\n\nperf-tools\n\n\nSamba\n\n\nProgram Library HOWTO", 
            "title": "Linux"
        }, 
        {
            "location": "/linux/#linux", 
            "text": "", 
            "title": "Linux"
        }, 
        {
            "location": "/linux/#useful-commands", 
            "text": "# create nested directories\nmkdir -p parent/child1/child2   cd $_\n\n# scroll file from bottom\nless +G /var/log/auth.log\n# follow also if doesn't exist\ntail -F /var/log/auth.log\n\n# find files\nfind /etc -name '*shadow'\n\n# prints lines that match regexp\n# -i case insensitive\n# -v inverts the search\n# -c count lines\ngrep -E '^root' /etc/passwd\n# password encryption\ngrep password.*unix /etc/pam.d/*\n\n# sed = stream editor\n# example substitution\necho -e  a='1st' b='2nd' c='3rd'\\na='4th' b='5th' c='6th'    test.txt\ncat test.txt | sed -nE  s/^.*a='([^']*).*c='([^']*).*$/\\2\\n\\1/gp  | sort -r | uniq\n# delete lines three through six\nsed 3,6d /etc/passwd\n\n# pick a single field out of an input stream\nls -l | awk '{print $9}'\n# extract 2nd string\necho  aaa bbb ccc    test.txt\ncat test.txt | awk '{printf( 2nd: %s\\n ,$2)}'\n\n# pack archive\ntar cvf archive.tar file1 file2\n# table-of-content\ntar tvf archive.tar\n# unpack archive\ntar xvf archive.tar -C output\n# compress and pack archive\ntar zcvf archive.tar.gz /path/to/images/*.jpg\n# unpack compressed archive\ntar zxvf archive.tar.gz\n\n# pack archive\nzip -r backup.zip file-name directory-name\n# zip with password (prompt)\nzip -e backup.zip file-name\n# unpack jar\nunzip my-lib.jar -d /tmp/my-lib\n\n# count lines\nwc -l file\n\n# lowercase random uuid\nuuidgen | tr  [:upper:]   [:lower:] \n\n# number of bytes\nstat --printf= %s  file\n\n# calculator\necho 1+2 | bc\n# print number in binary base 2 format\necho 'obase=2; 240' | bc\n# reverse-polish calculator\necho '1 2 + p' | dc\n# evaluate expressions\nexpr 1 + 2\n\n# unix timestamp\ndate +%s\n# timestamp in microsecond\ndate +%s%N\n\n# calendar\ncal -3\n\n# configure kernel parameters at runtime\nsysctl\n\n# test conditions ([)\ntest a = a   echo equal\n\n# create temporary file\nmktemp\n# X is a template\nmktemp /tmp/my-tmp.XXXXXX\n# signal handler to catch the signal that CTRL-C generates and remove the temporary files\nTMPFILE=$(mktemp /tmp/my-tmp.XXXXXX)\ntrap  rm -f $TMPFILE; exit 1  INT\n\n# compare files\ndiff FILE1 FILE2\n\n# here document\nDATE=$(date)\ncat  EOF\nDate: $DATE\nline1\nline2\nEOF\n\n# strip full path and extension if specified e.g. mail\nbasename /var/log/mail.log .log\n\n# image conversion\ngiftopnm\npnmtopng\n\n# when operating on huge number of files to avoid buffer issues\n# e.g. verify file's type\n# INSECURE find . -name '*.md' -print | xargs file\n# change the find output separator and the xargs argument delimiter from a newline to a NULL character\n# two dashes if there is a chance that any of the target files start with a single dash\nfind . -name '*.md' -print0 | xargs -0 file --\n# supply a {} to substitute the filename and a literal ; to indicate the end of the command\nfind . -name '*.md' -exec file {} \\;\n\n# replaces current shell process with the program you name after exec system call\n# after you press CTRL-D or CTRL-C to terminate the cat program,\n# your window should disappear because its child process no longer exists\nexec cat\n\n# subshell example () e.g. path remains the same outside\n(PATH=/bad/invalid:$PATH; echo $PATH)\n# fast way to copy and preserve permissions\ntar cf - orig | (cd target; tar xvf -)\n\n# X Window System\nxwininfo\nxlsclients -l\nxev\nxinput --list\ndbus-monitor --system\ndbus-monitor --session\n\n# compile C program\ncc -o hello hello.c\n# list shared library (so)\nldd /bin/bash  Script templates  # shebang\n#!/bin/sh\n#!/bin/bash\n\n# unofficial bash strict mode\nset -euo pipefail\nIFS=$'\\n\\t'\n\n# run from any directory (no symlink allowed)\nCURRENT_PATH=$(cd  $(dirname  ${BASH_SOURCE[0]} ) ; pwd -P)\ncd ${CURRENT_PATH}\n\n# import\n. imported_file.sh\nsource imported_file.sh\n\n# read and store in a variable\nread MY_VAR\necho $MY_VAR\n# read stdin\nread -p  Are you sure? [y/n]  -n 1 -r", 
            "title": "Useful commands"
        }, 
        {
            "location": "/linux/#diagnostic", 
            "text": "# sysfs info\nudevadm info --query=all --name=/dev/xvda\n# monitor kernel uevents\nudevadm monitor\n\n# view kernel's boot and runtime diagnostic messages\ndmesg | less\n\n# system logs paths configuration\nvim /etc/rsyslog.conf\nvim /etc/rsyslog.d/50-default.conf\n# test system logger\nlogger -p mail.info mail-message\ntail -n 1 /var/log/syslog", 
            "title": "Diagnostic"
        }, 
        {
            "location": "/linux/#filesystem", 
            "text": "# copy data in blocks of a fixed size\n# /dev/zero is a continuous stream of zero bytes\ndd if=/dev/zero of=DUMP_FILE bs=1024 count=1\n\n# view partition table\n# use (g)parted only for partioning disk (supports MBR and GPT)\nsudo parted -l\n\n# create filesystem\nmkfs -t ext4 /dev/PARTITION_NAME\nls -l /sbin/mkfs.*\n\n# list devices and corresponding filesystems UUID\nblkid\n# list attached filesystems\nmount\n# mount device on mount point\nmount -t ext4 /dev/PARTITION_NAME /MOUNT/POINT\n# mount filesystem by its UUID\nmount UUID=xxx-yyy-zzz /MOUNT/POINT\n# make changes permanent after reboot\necho  UUID=$(sudo blkid -s UUID -o value /dev/PARTITION_NAME)      /MOUNT/POINT   ext4    defaults,nofail        0       2  | sudo tee -a /etc/fstab\n# mount all filesystems\nmount -a\n# unmount (detach) a filesystem\numount /dev/PARTITION_NAME\n\n# view size and utilization of mounted filesystems\ndf -h\n# disk usage\ndu -sh /*\n\n# check memory and swap size\nfree -h\n\n# (1) create swap file (~1GB)\ndd if=/dev/zero of=/dev/SWAP_FILE bs=1024 count=1024000\n# (2) create swap file (2GB)\nfallocate -l 2G /dev/SWAP_FILE\n# change owner and permissions\nchown root:root /dev/SWAP_FILE\nchmod 0600 /dev/SWAP_FILE\n# put swap signature on partition\nmkswap /dev/SWAP_FILE\n# register space with the kernel\nswapon /dev/SWAP_FILE\n# make changes permanent after reboot\necho  /dev/SWAP_FILE    none    swap    sw    0   0  | tee -a /etc/fstab\n# list swap partitions\nswapon --show\n\n# simple static server on port 8000\npython -m SimpleHTTPServer\n\n# copy directory to remote host\nscp -r directory remote_host:~/new-directory\ntar cBvf - directory | ssh remote_host tar xBvpf -\nrsync -az directory remote_host:~/new-\n# equivalent to /*\n# -nv dry run\nrsync -a directory/ remote_host:~/new-directory", 
            "title": "Filesystem"
        }, 
        {
            "location": "/linux/#monitoring", 
            "text": "# list processes\n# m show threads\nps aux\n\n# display current system status\n# Spacebar Updates the display immediately\n# M Sorts by current resident memory usage\n# T Sorts by total (cumulative) CPU usage\n# P Sorts by current CPU usage (the default)\n# u Displays only one user\u2019s processes\n# f Selects different statistics to display\n# ? Displays a usage summary for all top commands\ntop\ntop -p PID1 PID2\n# alternatives\nhtop\natop\n\n# monitor system performance\nvmstat 2\n\n# list open files and the processes using them\nlsof | less\nlsof /dev\n\n# print all the system calls that a process makes\nstrace cat /dev/null\nstrace uptime\n# track shared library calls\nltrace ls /\n\n# CPU usage\n/usr/bin/time ls\n\n# change process priority (-20   nice value   +20)\nrenice 20 PID\n\n# load average: for the past 1 minute, 5 minutes and 15 minutes\nuptime\n\n# check memory status\nfree\ncat /proc/meminfo\n\n# check major/minor page faults\n/usr/bin/time cal   /dev/null\n\n# show statistics for machine\u2019s current uptime (install sysstat)\niostat\n# show partition information\niostat -p ALL\n\n# show I/O resources used by individual processes\niotop\n\n# see the resource consumption of a process over time\npidstat -p PID 1\n\n# reports CPU and IO stats\niostat -mt 2\n\n# system resource statistics\ndstat", 
            "title": "Monitoring"
        }, 
        {
            "location": "/linux/#network", 
            "text": "# active network interfaces\nifconfig\n# enable/disable network interface\nifconfig NETWORK_INTERFACE up\nifconfig NETWORK_INTERFACE down\n\n# show routing table\n# Destination: network prefix e.g. 0.0.0.0/0 matches every address (default route)\n# flag U: up\n# flag G: gateway\n# convention: the router is usually at address 1 of the subnet\nroute -n\n\n# ICMP echo request\n# icmp_req: verify order and no gap\n# time: round-trip time\nping -c 3 8.8.8.8\n\n# show path packets take to a remote host\ntraceroute 8.8.8.8\n\n# (DNS) find the IP address behind a domain name\nhost www.github.com\n\n# network manager\nnmcli\nnmcli device show\n# returns zero as its exit code if network is up\nnm-online\n# network details e.g. ssid/password\ncat /etc/NetworkManager/system-connections/NETWORK_NAME\n\n# override hostname lookups\nvim /etc/hosts\n\n# traditional configuration file for DNS servers\ncat /etc/resolv.conf\n# DNS settings\ncat /etc/nsswitch.conf\n\n# static IP\n/etc/network/interfaces\n\n# -t Prints TCP port information\n# -u Prints UDP port information\n# -l Prints listening ports\n# -a Prints every active port\n# -n Disables name lookups (speeds things up; also useful if DNS isn\u2019t working)\n# list open TCP connections\nnetstat -nt\n# print listening TCP ports\nnetstat -ntl\n# list running services\nnetstat -plunt\n\n# processes listening on open TCP ports\nlsof -i -n -P | grep TCP\nlsof -iTCP -sTCP:LISTEN\n# process running on specific port\nlsof -n -i:PORT_NUMBER\n# list unix domain socket\nlsof -U\n\n# well-known ports\ncat /etc/services\n\n# release IP with DHCP\ndhclient -r NETWORK_INTERFACE_NAME\n# renew IP\ndhclient -v NETWORK_INTERFACE_NAME\n\n# public IP via external services\nhttp ident.me\nhttp ipv4.ident.me\nhttp ipv6.ident.me\nhttp icanhazip.com\nhttp ipv4.icanhazip.com\nhttp ipv6.icanhazip.com\n\n# Linux kernel does not automatically move packets from one subnet to another\n# enable temporary IP forwarding in the router's kernel\nsysctl -w net.ipv4.ip_forward\n# change permanent configs upon reboot\nvim /etc/sysctl.conf\n\n# example NAT (IP masquerading)\nsysctl -w net.ipv4.ip_forward\niptables -P FORWARD DROP\niptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE\niptables -A FORWARD -i eth0 -o eth1 -m state --state ESTABLISHED,RELATED -j ACCEPT\niptables -A FORWARD -i eth1 -o eth0 -j ACCEPT\n\n# firewalling on individual machines is sometimes called IP filtering\n# firewall rules in series or chain make up a table\n# INPUT chain: protect individual machine\n# FORWARD chain: protect a network of machines\n# show iptable configuration\niptables -L\n# block IP\niptables -A INPUT -s BLOCKED_IP -j DROP\n# block IP/port\niptables -A INPUT -s BLOCKED_IP/CIDR -p tcp --destination-port BLOCKED_PORT -j DROP\n# allow IP (insert at the bottom)\niptables -A INPUT -s ALLOWED_IP -j ACCEPT\n# allow IP (insert at the top)\niptables -I INPUT -s ALLOWED_IP -j ACCEPT\n# allow IP (specify order)\niptables -I INPUT RULE_NUMBER -s ALLOWED_IP -j ACCEPT\n# delete rule #number in chain\niptables -D INPUT RULE_NUMBER\n\n# show ARP kernel cache\narp -n\n\n# list wireless network\niw dev NETWORK_INTERFACE scan\n# show details of connected network\niw dev NETWORK_INTERFACE link\n\n# manage both authentication and encryption for a wireless network interface\nwpa_supplicant", 
            "title": "Network"
        }, 
        {
            "location": "/linux/#applications", 
            "text": "# old insecure\ntelnet www.wikipedia.org 80\n# press enter twice after\nGET / HTTP/1.0\n\n# details about communication\ncurl --trace-ascii trace_file https://www.wikipedia.org   /dev/null\nvim trace_file\n\n# sshd server configs\nvim /etc/ssh/sshd_config\n# generate key pair\nssh-keygen -t rsa -b 4096 -C KEY_NAME -N  PASSPHRASE  -f KEY_PATH\n\n# list network interfaces\ntcpdump -D\n# sniff hex and ascii (-A) by interface/host/port\ntcpdump -XX -n -i INTERFACE_NAME tcp and host IP_ADDRESS and port PORT_NUMBER\n\n# Swiss Army knife\n# banner grabbing\ncat  (echo HEAD / HTTP/1.0) - | netcat IP_ADDRESS PORT_NUMBER\n# install traditional (with -e option)\napt-get install netcat -y\n# choose /bin/nc.traditional\nupdate-alternatives --config nc\n# listen on server\nnetcat -l -p 6996 -e /bin/bash\n# run client\ncat  (echo ls -la) - | netcat IP_ADDRESS 6996\n\n# scan open ports\nnmap -Pn IP_ADDRESS\n\n# other\n# * ssh/scp/sftp/rsync\n# * curl/wget\n\n# system calls\nman recv\nman send", 
            "title": "Applications"
        }, 
        {
            "location": "/linux/#useful-links", 
            "text": "Subnetting  OpenWrt  BusyBox  Netfilter  Iptables Essentials  iptables vs nftables  Shorewall  Nmap  tshark  Postfix  HTTPie  jq  perf-tools  Samba  Program Library HOWTO", 
            "title": "Useful links"
        }, 
        {
            "location": "/docker/", 
            "text": "Docker\n\n\n\n\nDocker\n is an open platform for developers and sysadmins to build, ship, and run distributed applications\n\n\n\n\nDocumentation\n\n\n\n\nDocker\n\n\n\n\nHow-To\n\n\nSetup\n\n\n# install docker\ncurl -fsSL get.docker.com -o get-docker.sh \n \\\n  chmod u+x $_ \n \\\n  ./$_ \n \\\n  sudo usermod -aG docker hadoop\n\ndocker --version\n\n# install docker-compose\nsudo curl -L https://github.com/docker/compose/releases/download/1.19.0/docker-compose-`uname -s`-`uname -m` \\\n  -o /usr/local/bin/docker-compose \n \\\n  sudo chmod +x /usr/local/bin/docker-compose\n\ndocker-compose --version\n\n# install docker-machine (VirtualBox required)\ncurl -L https://github.com/docker/machine/releases/download/v0.13.0/docker-machine-`uname -s`-`uname -m` \n/tmp/docker-machine \n \\\n  sudo install /tmp/docker-machine /usr/local/bin/docker-machine\n\ndocker-machine --version\n\n\n\n\nUseful commands\n\n\n# list images\ndocker images\n# list containers\ndocker ps -a\n# list volumes\ndocker volume ls\n\n# run temporary container\ndocker run --rm --name phusion phusion/baseimage:latest\n# access container from another shell\ndocker exec -it phusion bash\n\n# remove container by name\ndocker ps -a -q -f name=CONTAINER_NAME | xargs --no-run-if-empty docker rm -f\n# delete dangling images \nnone\n\ndocker images -q -f dangling=true | xargs --no-run-if-empty docker rmi\n# delete dangling volumes\ndocker volume ls -q -f dangling=true | xargs --no-run-if-empty docker volume rm\n\n\n\n\nDocker Machine\n\n\n# create local machine\ndocker-machine create --driver virtualbox default\n\n# list\ndocker-machine ls\ndocker-machine ls --filter name=default\ndocker-machine ls --filter state=Running\ndocker-machine ls --format \n{{.Name}}: {{.DriverName}} - {{.State}}\n\n\n# info\ndocker-machine inspect default\ndocker-machine inspect --format='{{.Driver.IPAddress}}' default\ndocker-machine status default\ndocker-machine ip default\n\n# management\ndocker-machine start default\ndocker-machine stop default\ndocker-machine restart default\ndocker-machine rm default\n\n# mount volume\n#https://docs.docker.com/machine/reference/mount\n\n# show command to connect to machine\ndocker-machine env default\n# check if variables are set\nenv | grep DOCKER\n\n# connect to machine\neval \n$(docker-machine env default)\n\ndocker ps -a\n\n# show command to disconnect from machine\ndocker-machine env -u\n# unset all\neval $(docker-machine env -u)\n\n# access\ndocker-machine ssh default\n# execute command and exit\ndocker-machine ssh default uptime\n# copy files from host to guest\ndocker-machine scp -r /FROM default:/TO\n\n# start nginx on default machine\ndocker run -d -p 8000:80 nginx\n# verify from host\ncurl $(docker-machine ip default):8000\n# forward to port 8080\ndocker-machine ssh default -L 8080:localhost:8000\n# verify tunnel from host\ncurl localhost:8080\n\n# disable error crash reporting\nmkdir -p ~/.docker/machine \n touch ~/.docker/machine/no-error-report\n\n\n\n\nBase image\n\n\nBuild \ndevops/base\n image\n\n\n# change path\ncd devops/base\n\n# build image\ndocker build -t devops/base .\n\n# temporary container\ndocker run --rm --name devops-base devops/base\n# access container\ndocker exec -it devops-base bash", 
            "title": "Docker"
        }, 
        {
            "location": "/docker/#docker", 
            "text": "Docker  is an open platform for developers and sysadmins to build, ship, and run distributed applications   Documentation   Docker", 
            "title": "Docker"
        }, 
        {
            "location": "/docker/#how-to", 
            "text": "Setup  # install docker\ncurl -fsSL get.docker.com -o get-docker.sh   \\\n  chmod u+x $_   \\\n  ./$_   \\\n  sudo usermod -aG docker hadoop\n\ndocker --version\n\n# install docker-compose\nsudo curl -L https://github.com/docker/compose/releases/download/1.19.0/docker-compose-`uname -s`-`uname -m` \\\n  -o /usr/local/bin/docker-compose   \\\n  sudo chmod +x /usr/local/bin/docker-compose\n\ndocker-compose --version\n\n# install docker-machine (VirtualBox required)\ncurl -L https://github.com/docker/machine/releases/download/v0.13.0/docker-machine-`uname -s`-`uname -m`  /tmp/docker-machine   \\\n  sudo install /tmp/docker-machine /usr/local/bin/docker-machine\n\ndocker-machine --version  Useful commands  # list images\ndocker images\n# list containers\ndocker ps -a\n# list volumes\ndocker volume ls\n\n# run temporary container\ndocker run --rm --name phusion phusion/baseimage:latest\n# access container from another shell\ndocker exec -it phusion bash\n\n# remove container by name\ndocker ps -a -q -f name=CONTAINER_NAME | xargs --no-run-if-empty docker rm -f\n# delete dangling images  none \ndocker images -q -f dangling=true | xargs --no-run-if-empty docker rmi\n# delete dangling volumes\ndocker volume ls -q -f dangling=true | xargs --no-run-if-empty docker volume rm  Docker Machine  # create local machine\ndocker-machine create --driver virtualbox default\n\n# list\ndocker-machine ls\ndocker-machine ls --filter name=default\ndocker-machine ls --filter state=Running\ndocker-machine ls --format  {{.Name}}: {{.DriverName}} - {{.State}} \n\n# info\ndocker-machine inspect default\ndocker-machine inspect --format='{{.Driver.IPAddress}}' default\ndocker-machine status default\ndocker-machine ip default\n\n# management\ndocker-machine start default\ndocker-machine stop default\ndocker-machine restart default\ndocker-machine rm default\n\n# mount volume\n#https://docs.docker.com/machine/reference/mount\n\n# show command to connect to machine\ndocker-machine env default\n# check if variables are set\nenv | grep DOCKER\n\n# connect to machine\neval  $(docker-machine env default) \ndocker ps -a\n\n# show command to disconnect from machine\ndocker-machine env -u\n# unset all\neval $(docker-machine env -u)\n\n# access\ndocker-machine ssh default\n# execute command and exit\ndocker-machine ssh default uptime\n# copy files from host to guest\ndocker-machine scp -r /FROM default:/TO\n\n# start nginx on default machine\ndocker run -d -p 8000:80 nginx\n# verify from host\ncurl $(docker-machine ip default):8000\n# forward to port 8080\ndocker-machine ssh default -L 8080:localhost:8000\n# verify tunnel from host\ncurl localhost:8080\n\n# disable error crash reporting\nmkdir -p ~/.docker/machine   touch ~/.docker/machine/no-error-report", 
            "title": "How-To"
        }, 
        {
            "location": "/docker/#base-image", 
            "text": "Build  devops/base  image  # change path\ncd devops/base\n\n# build image\ndocker build -t devops/base .\n\n# temporary container\ndocker run --rm --name devops-base devops/base\n# access container\ndocker exec -it devops-base bash", 
            "title": "Base image"
        }, 
        {
            "location": "/ansible/", 
            "text": "Ansible\n\n\n\n\nAnsible\n is an open source automation platform that can help with config management, deployment and task automation\n\n\n\n\nDocumentation\n\n\n\n\nAnsible\n\n\nTutorial\n\n\nPlaybook example\n\n\n\n\nThe following guide explains how to provision Ansible locally and play with it. Checkout the \nVagrantfile\n and the Vagrant \nguide\n for more details.\n\n\nSetup\n\n\nRequirement\n\n\n\n\nVagrant\n\n\nVirtualBox\n\n\n\n\nDirectory structure\n\n\ntree -a ansible/\nansible/\n\u251c\u2500\u2500 .share\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-3\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ssh\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ansible_rsa\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 ansible_rsa.pub\n\u251c\u2500\u2500 Vagrantfile\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 group_vars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 host_vars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hosts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 roles\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 common\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 defaults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 files\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 handlers\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 motd.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 package.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 templates\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 motd\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 vars\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 docker\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 site.yml\n\u251c\u2500\u2500 setup_ansible.sh\n\u2514\u2500\u2500 setup_share.sh\n\n\n\n\nThe first time \nonly\n, you have to setup the shared folders and generate the ssh key needed by ansible to access all nodes executing\n\n\n./setup_share.sh\n\n\n\n\nStart the boxes with\n\n\nvagrant up\n\n\n\n\nThe first time it could take a while\n\n\nVerify status of the boxes with\n\n\nvagrant status\n\n\n\n\nVerify access to the boxes with\n\n\nvagrant ssh ansible\nvagrant ssh node-1\n\n\n\n\nFrom inside the boxes you should be able to communicate with the others\n\n\nping ansible.local\nping ip-192-168-100-11.local\nping 192.168.100.12\n\n\n\n\nThe following paths are shared with the boxes\n\n\n\n\n/vagrant\n provision-tool\n\n\n/local\n host $HOME\n\n\n/ansible\n data \n(ansible only)\n\n\n/data\n .share \n(node only)\n\n\n\n\nAd-Hoc Commands\n\n\nAccess the ansible box with\n\n\nvagrant ssh ansible\n\n\n\n\nBelow a list of examples\n\n\n\n# ping all nodes (default inventory /etc/ansible/hosts)\nansible all -m ping\nansible ansible -m ping\nansible cluster -m ping\n\n# ping all nodes (specify inventory)\nansible all -i \n/vagrant/data/hosts\n -m ping\n\n# gathering facts\nansible all -m setup\nansible ansible -m setup\n\n# specify host and user\nansible ip-192-168-100-11.local -m ping -u vagrant\n\n# execute command\nansible all -a \n/bin/echo hello\n\nansible all -a \nuptime\n\nansible all -a \n/bin/date\n\n# do NOT reboot vagrant through ansible (use vagrant reload)\nansible cluster -a \n/sbin/reboot\n --become\n\n# shell module\nansible all -m shell -a \npwd\n\n# be carefull to quotes\nansible all -m shell -a 'echo $HOME'\n\n# update \n upgrade\nansible all -m apt -a \nupdate_cache=yes upgrade=dist\n --become\n# restart after upgrade\nvagrant reload\n# install package\nansible all -m apt -a \nname=tree state=present\n --become\n\n\n\n\nPlaybooks\n\n\nAccess the ansible box with\n\n\nvagrant ssh ansible\n\n\n\n\nBelow a list of examples\n\n\n# test uptime on all node\nansible-playbook /ansible/site.yml --tags=test --verbose\n\n# update \n upgrade only cluster nodes\nansible-playbook /ansible/site.yml -t package --skip-tags=java --verbose\n\n# install packages on cluster nodes\nansible-playbook /ansible/site.yml -t package --verbose\n\n# run common task on cluster node\nansible-playbook /ansible/site.yml -t common\n\n# setup docker\nansible-playbook /ansible/site.yml -t docker\n# test docker\nvagrant ssh node-1\nsudo -i -u docker\ndocker ps -a\n\n# custom banner\nansible-playbook /ansible/site.yml -t motd\n\n# setup all infrastructure at once\nansible-playbook /ansible/site.yml\n\n# dry run\nansible-playbook -i /ansible/hosts /ansible/site.yml --check --diff", 
            "title": "Ansible"
        }, 
        {
            "location": "/ansible/#ansible", 
            "text": "Ansible  is an open source automation platform that can help with config management, deployment and task automation   Documentation   Ansible  Tutorial  Playbook example   The following guide explains how to provision Ansible locally and play with it. Checkout the  Vagrantfile  and the Vagrant  guide  for more details.", 
            "title": "Ansible"
        }, 
        {
            "location": "/ansible/#setup", 
            "text": "Requirement   Vagrant  VirtualBox   Directory structure  tree -a ansible/\nansible/\n\u251c\u2500\u2500 .share\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-3\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ssh\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ansible_rsa\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 ansible_rsa.pub\n\u251c\u2500\u2500 Vagrantfile\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 group_vars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 host_vars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hosts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 roles\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 common\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 defaults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 files\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 handlers\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 motd.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 package.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 templates\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 motd\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 vars\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 docker\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 site.yml\n\u251c\u2500\u2500 setup_ansible.sh\n\u2514\u2500\u2500 setup_share.sh  The first time  only , you have to setup the shared folders and generate the ssh key needed by ansible to access all nodes executing  ./setup_share.sh  Start the boxes with  vagrant up  The first time it could take a while  Verify status of the boxes with  vagrant status  Verify access to the boxes with  vagrant ssh ansible\nvagrant ssh node-1  From inside the boxes you should be able to communicate with the others  ping ansible.local\nping ip-192-168-100-11.local\nping 192.168.100.12  The following paths are shared with the boxes   /vagrant  provision-tool  /local  host $HOME  /ansible  data  (ansible only)  /data  .share  (node only)", 
            "title": "Setup"
        }, 
        {
            "location": "/ansible/#ad-hoc-commands", 
            "text": "Access the ansible box with  vagrant ssh ansible  Below a list of examples  \n# ping all nodes (default inventory /etc/ansible/hosts)\nansible all -m ping\nansible ansible -m ping\nansible cluster -m ping\n\n# ping all nodes (specify inventory)\nansible all -i  /vagrant/data/hosts  -m ping\n\n# gathering facts\nansible all -m setup\nansible ansible -m setup\n\n# specify host and user\nansible ip-192-168-100-11.local -m ping -u vagrant\n\n# execute command\nansible all -a  /bin/echo hello \nansible all -a  uptime \nansible all -a  /bin/date \n# do NOT reboot vagrant through ansible (use vagrant reload)\nansible cluster -a  /sbin/reboot  --become\n\n# shell module\nansible all -m shell -a  pwd \n# be carefull to quotes\nansible all -m shell -a 'echo $HOME'\n\n# update   upgrade\nansible all -m apt -a  update_cache=yes upgrade=dist  --become\n# restart after upgrade\nvagrant reload\n# install package\nansible all -m apt -a  name=tree state=present  --become", 
            "title": "Ad-Hoc Commands"
        }, 
        {
            "location": "/ansible/#playbooks", 
            "text": "Access the ansible box with  vagrant ssh ansible  Below a list of examples  # test uptime on all node\nansible-playbook /ansible/site.yml --tags=test --verbose\n\n# update   upgrade only cluster nodes\nansible-playbook /ansible/site.yml -t package --skip-tags=java --verbose\n\n# install packages on cluster nodes\nansible-playbook /ansible/site.yml -t package --verbose\n\n# run common task on cluster node\nansible-playbook /ansible/site.yml -t common\n\n# setup docker\nansible-playbook /ansible/site.yml -t docker\n# test docker\nvagrant ssh node-1\nsudo -i -u docker\ndocker ps -a\n\n# custom banner\nansible-playbook /ansible/site.yml -t motd\n\n# setup all infrastructure at once\nansible-playbook /ansible/site.yml\n\n# dry run\nansible-playbook -i /ansible/hosts /ansible/site.yml --check --diff", 
            "title": "Playbooks"
        }, 
        {
            "location": "/cassandra/", 
            "text": "Cassandra\n\n\n\n\nCassandra\n is a distributed database for managing large amounts of structured data across many commodity servers, while providing highly available service and no single point of failure\n\n\n\n\nUseful documentation\n\n\n\n\n\n\nCassandra\n\n\n\n\n\n\nA Decentralized Structured Storage System\n (Paper)\n\n\n\n\n\n\nA Big Data Modeling Methodology for Apache Cassandra\n (Paper)\n\n\n\n\n\n\nFacebook\u2019s Cassandra paper\n\n\n\n\n\n\nCassandra Data Modeling Best Practices\n\n\n\n\n\n\nDifference between partition key, composite key and clustering key\n\n\n\n\n\n\n\n\n\nCassandra uses a tick-tock release model, even-numbered releases are feature releases, while odd-numbered releases are focused on bug fixes\n\n\nSetup\n\n\nSingle Node Cluster\n\n\n# change path\ncd devops/cassandra\n\n# start single node\ndocker-compose up\n\n# paths\n/etc/cassandra\n/var/lib/cassandra\n/var/log/cassandra\n\n# remove container and volume\ndocker rm -fv devops-cassandra\n\n\n\n\nMulti Node Cluster\n\n\n# change path\ncd devops/cassandra\n\n# start node\ndocker-compose -f docker-compose-cluster.yml up\n\n# optional mounted volumes\nmkdir -p \\\n  .cassandra/cassandra-seed/{data,log} \\\n  .cassandra/cassandra-node-1/{data,log} \\\n  .cassandra/cassandra-node-2/{data,log}\ntree .cassandra/\n\n# ISSUES releated to host permissions\n# \n Small commitlog volume detected at /var/lib/cassandra/commitlog\n# \n There is insufficient memory for the Java Runtime Environment to continue\n(cassandra) /var/lib/cassandra\n(root) /var/log/cassandra\n\n\n\n\nAccess container\n\n\n# access container\ndocker exec -it devops-cassandra bash\ndocker exec -it devops-cassandra bash -c cqlsh\ndocker exec -it devops-cassandra-seed bash\ndocker exec -it devops-cassandra-node-1 bash\n\n# execute cql script from host\n(docker exec -i devops-cassandra bash \\\n  -c \ncat \n example.cql; cqlsh -f example.cql\n) \n cql/example_create.cql\n\n\n\n\nExamples\n\n\ncqlsh\n example \nscripts\n\n\n# connect\ncqlsh localhost 9042\ncqlsh localhost 9042 -u cassandra -p cassandra\n\n# execute cql script\ncqlsh -f cql/example_create.cql\n\n# info\nSHOW VERSION;\nDESCRIBE CLUSTER;\nDESCRIBE KEYSPACES;\nDESCRIBE KEYSPACE example;\nDESCRIBE TABLE example.messages;\n\n\n\n\nOld \ncassandra-cli\n deprecated and removed in Cassandra 3.0\n\n\nUSE keyspace_name;\nLIST table_name;\nGET table_name[\nprimary_key\n];\nSET table_name[\nprimary_key\n][\ncolumn_name\n];", 
            "title": "Cassandra"
        }, 
        {
            "location": "/cassandra/#cassandra", 
            "text": "Cassandra  is a distributed database for managing large amounts of structured data across many commodity servers, while providing highly available service and no single point of failure   Useful documentation    Cassandra    A Decentralized Structured Storage System  (Paper)    A Big Data Modeling Methodology for Apache Cassandra  (Paper)    Facebook\u2019s Cassandra paper    Cassandra Data Modeling Best Practices    Difference between partition key, composite key and clustering key     Cassandra uses a tick-tock release model, even-numbered releases are feature releases, while odd-numbered releases are focused on bug fixes", 
            "title": "Cassandra"
        }, 
        {
            "location": "/cassandra/#setup", 
            "text": "Single Node Cluster  # change path\ncd devops/cassandra\n\n# start single node\ndocker-compose up\n\n# paths\n/etc/cassandra\n/var/lib/cassandra\n/var/log/cassandra\n\n# remove container and volume\ndocker rm -fv devops-cassandra  Multi Node Cluster  # change path\ncd devops/cassandra\n\n# start node\ndocker-compose -f docker-compose-cluster.yml up\n\n# optional mounted volumes\nmkdir -p \\\n  .cassandra/cassandra-seed/{data,log} \\\n  .cassandra/cassandra-node-1/{data,log} \\\n  .cassandra/cassandra-node-2/{data,log}\ntree .cassandra/\n\n# ISSUES releated to host permissions\n#   Small commitlog volume detected at /var/lib/cassandra/commitlog\n#   There is insufficient memory for the Java Runtime Environment to continue\n(cassandra) /var/lib/cassandra\n(root) /var/log/cassandra  Access container  # access container\ndocker exec -it devops-cassandra bash\ndocker exec -it devops-cassandra bash -c cqlsh\ndocker exec -it devops-cassandra-seed bash\ndocker exec -it devops-cassandra-node-1 bash\n\n# execute cql script from host\n(docker exec -i devops-cassandra bash \\\n  -c  cat   example.cql; cqlsh -f example.cql )   cql/example_create.cql", 
            "title": "Setup"
        }, 
        {
            "location": "/cassandra/#examples", 
            "text": "cqlsh  example  scripts  # connect\ncqlsh localhost 9042\ncqlsh localhost 9042 -u cassandra -p cassandra\n\n# execute cql script\ncqlsh -f cql/example_create.cql\n\n# info\nSHOW VERSION;\nDESCRIBE CLUSTER;\nDESCRIBE KEYSPACES;\nDESCRIBE KEYSPACE example;\nDESCRIBE TABLE example.messages;  Old  cassandra-cli  deprecated and removed in Cassandra 3.0  USE keyspace_name;\nLIST table_name;\nGET table_name[ primary_key ];\nSET table_name[ primary_key ][ column_name ];", 
            "title": "Examples"
        }, 
        {
            "location": "/zookeeper/", 
            "text": "ZooKeeper\n\n\n\n\nZooKeeper\n is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\n\n\n\n\nDocumentation\n\n\n\n\nZooKeeper\n\n\nCurator\n\n\n\n\nSetup\n\n\nRequirement\n\n\n\n\nBase\n image\n\n\n\n\nBuild \ndevops/zookeeper\n image\n\n\n# change path\ncd devops/zookeeper\n\n# build image\ndocker build -t devops/zookeeper:latest .\n# build image with specific version - see Dockerfile for version 3.5.x\ndocker build -t devops/zookeeper:3.4.10 --build-arg VERSION=3.4.10 .\n\n# temporary container [host:container]\ndocker run --rm --name zookeeper -p 12181:2181 devops/zookeeper\n# access container\ndocker exec -it zookeeper bash\n\n# paths\n/opt/zookeeper\n/var/log/zookeeper\n/var/lib/zookeeper\n/var/log/supervisord.log\n\n# logs\ntail -F /var/log/supervisord.log\n# check service status\nsupervisorctl status\nsupervisorctl restart zookeeper\n\n\n\n\nExample\n\n\ndocker exec -it zookeeper bash\n\n# (option 1) check zookeeper status\necho ruok | nc localhost 2181\n\n# (option 2) check zookeeper status\ntelnet localhost 2181\n# expect answer imok\n\n ruok\n\nzkCli.sh -server 127.0.0.1:2181\nhelp\n# list znodes\nls /\n# create znode and associate value\ncreate /zk_test my_data\n# verify data\nget /zk_test\n# change value\nset /zk_test junk\n# delete znode\ndelete /zk_test\n\n\n\n\nThe four-letter words\n\n\n\n\n\n\n\n\nCategory\n\n\nCommand\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nServer status\n\n\nruok\n\n\nPrints \nimok\n if the server is running and not in an error state\n\n\n\n\n\n\n\n\nconf\n\n\nPrints the server configuration (from zoo.cfg)\n\n\n\n\n\n\n\n\nenvi\n\n\nPrints the server environment, including ZooKeeper version, Java version, and other system properties\n\n\n\n\n\n\n\n\nsrvr\n\n\nPrints server statistics, including latency statistics, the number of znodes, and the server mode (standalone, leader, or follower)\n\n\n\n\n\n\n\n\nstat\n\n\nPrints server statistics and connected clients\n\n\n\n\n\n\n\n\nsrst\n\n\nResets server statistics\n\n\n\n\n\n\n\n\nisro\n\n\nShows whether the server is in read-only ( ro ) mode (due to a network partition) or read/write mode (rw)\n\n\n\n\n\n\nClient connections\n\n\ndump\n\n\nLists all the sessions and ephemeral znodes for the ensemble. You must connect to the leader (see srvr) for this command\n\n\n\n\n\n\n\n\ncons\n\n\nLists connection statistics for all the server's clients\n\n\n\n\n\n\n\n\ncrst\n\n\nResets connection statistics\n\n\n\n\n\n\nWatches\n\n\nwchs\n\n\nLists summary information for the server's watches\n\n\n\n\n\n\n\n\nwchc\n\n\nLists all the server's watches by connection, may impact server performance for a large number of watches\n\n\n\n\n\n\n\n\nwchp\n\n\nLists all the server\u2019s watches by znode path, may impact server performance for a large number of watches\n\n\n\n\n\n\nMonitoring\n\n\nmntr\n\n\nLists server statistics in Java properties format, suitable as a source for monitoring systems such as Ganglia and Nagios", 
            "title": "ZooKeeper"
        }, 
        {
            "location": "/zookeeper/#zookeeper", 
            "text": "ZooKeeper  is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services   Documentation   ZooKeeper  Curator", 
            "title": "ZooKeeper"
        }, 
        {
            "location": "/zookeeper/#setup", 
            "text": "Requirement   Base  image   Build  devops/zookeeper  image  # change path\ncd devops/zookeeper\n\n# build image\ndocker build -t devops/zookeeper:latest .\n# build image with specific version - see Dockerfile for version 3.5.x\ndocker build -t devops/zookeeper:3.4.10 --build-arg VERSION=3.4.10 .\n\n# temporary container [host:container]\ndocker run --rm --name zookeeper -p 12181:2181 devops/zookeeper\n# access container\ndocker exec -it zookeeper bash\n\n# paths\n/opt/zookeeper\n/var/log/zookeeper\n/var/lib/zookeeper\n/var/log/supervisord.log\n\n# logs\ntail -F /var/log/supervisord.log\n# check service status\nsupervisorctl status\nsupervisorctl restart zookeeper  Example  docker exec -it zookeeper bash\n\n# (option 1) check zookeeper status\necho ruok | nc localhost 2181\n\n# (option 2) check zookeeper status\ntelnet localhost 2181\n# expect answer imok  ruok\n\nzkCli.sh -server 127.0.0.1:2181\nhelp\n# list znodes\nls /\n# create znode and associate value\ncreate /zk_test my_data\n# verify data\nget /zk_test\n# change value\nset /zk_test junk\n# delete znode\ndelete /zk_test", 
            "title": "Setup"
        }, 
        {
            "location": "/zookeeper/#the-four-letter-words", 
            "text": "Category  Command  Description      Server status  ruok  Prints  imok  if the server is running and not in an error state     conf  Prints the server configuration (from zoo.cfg)     envi  Prints the server environment, including ZooKeeper version, Java version, and other system properties     srvr  Prints server statistics, including latency statistics, the number of znodes, and the server mode (standalone, leader, or follower)     stat  Prints server statistics and connected clients     srst  Resets server statistics     isro  Shows whether the server is in read-only ( ro ) mode (due to a network partition) or read/write mode (rw)    Client connections  dump  Lists all the sessions and ephemeral znodes for the ensemble. You must connect to the leader (see srvr) for this command     cons  Lists connection statistics for all the server's clients     crst  Resets connection statistics    Watches  wchs  Lists summary information for the server's watches     wchc  Lists all the server's watches by connection, may impact server performance for a large number of watches     wchp  Lists all the server\u2019s watches by znode path, may impact server performance for a large number of watches    Monitoring  mntr  Lists server statistics in Java properties format, suitable as a source for monitoring systems such as Ganglia and Nagios", 
            "title": "The four-letter words"
        }, 
        {
            "location": "/kafka/", 
            "text": "Kafka\n\n\n\n\nKafka\n is a distributed streaming platform\n\n\n\n\nDocumentation\n\n\n\n\nKafka\n\n\n\n\nSetup\n\n\nRequirement\n\n\n\n\nBase\n docker image \n\n\nZooKeeper\n docker image\n\n\n\n\nBuild \ndevops/kafka\n image\n\n\n# change path\ncd devops/kafka\n\n# build image\ndocker build -t devops/kafka .\n\n# create network\ndocker network create --driver bridge my_network\ndocker network ls\ndocker network inspect my_network\n\n# start temporary zookeeper container [host:container]\ndocker run --rm \\\n  --name zookeeper \\\n  -p 12181:2181 \\\n  --network=my_network \\\n  devops/zookeeper\n# access container\ndocker exec -it zookeeper bash\n\n# start temporary kafka container [host:container]\ndocker run --rm \\\n  --name kafka \\\n  -p 19092:9092 \\\n  --network=my_network \\\n  -e ZOOKEEPER_HOSTS=\nzookeeper:2181\n \\\n  devops/kafka\n# access container\ndocker exec -it kafka bash\n\n# paths\n/opt/kafka\n/opt/kafka/logs\n/var/log/kafka\n/var/lib/kafka/data\n\n\n\n\nAlternatively use \ndocker-compose\n\n\n# change path\ncd devops/kafka\n\n# build base image\ndocker build -t devops/base ../base\n# build + start zookeeper and kafka\ndocker-compose up\n\n# access container\ndocker exec -it devops-zookeeper bash\ndocker exec -it devops-kafka bash\n\n\n\n\nExample\n\n\ndocker exec -it kafka bash\ncd /opt/kafka/bin\n\n./kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic test\n./kafka-topics.sh --list --zookeeper zookeeper:2181\n./kafka-console-producer.sh --broker-list kafka:9092 --topic test\n./kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic test --from-beginning", 
            "title": "Kafka"
        }, 
        {
            "location": "/kafka/#kafka", 
            "text": "Kafka  is a distributed streaming platform   Documentation   Kafka", 
            "title": "Kafka"
        }, 
        {
            "location": "/kafka/#setup", 
            "text": "Requirement   Base  docker image   ZooKeeper  docker image   Build  devops/kafka  image  # change path\ncd devops/kafka\n\n# build image\ndocker build -t devops/kafka .\n\n# create network\ndocker network create --driver bridge my_network\ndocker network ls\ndocker network inspect my_network\n\n# start temporary zookeeper container [host:container]\ndocker run --rm \\\n  --name zookeeper \\\n  -p 12181:2181 \\\n  --network=my_network \\\n  devops/zookeeper\n# access container\ndocker exec -it zookeeper bash\n\n# start temporary kafka container [host:container]\ndocker run --rm \\\n  --name kafka \\\n  -p 19092:9092 \\\n  --network=my_network \\\n  -e ZOOKEEPER_HOSTS= zookeeper:2181  \\\n  devops/kafka\n# access container\ndocker exec -it kafka bash\n\n# paths\n/opt/kafka\n/opt/kafka/logs\n/var/log/kafka\n/var/lib/kafka/data  Alternatively use  docker-compose  # change path\ncd devops/kafka\n\n# build base image\ndocker build -t devops/base ../base\n# build + start zookeeper and kafka\ndocker-compose up\n\n# access container\ndocker exec -it devops-zookeeper bash\ndocker exec -it devops-kafka bash  Example  docker exec -it kafka bash\ncd /opt/kafka/bin\n\n./kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic test\n./kafka-topics.sh --list --zookeeper zookeeper:2181\n./kafka-console-producer.sh --broker-list kafka:9092 --topic test\n./kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic test --from-beginning", 
            "title": "Setup"
        }, 
        {
            "location": "/hadoop/", 
            "text": "Hadoop\n\n\nThe following guide explains how to provision a Multi Node Hadoop Cluster locally and play with it. Checkout the \nVagrantfile\n and the Vagrant \nguide\n for more details.\n\n\nDocumentation\n\n\n\n\nHadoop\n\n\nThe Hadoop Ecosystem Table\n\n\n\n\nSetup\n\n\nRequirements\n\n\n\n\nVagrant\n\n\nVirtualBox\n\n\n\n\nDirectory structure\n\n\ntree -a hadoop/\nhadoop/\n\u251c\u2500\u2500 .data # mounted volume\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop_rsa\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop_rsa.pub\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 master\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 mapred\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 yarn\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 namenode\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 secondary\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 spark\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 datanode\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 mapred\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u2514\u2500\u2500 yarn\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-3\n\u251c\u2500\u2500 example\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 map-reduce\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 spark\n\u251c\u2500\u2500 file\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 core-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fair-scheduler.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hdfs-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 mapred-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 masters\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 slaves\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 yarn-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-hadoop.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hosts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 motd\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie-env.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 oozie-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-oozie.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 spark\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 log4j.properties\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 spark-env.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-spark.sh\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ssh\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 config\n\u251c\u2500\u2500 script\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bootstrap.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_hadoop.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_oozie.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_spark.sh\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 setup_ubuntu.sh\n\u251c\u2500\u2500 Vagrantfile\n\u2514\u2500\u2500 vagrant_hadoop.sh\n\n\n\n\nImport the script\n\n\nsource vagrant_hadoop.sh\n\n\n\n\nCreate and start a Multi Node Hadoop Cluster\n\n\nhadoop-start\n\n\n\n\nThe first time it might take a while\n\n\nAccess the cluster via ssh, check also the \n/etc/hosts\n file\n\n\nvagrant ssh master\nssh hadoop@172.16.0.10 -i .data/hadoop_rsa\n\n# 3 nodes\nvagrant ssh node-1\nssh hadoop@172.16.0.101 -i .data/hadoop_rsa\n\n\n\n\nDestroy the cluster\n\n\nhadoop-destroy\n\n\n\n\nFor convenience add to the host machine\n\n\ncat hadoop/file/hosts | sudo tee --append /etc/hosts\n\n\n\n\nWeb UI links\n\n\n\n\nNameNode: \nhttp://namenode.local:50070\n\n\nNameNode metrics: \nhttp://namenode.local:50070/jmx\n\n\nResourceManager: \nhttp://resource-manager.local:8088\n\n\nLog Level: \nhttp://resource-manager.local:8088/logLevel\n\n\nWeb Application Proxy Server: \nhttp://web-proxy.local:8100/proxy/application_XXX_0000\n\n\nMapReduce Job History Server: \nhttp://history.local:19888\n\n\nDataNode/NodeManager (1): \nhttp://node-1.local:8042/node\n\n\nDataNode/NodeManager (2): \nhttp://node-2.local:8042/node\n\n\nDataNode/NodeManager (3): \nhttp://node-3.local:8042/node\n\n\nSpark: \nhttp://spark.local:4040\n\n\nOozie*: \nhttp://oozie.local:11000\n\n\n\n\n\n\nHDFS and MapReduce\n\n\n\n\nHDFS\n is a distributed file system that provides high-throughput access to application data\n\n\nYARN\n is a framework for job scheduling and cluster resource management\n\n\nMapReduce\n is a YARN-based system for parallel processing of large data sets\n\n\n\n\nDocumentation\n\n\n\n\nHadoop v2.7.5\n\n\nUntangling Apache Hadoop YARN\n series\n\n\n\n\nHDFS Admin\n\n\n# filesystem statistics\nhdfs dfsadmin -report\n\n# filesystem check\nhdfs fsck /\n\n\n\n\nUseful paths\n\n\n# data and logs\ndevops/hadoop/.data/master/hadoop # host\n/vol/hadoop # guest\n\n# (guest) config\n/usr/local/hadoop/etc/hadoop\n\n# (hdfs) map-reduce history\n/mr-history/history/done_intermediate/hadoop\n\n# (hdfs) aggregated app logs\n/yarn/app/hadoop/logs/application_XXX\n\n\n\n\nMapReduce WordCount Job\n\n\n# build jar on the host machine\ncd devops/hadoop/example/map-reduce\n./gradlew clean build\n\ncd devops/hadoop\nvagrant ssh master\n\n# create base directory using hdfs\nhdfs dfs -mkdir -p /user/ubuntu\n\n# create example directory\nhadoop fs -mkdir -p /user/ubuntu/word-count/input\n\n# list directory\nhadoop fs -ls -h -R /\nhadoop fs -ls -h -R /user/ubuntu\n\n# create sample files\necho \nHello World Bye World\n \n file01\necho \nHello Hadoop Goodbye Hadoop\n \n file02\n\n# copy from local to hdfs\nhadoop fs -copyFromLocal file01 /user/ubuntu/word-count/input\nhadoop fs -put file02 /user/ubuntu/word-count/input\n\n# verify copied files\nhadoop fs -ls -h -R /user/ubuntu\nhadoop fs -cat /user/ubuntu/word-count/input/file01\nhadoop fs -cat /user/ubuntu/word-count/input/file02\nhadoop fs -cat /user/ubuntu/word-count/input/*\n\n# run application\nhadoop jar /vagrant/example/map-reduce/build/libs/map-reduce.jar \\\n  /user/ubuntu/word-count/input \\\n  /user/ubuntu/word-count/output\n\n# check output\nhadoop fs -cat /user/ubuntu/word-count/output/part-r-00000\n\n# delete directory to run it again\nhadoop fs -rm -R /user/ubuntu/word-count/output\n\n# run sample job in a different queue\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  wordcount \\\n  -Dmapreduce.job.queuename=root.priority_queue \\\n  /user/ubuntu/word-count/input \\\n  /user/ubuntu/word-count/output\n\n# well known WARN issue\n# https://issues.apache.org/jira/browse/HDFS-10429\n\n\n\n\nBenchmarking MapReduce with TeraSort\n\n\n# generate random data\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  teragen 1000 random-data\n\n# run terasort benchmark\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  terasort random-data sorted-data\n\n# validate data\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  teravalidate sorted-data report\n\n# useful commands\nhadoop fs -ls -h -R .\nhadoop fs -rm -r random-data\nhadoop fs -cat random-data/part-m-00000\nhadoop fs -cat sorted-data/part-r-00000\n\n\n\n\n\n\nSpark\n\n\n\n\nSpark\n is an open-source cluster-computing framework\n\n\n\n\nDocumentation\n\n\n\n\nSpark\n\n\nHow-to: Tune Your Apache Spark Jobs\n series\n\n\nUnderstanding Resource Allocation configurations for a Spark application\n\n\n\n\n# start REPL\nspark-shell\npyspark\n\n\n\n\nInteractive Analysis example\n\n\nspark-shell\n\nval licenceLines = sc.textFile(\nfile:/usr/local/spark/LICENSE\n)\nval lineCount = licenceLines.count\nval isBsd = (line: String) =\n line.contains(\nBSD\n)\nval bsdLines = licenceLines.filter(isBsd)\nbsdLines.count\nbsdLines.foreach(println)\n\n\n\n\nSpark Job example\n\n\n# GitHub event documentation\n# https://developer.github.com/v3/activity/events/types\n\n# build jar on the host machine\ncd devops/hadoop/example/spark\nsbt clean package\n\ncd devops/hadoop\nvagrant ssh master\n\n# sample dataset\nmkdir -p github-archive \n \\\n  cd $_ \n \\\n  wget http://data.githubarchive.org/2018-01-01-{0..10}.json.gz \n \\\n  gunzip -k *\n# sample line\nhead -n 1 2018-01-01-0.json | jq '.'\n\n# run job\nspark-submit \\\n  --class \ncom.github.niqdev.App\n \\\n  --master local[*] \\\n  /vagrant/example/spark/target/scala-2.11/spark-github_2.11-0.1.0-SNAPSHOT.jar\n\n\n\n\n\n\nOozie\n\n\n\n\nOozie\n is a workflow scheduler system to manage Hadoop jobs\n\n\n\n\nDocumentation\n\n\n\n\nOozie\n\n\n\n\nSetup\n\n\n\n\nOozie is not installed by default\n\n\n\n\nOptional PostgreSQL configuration\n - By default Oozie is configured to use Embedded Derby\n\n\n# access master node\nvagrant ssh master\n\n# install docker\ncurl -fsSL get.docker.com -o get-docker.sh \n \\\n  chmod u+x $_ \n \\\n  ./$_ \n \\\n  sudo usermod -aG docker hadoop\n\n# logout and login again to verify docker installation\nexit\nvagrant ssh master\nwhoami # hadoop\ndocker ps -a\n\n# uncomment PostgreSQL configurations\nvim devops/hadoop/file/oozie/config/oozie-site.xml # from host\nvim /vagrant/file/oozie/config/oozie-site.xml # from guest\n\n# start postgres on guest machine \ndocker run \\\n  --detach \\\n  --name oozie-postgres \\\n  -p 5432:5432 \\\n  -e POSTGRES_DB=\noozie-db\n \\\n  -e POSTGRES_USER=\npostgres\n \\\n  -e POSTGRES_PASSWORD=\npassword\n \\\n  postgres\n\n# permission issue\n# https://github.com/docker-library/postgres/issues/116\n# --volume /vol/postgres:/var/lib/postgresql/data\n\n# access container\ndocker exec -it oozie-postgres bash\npsql --username=postgres\n# list all databases\n\\list\n\\connect oozie-db\n# list all tables\n\\dt\n# describe table\n\\d+ wf_jobs\n# list workflow\nselect * from wf_jobs;\n\n\n\n\nInstall and start Oozie\n\n\n# access master node\nvagrant ssh master\n\n# login as root\nsudo su -\n\n# build, install and init\n/vagrant/script/setup_oozie.sh\n\n# start oozie manually (first time only)\nsu --login hadoop /vagrant/script/bootstrap.sh oozie\n\n\n\n\nIt might take a while to build the sources\n\n\nUseful paths\n\n\n# data and logs\ndevops/hadoop/.data/master/oozie # host\n/vol/oozie # guest\n\n# (guest) config\n/usr/local/oozie/conf\n\n# (hdfs) examples\n/user/hadoop/examples\n\n\n\n\nExamples\n\n\nRun bundled examples within distribution\n\n\n# examples path\n.data/master/oozie/examples # host\n/vol/oozie/examples # guest\n\n# access master node as hadoop user\nvagrant ssh master\n\nexport OOZIE_EXAMPLE_PATH=/vol/oozie/examples\nexport OOZIE_HDFS_PATH=/user/$(whoami)/examples\n\n# open map-reduce job.properties\nvim $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties\n\n# edit the following properties\nnameNode=hdfs://namenode.local:9000 # fs.defaultFS @ core-site.xml\njobTracker=resource-manager.local:8032 # yarn.resourcemanager.address @ yarn-site.xml\nqueueName=priority_queue # or default @ fair-scheduler.xml\n\n# upload all the examples\nhadoop fs -put $OOZIE_EXAMPLE_PATH $OOZIE_HDFS_PATH\n\n# verify uploaded files\nhadoop fs -ls -h -R /user/$(whoami)\n\n# run the map-reduce workflow example\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -config $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties \\\n  -run\n\n# verify status\noozie job -oozie http://oozie.local:11000/oozie -info WORKFLOW_ID\n\n# verify result\nhadoop fs -cat $OOZIE_HDFS_PATH/output-data/map-reduce/part-00000\n\n# remove all the examples\nhadoop fs -rm -R $OOZIE_HDFS_PATH\n\n\n\n\nUseful commands\n\n\n\n\nWorkflow requires \noozie.wf.application.path\n property\n\n\nCoordinator requires \noozie.coord.application.path\n property\n\n\n\n\n# verify oozie status\noozie admin \\\n  -oozie http://oozie.local:11000/oozie \\\n  -status\n\n# verify workflow or coordinator status\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -info JOB_ID \\\n  -verbose\n\n# poll workflow or coordinator status\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -poll JOB_ID \\\n  -interval 10 \\\n  -timeout 60 \\\n  -verbose", 
            "title": "Hadoop"
        }, 
        {
            "location": "/hadoop/#hadoop", 
            "text": "The following guide explains how to provision a Multi Node Hadoop Cluster locally and play with it. Checkout the  Vagrantfile  and the Vagrant  guide  for more details.  Documentation   Hadoop  The Hadoop Ecosystem Table", 
            "title": "Hadoop"
        }, 
        {
            "location": "/hadoop/#setup", 
            "text": "Requirements   Vagrant  VirtualBox   Directory structure  tree -a hadoop/\nhadoop/\n\u251c\u2500\u2500 .data # mounted volume\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop_rsa\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop_rsa.pub\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 master\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 mapred\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 yarn\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 namenode\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 secondary\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 spark\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 datanode\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 mapred\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u2514\u2500\u2500 yarn\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-3\n\u251c\u2500\u2500 example\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 map-reduce\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 spark\n\u251c\u2500\u2500 file\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 core-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fair-scheduler.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hdfs-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 mapred-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 masters\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 slaves\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 yarn-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-hadoop.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hosts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 motd\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie-env.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 oozie-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-oozie.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 spark\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 log4j.properties\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 spark-env.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-spark.sh\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ssh\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 config\n\u251c\u2500\u2500 script\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bootstrap.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_hadoop.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_oozie.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_spark.sh\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 setup_ubuntu.sh\n\u251c\u2500\u2500 Vagrantfile\n\u2514\u2500\u2500 vagrant_hadoop.sh  Import the script  source vagrant_hadoop.sh  Create and start a Multi Node Hadoop Cluster  hadoop-start  The first time it might take a while  Access the cluster via ssh, check also the  /etc/hosts  file  vagrant ssh master\nssh hadoop@172.16.0.10 -i .data/hadoop_rsa\n\n# 3 nodes\nvagrant ssh node-1\nssh hadoop@172.16.0.101 -i .data/hadoop_rsa  Destroy the cluster  hadoop-destroy  For convenience add to the host machine  cat hadoop/file/hosts | sudo tee --append /etc/hosts  Web UI links   NameNode:  http://namenode.local:50070  NameNode metrics:  http://namenode.local:50070/jmx  ResourceManager:  http://resource-manager.local:8088  Log Level:  http://resource-manager.local:8088/logLevel  Web Application Proxy Server:  http://web-proxy.local:8100/proxy/application_XXX_0000  MapReduce Job History Server:  http://history.local:19888  DataNode/NodeManager (1):  http://node-1.local:8042/node  DataNode/NodeManager (2):  http://node-2.local:8042/node  DataNode/NodeManager (3):  http://node-3.local:8042/node  Spark:  http://spark.local:4040  Oozie*:  http://oozie.local:11000", 
            "title": "Setup"
        }, 
        {
            "location": "/hadoop/#hdfs-and-mapreduce", 
            "text": "HDFS  is a distributed file system that provides high-throughput access to application data  YARN  is a framework for job scheduling and cluster resource management  MapReduce  is a YARN-based system for parallel processing of large data sets   Documentation   Hadoop v2.7.5  Untangling Apache Hadoop YARN  series", 
            "title": "HDFS and MapReduce"
        }, 
        {
            "location": "/hadoop/#hdfs-admin", 
            "text": "# filesystem statistics\nhdfs dfsadmin -report\n\n# filesystem check\nhdfs fsck /  Useful paths  # data and logs\ndevops/hadoop/.data/master/hadoop # host\n/vol/hadoop # guest\n\n# (guest) config\n/usr/local/hadoop/etc/hadoop\n\n# (hdfs) map-reduce history\n/mr-history/history/done_intermediate/hadoop\n\n# (hdfs) aggregated app logs\n/yarn/app/hadoop/logs/application_XXX", 
            "title": "HDFS Admin"
        }, 
        {
            "location": "/hadoop/#mapreduce-wordcount-job", 
            "text": "# build jar on the host machine\ncd devops/hadoop/example/map-reduce\n./gradlew clean build\n\ncd devops/hadoop\nvagrant ssh master\n\n# create base directory using hdfs\nhdfs dfs -mkdir -p /user/ubuntu\n\n# create example directory\nhadoop fs -mkdir -p /user/ubuntu/word-count/input\n\n# list directory\nhadoop fs -ls -h -R /\nhadoop fs -ls -h -R /user/ubuntu\n\n# create sample files\necho  Hello World Bye World    file01\necho  Hello Hadoop Goodbye Hadoop    file02\n\n# copy from local to hdfs\nhadoop fs -copyFromLocal file01 /user/ubuntu/word-count/input\nhadoop fs -put file02 /user/ubuntu/word-count/input\n\n# verify copied files\nhadoop fs -ls -h -R /user/ubuntu\nhadoop fs -cat /user/ubuntu/word-count/input/file01\nhadoop fs -cat /user/ubuntu/word-count/input/file02\nhadoop fs -cat /user/ubuntu/word-count/input/*\n\n# run application\nhadoop jar /vagrant/example/map-reduce/build/libs/map-reduce.jar \\\n  /user/ubuntu/word-count/input \\\n  /user/ubuntu/word-count/output\n\n# check output\nhadoop fs -cat /user/ubuntu/word-count/output/part-r-00000\n\n# delete directory to run it again\nhadoop fs -rm -R /user/ubuntu/word-count/output\n\n# run sample job in a different queue\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  wordcount \\\n  -Dmapreduce.job.queuename=root.priority_queue \\\n  /user/ubuntu/word-count/input \\\n  /user/ubuntu/word-count/output\n\n# well known WARN issue\n# https://issues.apache.org/jira/browse/HDFS-10429", 
            "title": "MapReduce WordCount Job"
        }, 
        {
            "location": "/hadoop/#benchmarking-mapreduce-with-terasort", 
            "text": "# generate random data\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  teragen 1000 random-data\n\n# run terasort benchmark\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  terasort random-data sorted-data\n\n# validate data\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  teravalidate sorted-data report\n\n# useful commands\nhadoop fs -ls -h -R .\nhadoop fs -rm -r random-data\nhadoop fs -cat random-data/part-m-00000\nhadoop fs -cat sorted-data/part-r-00000", 
            "title": "Benchmarking MapReduce with TeraSort"
        }, 
        {
            "location": "/hadoop/#spark", 
            "text": "Spark  is an open-source cluster-computing framework   Documentation   Spark  How-to: Tune Your Apache Spark Jobs  series  Understanding Resource Allocation configurations for a Spark application   # start REPL\nspark-shell\npyspark", 
            "title": "Spark"
        }, 
        {
            "location": "/hadoop/#interactive-analysis-example", 
            "text": "spark-shell\n\nval licenceLines = sc.textFile( file:/usr/local/spark/LICENSE )\nval lineCount = licenceLines.count\nval isBsd = (line: String) =  line.contains( BSD )\nval bsdLines = licenceLines.filter(isBsd)\nbsdLines.count\nbsdLines.foreach(println)", 
            "title": "Interactive Analysis example"
        }, 
        {
            "location": "/hadoop/#spark-job-example", 
            "text": "# GitHub event documentation\n# https://developer.github.com/v3/activity/events/types\n\n# build jar on the host machine\ncd devops/hadoop/example/spark\nsbt clean package\n\ncd devops/hadoop\nvagrant ssh master\n\n# sample dataset\nmkdir -p github-archive   \\\n  cd $_   \\\n  wget http://data.githubarchive.org/2018-01-01-{0..10}.json.gz   \\\n  gunzip -k *\n# sample line\nhead -n 1 2018-01-01-0.json | jq '.'\n\n# run job\nspark-submit \\\n  --class  com.github.niqdev.App  \\\n  --master local[*] \\\n  /vagrant/example/spark/target/scala-2.11/spark-github_2.11-0.1.0-SNAPSHOT.jar", 
            "title": "Spark Job example"
        }, 
        {
            "location": "/hadoop/#oozie", 
            "text": "Oozie  is a workflow scheduler system to manage Hadoop jobs   Documentation   Oozie", 
            "title": "Oozie"
        }, 
        {
            "location": "/hadoop/#setup_1", 
            "text": "Oozie is not installed by default   Optional PostgreSQL configuration  - By default Oozie is configured to use Embedded Derby  # access master node\nvagrant ssh master\n\n# install docker\ncurl -fsSL get.docker.com -o get-docker.sh   \\\n  chmod u+x $_   \\\n  ./$_   \\\n  sudo usermod -aG docker hadoop\n\n# logout and login again to verify docker installation\nexit\nvagrant ssh master\nwhoami # hadoop\ndocker ps -a\n\n# uncomment PostgreSQL configurations\nvim devops/hadoop/file/oozie/config/oozie-site.xml # from host\nvim /vagrant/file/oozie/config/oozie-site.xml # from guest\n\n# start postgres on guest machine \ndocker run \\\n  --detach \\\n  --name oozie-postgres \\\n  -p 5432:5432 \\\n  -e POSTGRES_DB= oozie-db  \\\n  -e POSTGRES_USER= postgres  \\\n  -e POSTGRES_PASSWORD= password  \\\n  postgres\n\n# permission issue\n# https://github.com/docker-library/postgres/issues/116\n# --volume /vol/postgres:/var/lib/postgresql/data\n\n# access container\ndocker exec -it oozie-postgres bash\npsql --username=postgres\n# list all databases\n\\list\n\\connect oozie-db\n# list all tables\n\\dt\n# describe table\n\\d+ wf_jobs\n# list workflow\nselect * from wf_jobs;  Install and start Oozie  # access master node\nvagrant ssh master\n\n# login as root\nsudo su -\n\n# build, install and init\n/vagrant/script/setup_oozie.sh\n\n# start oozie manually (first time only)\nsu --login hadoop /vagrant/script/bootstrap.sh oozie  It might take a while to build the sources  Useful paths  # data and logs\ndevops/hadoop/.data/master/oozie # host\n/vol/oozie # guest\n\n# (guest) config\n/usr/local/oozie/conf\n\n# (hdfs) examples\n/user/hadoop/examples", 
            "title": "Setup"
        }, 
        {
            "location": "/hadoop/#examples", 
            "text": "Run bundled examples within distribution  # examples path\n.data/master/oozie/examples # host\n/vol/oozie/examples # guest\n\n# access master node as hadoop user\nvagrant ssh master\n\nexport OOZIE_EXAMPLE_PATH=/vol/oozie/examples\nexport OOZIE_HDFS_PATH=/user/$(whoami)/examples\n\n# open map-reduce job.properties\nvim $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties\n\n# edit the following properties\nnameNode=hdfs://namenode.local:9000 # fs.defaultFS @ core-site.xml\njobTracker=resource-manager.local:8032 # yarn.resourcemanager.address @ yarn-site.xml\nqueueName=priority_queue # or default @ fair-scheduler.xml\n\n# upload all the examples\nhadoop fs -put $OOZIE_EXAMPLE_PATH $OOZIE_HDFS_PATH\n\n# verify uploaded files\nhadoop fs -ls -h -R /user/$(whoami)\n\n# run the map-reduce workflow example\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -config $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties \\\n  -run\n\n# verify status\noozie job -oozie http://oozie.local:11000/oozie -info WORKFLOW_ID\n\n# verify result\nhadoop fs -cat $OOZIE_HDFS_PATH/output-data/map-reduce/part-00000\n\n# remove all the examples\nhadoop fs -rm -R $OOZIE_HDFS_PATH", 
            "title": "Examples"
        }, 
        {
            "location": "/hadoop/#useful-commands", 
            "text": "Workflow requires  oozie.wf.application.path  property  Coordinator requires  oozie.coord.application.path  property   # verify oozie status\noozie admin \\\n  -oozie http://oozie.local:11000/oozie \\\n  -status\n\n# verify workflow or coordinator status\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -info JOB_ID \\\n  -verbose\n\n# poll workflow or coordinator status\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -poll JOB_ID \\\n  -interval 10 \\\n  -timeout 60 \\\n  -verbose", 
            "title": "Useful commands"
        }, 
        {
            "location": "/readings/", 
            "text": "Papers\n\n\n\n\n\n\nThe Google File System\n\n\n\n\n\n\nMapReduce\n\n\n\n\n\n\nRaft\n\n\n\n\n\n\nPaxos\n\n\n\n\n\n\nZab\n\n\n\n\n\n\nChubby\n\n\n\n\n\n\nSpanner\n\n\n\n\n\n\nDynamo\n\n\n\n\n\n\nHyperLogLog\n\n\n\n\n\n\nDapper\n\n\n\n\n\n\nHarvest, Yield, and Scalable Tolerant Systems\n\n\n\n\n\n\nLife beyond Distributed Transactions\n\n\n\n\n\n\nThe \u03d5 Accrual Failure Detector\n\n\n\n\n\n\nConflict-free Replicated Data Types\n\n\n\n\n\n\nMerkle Hash Tree\n\n\n\n\n\n\nWhat Every Programmer Should Know About Memory\n\n\n\n\n\n\nStatistically Rigorous Java Performance Evaluation\n\n\n\n\n\n\n\n\nArticles\n\n\nCAP Theorem\n\n\n\n\n\n\nBrewer's CAP Theorem\n\n\n\n\n\n\nCAP Twelve Years Later: How the \"Rules\" Have Changed\n\n\n\n\n\n\n\nPlease stop calling databases CP or AP\n\n\n\n\n\n\n\n\nBooks\n\n\n\n\n\n\nHow Linux Works\n (2014)(2nd) by Brian Ward\n\n\n\n\n\n\nDocker in Action\n (2016) by Jeff Nickoloff\n\n\n\n\n\n\nDesigning Data-Intensive Applications\n (2017) by Martin Kleppmann\n\n\n\n\n\n\nHadoop: The Definitive Guide\n (2015)(4th) by Tom White\n\n\n\n\n\n\nSpark in Action\n (2016) by Petar Ze\u010devi\u0107 and Marko Bona\u0107i\n\n\n\n\n\n\nCassandra: The Definitive Guide\n (2016)(4th) by Eben Hewitt, Jeff Carpenter\n\n\n\n\n\n\nKafka: The Definitive Guide\n (2017) by Gwen Shapira, Neha Narkhede, Todd Palino\n\n\n\n\n\n\nAddison-Wesley Professional Computing Series\n\n\n\n\n\n\nScala\n\n\n\n\n\n\nProgramming in Scala\n (2016)(3rd) by Martin Odersky, Lex Spoon, and Bill Venners\n\n\n\n\n\n\nFunctional Programming in Scala\n (2014) by Paul Chiusano and Runar Bjarnason\n\n\n\n\n\n\nAkka in Action\n (2016) by Raymond Roestenburg, Rob Bakker, and Rob Williams", 
            "title": "Readings"
        }, 
        {
            "location": "/readings/#papers", 
            "text": "The Google File System    MapReduce    Raft    Paxos    Zab    Chubby    Spanner    Dynamo    HyperLogLog    Dapper    Harvest, Yield, and Scalable Tolerant Systems    Life beyond Distributed Transactions    The \u03d5 Accrual Failure Detector    Conflict-free Replicated Data Types    Merkle Hash Tree    What Every Programmer Should Know About Memory    Statistically Rigorous Java Performance Evaluation", 
            "title": "Papers"
        }, 
        {
            "location": "/readings/#articles", 
            "text": "CAP Theorem    Brewer's CAP Theorem    CAP Twelve Years Later: How the \"Rules\" Have Changed    Please stop calling databases CP or AP", 
            "title": "Articles"
        }, 
        {
            "location": "/readings/#books", 
            "text": "How Linux Works  (2014)(2nd) by Brian Ward    Docker in Action  (2016) by Jeff Nickoloff    Designing Data-Intensive Applications  (2017) by Martin Kleppmann    Hadoop: The Definitive Guide  (2015)(4th) by Tom White    Spark in Action  (2016) by Petar Ze\u010devi\u0107 and Marko Bona\u0107i    Cassandra: The Definitive Guide  (2016)(4th) by Eben Hewitt, Jeff Carpenter    Kafka: The Definitive Guide  (2017) by Gwen Shapira, Neha Narkhede, Todd Palino    Addison-Wesley Professional Computing Series    Scala    Programming in Scala  (2016)(3rd) by Martin Odersky, Lex Spoon, and Bill Venners    Functional Programming in Scala  (2014) by Paul Chiusano and Runar Bjarnason    Akka in Action  (2016) by Raymond Roestenburg, Rob Bakker, and Rob Williams", 
            "title": "Books"
        }, 
        {
            "location": "/other/", 
            "text": "Online tools\n\n\n\n\nRegular Expressions\n\n\nCurrent Millis\n\n\nJSFiddle\n\n\nScalaFiddle\n\n\nJSON Formatter\n\n\nBeautify JavaScript or HTML\n\n\n\n\n\n\nVagrant\n\n\n\n\nVagrant\n is a tool for building and managing virtual machine environments in a single workflow\n\n\n\n\nDocumentation\n\n\n\n\nVagrant\n\n\nVirtualBox\n\n\n\n\nSetup project creating a Vagrantfile\n\n\nvagrant init\n\n\n\n\nBoot and connect to the default virtual machine\n\n\nvagrant up\nvagrant status\nvagrant ssh\n\n\n\n\nUseful commands\n\n\n# shut down gracefully\nvagrant halt\n\n# reload (halt + up) + re-provision\nvagrant reload --provision\n\n# update box\nvagrant box update\n\n# delete virtual machine without prompt\nvagrant destory -f\n\n\n\n\n\n\nMkDocs\n\n\n\n\nMkDocs\n is a static site generator\n\n\n\n\nDocumentation\n\n\n\n\nMkDocs\n\n\n\n\nInstall\n\n\npip install mkdocs\n\n\n\n\nUseful commands\n\n\n# setup in current directory\nmkdocs new .\n\n# start dev server with hot reload @ http://127.0.0.1:8000\nmkdocs serve\n\n# build static site\nmkdocs build --clean\n\n# deploy to github\nmkdocs gh-deploy\n\n\n\n\n\n\nSDKMAN!\n\n\n\n\nSDKMAN!\n is a tool for managing parallel versions of multiple Software Development Kits on most Unix based systems\n\n\n\n\nDocumentation\n\n\n\n\nSDKMAN!\n\n\n\n\nSetup\n\n\ncurl -s \nhttps://get.sdkman.io\n | bash\nsource \n$HOME/.sdkman/bin/sdkman-init.sh\n\nsdk version\n\n\n\n\nGradle\n\n\n# setup\nsdk list gradle\nsdk install gradle 4.4.1\ngradle -version\n\n# create Gradle project\nmkdir -p PROJECT_NAME \n cd $_\ngradle init --type java-library\n\n./gradlew clean build\n\n\n\n\nScala\n\n\n# setup sbt\nsdk list sbt\nsdk install sbt\nsbt sbtVersion\nsbt about\n\n# setup scala\nsdk list scala\nsdk install scala 2.11.8\nscala -version\n\n# sample project\nsbt new sbt/scala-seed.g8\n\n\n\n\n\n\nGiter8\n\n\n\n\nGiter8\n is a command line tool to generate files and directories from templates published on GitHub or any other git repository\n\n\n\n\nDocumentation\n\n\n\n\nGiter8\n\n\nTemplates\n\n\n\n\nSetup\n\n\n# install conscript\ncurl https://raw.githubusercontent.com/foundweekends/conscript/master/setup.sh | sh\nsource ~/.bashrc\n\n# install g8\ncs foundweekends/giter8\n\n\n\n\nExample\n\n\n# interactive\ng8 sbt/scala-seed.g8\n# non-interactive\ng8 sbt/scala-seed.g8 --name=my-new-website\n\n\n\n\n\n\nPython\n\n\nDocumentation\n\n\n\n\npip\n\n\nvirtualenv\n\n\nWhat is the difference between virtualenv | pyenv | virtualenvwrapper | venv ?\n\n\n\n\nSetup\n\n\n# search\napt-get update \n apt-cache search python | grep python2\n\n# setup python\napt-get install -y python2.7\napt-get install -y python3\n\n# install pip + setuptools\ncurl https://bootstrap.pypa.io/get-pip.py | python2.7 -\ncurl https://bootstrap.pypa.io/get-pip.py | python3 -\n\n# upgrade pip\npip install -U pip\n\n# install virtualenv globally \npip install virtualenv\n\n\n\n\nvirtualenv\n\n\n# create virtualenv\nvirtualenv venv\nvirtualenv -p python3 venv\nvirtualenv -p $(which python3) venv\n\n# activate virtualenv\nsource venv/bin/activate\n\n# verify virtualenv\nwhich python\npython --version\n\n# deactivate virtualenv\ndeactivate\n\n\n\n\npip\n\n\n# search package\npip search \npackage\n\n\n# install new package\npip install \npackage\n\n\n# update requirements with new packages\npip freeze \n requirements.txt\n\n# install all requirements\npip install -r requirements.txt\n\n\n\n\nOther\n\n\n# generate rc file\npylint --generate-rcfile \n .pylintrc\n\n# create module\ntouch app/{__init__,main}.py", 
            "title": "Other"
        }, 
        {
            "location": "/other/#online-tools", 
            "text": "Regular Expressions  Current Millis  JSFiddle  ScalaFiddle  JSON Formatter  Beautify JavaScript or HTML", 
            "title": "Online tools"
        }, 
        {
            "location": "/other/#vagrant", 
            "text": "Vagrant  is a tool for building and managing virtual machine environments in a single workflow   Documentation   Vagrant  VirtualBox   Setup project creating a Vagrantfile  vagrant init  Boot and connect to the default virtual machine  vagrant up\nvagrant status\nvagrant ssh  Useful commands  # shut down gracefully\nvagrant halt\n\n# reload (halt + up) + re-provision\nvagrant reload --provision\n\n# update box\nvagrant box update\n\n# delete virtual machine without prompt\nvagrant destory -f", 
            "title": "Vagrant"
        }, 
        {
            "location": "/other/#mkdocs", 
            "text": "MkDocs  is a static site generator   Documentation   MkDocs   Install  pip install mkdocs  Useful commands  # setup in current directory\nmkdocs new .\n\n# start dev server with hot reload @ http://127.0.0.1:8000\nmkdocs serve\n\n# build static site\nmkdocs build --clean\n\n# deploy to github\nmkdocs gh-deploy", 
            "title": "MkDocs"
        }, 
        {
            "location": "/other/#sdkman", 
            "text": "SDKMAN!  is a tool for managing parallel versions of multiple Software Development Kits on most Unix based systems   Documentation   SDKMAN!   Setup  curl -s  https://get.sdkman.io  | bash\nsource  $HOME/.sdkman/bin/sdkman-init.sh \nsdk version  Gradle  # setup\nsdk list gradle\nsdk install gradle 4.4.1\ngradle -version\n\n# create Gradle project\nmkdir -p PROJECT_NAME   cd $_\ngradle init --type java-library\n\n./gradlew clean build  Scala  # setup sbt\nsdk list sbt\nsdk install sbt\nsbt sbtVersion\nsbt about\n\n# setup scala\nsdk list scala\nsdk install scala 2.11.8\nscala -version\n\n# sample project\nsbt new sbt/scala-seed.g8", 
            "title": "SDKMAN!"
        }, 
        {
            "location": "/other/#giter8", 
            "text": "Giter8  is a command line tool to generate files and directories from templates published on GitHub or any other git repository   Documentation   Giter8  Templates   Setup  # install conscript\ncurl https://raw.githubusercontent.com/foundweekends/conscript/master/setup.sh | sh\nsource ~/.bashrc\n\n# install g8\ncs foundweekends/giter8  Example  # interactive\ng8 sbt/scala-seed.g8\n# non-interactive\ng8 sbt/scala-seed.g8 --name=my-new-website", 
            "title": "Giter8"
        }, 
        {
            "location": "/other/#python", 
            "text": "Documentation   pip  virtualenv  What is the difference between virtualenv | pyenv | virtualenvwrapper | venv ?   Setup  # search\napt-get update   apt-cache search python | grep python2\n\n# setup python\napt-get install -y python2.7\napt-get install -y python3\n\n# install pip + setuptools\ncurl https://bootstrap.pypa.io/get-pip.py | python2.7 -\ncurl https://bootstrap.pypa.io/get-pip.py | python3 -\n\n# upgrade pip\npip install -U pip\n\n# install virtualenv globally \npip install virtualenv  virtualenv  # create virtualenv\nvirtualenv venv\nvirtualenv -p python3 venv\nvirtualenv -p $(which python3) venv\n\n# activate virtualenv\nsource venv/bin/activate\n\n# verify virtualenv\nwhich python\npython --version\n\n# deactivate virtualenv\ndeactivate  pip  # search package\npip search  package \n\n# install new package\npip install  package \n\n# update requirements with new packages\npip freeze   requirements.txt\n\n# install all requirements\npip install -r requirements.txt  Other  # generate rc file\npylint --generate-rcfile   .pylintrc\n\n# create module\ntouch app/{__init__,main}.py", 
            "title": "Python"
        }
    ]
}