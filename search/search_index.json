{
    "docs": [
        {
            "location": "/", 
            "text": "DevOps\n\n\nA collection of Docker and Vagrant images, scripts and documentation mainly related to distributed systems for local development, learning purposes and quick prototyping.\n\n\n\n\nDocker\n\n\nAnsible\n\n\nCassandra\n\n\nZookeeper\n\n\nKafka\n\n\nHadoop\n\n\nPapers\n\n\nOther", 
            "title": "Home"
        }, 
        {
            "location": "/#devops", 
            "text": "A collection of Docker and Vagrant images, scripts and documentation mainly related to distributed systems for local development, learning purposes and quick prototyping.   Docker  Ansible  Cassandra  Zookeeper  Kafka  Hadoop  Papers  Other", 
            "title": "DevOps"
        }, 
        {
            "location": "/docker/", 
            "text": "Docker\n\n\n\n\nDocker\n is an open platform for developers and sysadmins to build, ship, and run distributed applications\n\n\n\n\nDocumentation\n\n\n\n\nDocker\n\n\n\n\nHow-To\n\n\nSetup\n\n\n# install docker\ncurl -fsSL get.docker.com -o get-docker.sh \n \\\n  chmod u+x $_ \n \\\n  ./$_ \n \\\n  sudo usermod -aG docker hadoop\n\ndocker --version\n\n# install docker-compose\nsudo curl -L https://github.com/docker/compose/releases/download/1.19.0/docker-compose-`uname -s`-`uname -m` \\\n  -o /usr/local/bin/docker-compose \n \\\n  sudo chmod +x /usr/local/bin/docker-compose\n\ndocker-compose --version\n\n# install docker-machine (VirtualBox required)\ncurl -L https://github.com/docker/machine/releases/download/v0.13.0/docker-machine-`uname -s`-`uname -m` \n/tmp/docker-machine \n \\\n  sudo install /tmp/docker-machine /usr/local/bin/docker-machine\n\ndocker-machine --version\n\n\n\n\nUseful commands\n\n\n# list images\ndocker images\n# list containers\ndocker ps -a\n# list volumes\ndocker volume ls\n\n# run temporary container\ndocker run --rm --name phusion phusion/baseimage:latest\n# access container from another shell\ndocker exec -it phusion bash\n\n# remove container by name\ndocker ps -a -q -f name=CONTAINER_NAME | xargs --no-run-if-empty docker rm -f\n# delete dangling images \nnone\n\ndocker images -q -f dangling=true | xargs --no-run-if-empty docker rmi\n# delete dangling volumes\ndocker volume ls -q -f dangling=true | xargs --no-run-if-empty docker volume rm\n\n\n\n\nDocker Machine\n\n\n# create local machine\ndocker-machine create --driver virtualbox default\n\n# list\ndocker-machine ls\ndocker-machine ls --filter name=default\ndocker-machine ls --filter state=Running\ndocker-machine ls --format \n{{.Name}}: {{.DriverName}} - {{.State}}\n\n\n# info\ndocker-machine inspect default\ndocker-machine inspect --format='{{.Driver.IPAddress}}' default\ndocker-machine status default\ndocker-machine ip default\n\n# management\ndocker-machine start default\ndocker-machine stop default\ndocker-machine restart default\ndocker-machine rm default\n\n# mount volume\n#https://docs.docker.com/machine/reference/mount\n\n# show command to connect to machine\ndocker-machine env default\n# check if variables are set\nenv | grep DOCKER\n\n# connect to machine\neval \n$(docker-machine env default)\n\ndocker ps -a\n\n# show command to disconnect from machine\ndocker-machine env -u\n# unset all\neval $(docker-machine env -u)\n\n# access\ndocker-machine ssh default\n# execute command and exit\ndocker-machine ssh default uptime\n# copy files from host to guest\ndocker-machine scp -r /FROM default:/TO\n\n# start nginx on default machine\ndocker run -d -p 8000:80 nginx\n# verify from host\ncurl $(docker-machine ip default):8000\n# forward to port 8080\ndocker-machine ssh default -L 8080:localhost:8000\n# verify tunnel from host\ncurl localhost:8080\n\n# disable error crash reporting\nmkdir -p ~/.docker/machine \n touch ~/.docker/machine/no-error-report\n\n\n\n\nBase image\n\n\nBuild \ndevops/base\n image\n\n\n# change path\ncd devops/base\n\n# build image\ndocker build -t devops/base .\n\n# temporary container\ndocker run --rm --name devops-base devops/base\n# access container\ndocker exec -it devops-base bash", 
            "title": "Docker"
        }, 
        {
            "location": "/docker/#docker", 
            "text": "Docker  is an open platform for developers and sysadmins to build, ship, and run distributed applications   Documentation   Docker", 
            "title": "Docker"
        }, 
        {
            "location": "/docker/#how-to", 
            "text": "Setup  # install docker\ncurl -fsSL get.docker.com -o get-docker.sh   \\\n  chmod u+x $_   \\\n  ./$_   \\\n  sudo usermod -aG docker hadoop\n\ndocker --version\n\n# install docker-compose\nsudo curl -L https://github.com/docker/compose/releases/download/1.19.0/docker-compose-`uname -s`-`uname -m` \\\n  -o /usr/local/bin/docker-compose   \\\n  sudo chmod +x /usr/local/bin/docker-compose\n\ndocker-compose --version\n\n# install docker-machine (VirtualBox required)\ncurl -L https://github.com/docker/machine/releases/download/v0.13.0/docker-machine-`uname -s`-`uname -m`  /tmp/docker-machine   \\\n  sudo install /tmp/docker-machine /usr/local/bin/docker-machine\n\ndocker-machine --version  Useful commands  # list images\ndocker images\n# list containers\ndocker ps -a\n# list volumes\ndocker volume ls\n\n# run temporary container\ndocker run --rm --name phusion phusion/baseimage:latest\n# access container from another shell\ndocker exec -it phusion bash\n\n# remove container by name\ndocker ps -a -q -f name=CONTAINER_NAME | xargs --no-run-if-empty docker rm -f\n# delete dangling images  none \ndocker images -q -f dangling=true | xargs --no-run-if-empty docker rmi\n# delete dangling volumes\ndocker volume ls -q -f dangling=true | xargs --no-run-if-empty docker volume rm  Docker Machine  # create local machine\ndocker-machine create --driver virtualbox default\n\n# list\ndocker-machine ls\ndocker-machine ls --filter name=default\ndocker-machine ls --filter state=Running\ndocker-machine ls --format  {{.Name}}: {{.DriverName}} - {{.State}} \n\n# info\ndocker-machine inspect default\ndocker-machine inspect --format='{{.Driver.IPAddress}}' default\ndocker-machine status default\ndocker-machine ip default\n\n# management\ndocker-machine start default\ndocker-machine stop default\ndocker-machine restart default\ndocker-machine rm default\n\n# mount volume\n#https://docs.docker.com/machine/reference/mount\n\n# show command to connect to machine\ndocker-machine env default\n# check if variables are set\nenv | grep DOCKER\n\n# connect to machine\neval  $(docker-machine env default) \ndocker ps -a\n\n# show command to disconnect from machine\ndocker-machine env -u\n# unset all\neval $(docker-machine env -u)\n\n# access\ndocker-machine ssh default\n# execute command and exit\ndocker-machine ssh default uptime\n# copy files from host to guest\ndocker-machine scp -r /FROM default:/TO\n\n# start nginx on default machine\ndocker run -d -p 8000:80 nginx\n# verify from host\ncurl $(docker-machine ip default):8000\n# forward to port 8080\ndocker-machine ssh default -L 8080:localhost:8000\n# verify tunnel from host\ncurl localhost:8080\n\n# disable error crash reporting\nmkdir -p ~/.docker/machine   touch ~/.docker/machine/no-error-report", 
            "title": "How-To"
        }, 
        {
            "location": "/docker/#base-image", 
            "text": "Build  devops/base  image  # change path\ncd devops/base\n\n# build image\ndocker build -t devops/base .\n\n# temporary container\ndocker run --rm --name devops-base devops/base\n# access container\ndocker exec -it devops-base bash", 
            "title": "Base image"
        }, 
        {
            "location": "/ansible/", 
            "text": "Ansible\n\n\n\n\nAnsible\n is an open source automation platform that can help with config management, deployment and task automation\n\n\n\n\nDocumentation\n\n\n\n\nAnsible\n\n\nTutorial\n\n\nPlaybook example\n\n\n\n\nThe following guide explains how to provision Ansible locally and play with it. Checkout the \nVagrantfile\n and the Vagrant \nguide\n for more details.\n\n\nSetup\n\n\nRequirement\n\n\n\n\nVagrant\n\n\nVirtualBox\n\n\n\n\nDirectory structure\n\n\ntree -a ansible/\nansible/\n\u251c\u2500\u2500 .share\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-3\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ssh\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ansible_rsa\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 ansible_rsa.pub\n\u251c\u2500\u2500 Vagrantfile\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 group_vars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 host_vars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hosts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 roles\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 common\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 defaults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 files\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 handlers\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 motd.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 package.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 templates\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 motd\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 vars\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 docker\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 site.yml\n\u251c\u2500\u2500 setup_ansible.sh\n\u2514\u2500\u2500 setup_share.sh\n\n\n\n\nThe first time \nonly\n, you have to setup the shared folders and generate the ssh key needed by ansible to access all nodes executing\n\n\n./setup_share.sh\n\n\n\n\nStart the boxes with\n\n\nvagrant up\n\n\n\n\nThe first time it could take a while\n\n\nVerify status of the boxes with\n\n\nvagrant status\n\n\n\n\nVerify access to the boxes with\n\n\nvagrant ssh ansible\nvagrant ssh node-1\n\n\n\n\nFrom inside the boxes you should be able to communicate with the others\n\n\nping ansible.local\nping ip-192-168-100-11.local\nping 192.168.100.12\n\n\n\n\nThe following paths are shared with the boxes\n\n\n\n\n/vagrant\n provision-tool\n\n\n/local\n host $HOME\n\n\n/ansible\n data \n(ansible only)\n\n\n/data\n .share \n(node only)\n\n\n\n\nAd-Hoc Commands\n\n\nAccess the ansible box with\n\n\nvagrant ssh ansible\n\n\n\n\nBelow a list of examples\n\n\n\n# ping all nodes (default inventory /etc/ansible/hosts)\nansible all -m ping\nansible ansible -m ping\nansible cluster -m ping\n\n# ping all nodes (specify inventory)\nansible all -i \n/vagrant/data/hosts\n -m ping\n\n# gathering facts\nansible all -m setup\nansible ansible -m setup\n\n# specify host and user\nansible ip-192-168-100-11.local -m ping -u vagrant\n\n# execute command\nansible all -a \n/bin/echo hello\n\nansible all -a \nuptime\n\nansible all -a \n/bin/date\n\n# do NOT reboot vagrant through ansible (use vagrant reload)\nansible cluster -a \n/sbin/reboot\n --become\n\n# shell module\nansible all -m shell -a \npwd\n\n# be carefull to quotes\nansible all -m shell -a 'echo $HOME'\n\n# update \n upgrade\nansible all -m apt -a \nupdate_cache=yes upgrade=dist\n --become\n# restart after upgrade\nvagrant reload\n# install package\nansible all -m apt -a \nname=tree state=present\n --become\n\n\n\n\nPlaybooks\n\n\nAccess the ansible box with\n\n\nvagrant ssh ansible\n\n\n\n\nBelow a list of examples\n\n\n# test uptime on all node\nansible-playbook /ansible/site.yml --tags=test --verbose\n\n# update \n upgrade only cluster nodes\nansible-playbook /ansible/site.yml -t package --skip-tags=java --verbose\n\n# install packages on cluster nodes\nansible-playbook /ansible/site.yml -t package --verbose\n\n# run common task on cluster node\nansible-playbook /ansible/site.yml -t common\n\n# setup docker\nansible-playbook /ansible/site.yml -t docker\n# test docker\nvagrant ssh node-1\nsudo -i -u docker\ndocker ps -a\n\n# custom banner\nansible-playbook /ansible/site.yml -t motd\n\n# setup all infrastructure at once\nansible-playbook /ansible/site.yml\n\n# dry run\nansible-playbook -i /ansible/hosts /ansible/site.yml --check --diff", 
            "title": "Ansible"
        }, 
        {
            "location": "/ansible/#ansible", 
            "text": "Ansible  is an open source automation platform that can help with config management, deployment and task automation   Documentation   Ansible  Tutorial  Playbook example   The following guide explains how to provision Ansible locally and play with it. Checkout the  Vagrantfile  and the Vagrant  guide  for more details.", 
            "title": "Ansible"
        }, 
        {
            "location": "/ansible/#setup", 
            "text": "Requirement   Vagrant  VirtualBox   Directory structure  tree -a ansible/\nansible/\n\u251c\u2500\u2500 .share\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-3\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ssh\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ansible_rsa\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 ansible_rsa.pub\n\u251c\u2500\u2500 Vagrantfile\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 group_vars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 host_vars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hosts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 roles\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 common\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 defaults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 files\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 handlers\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 motd.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 package.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 templates\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 motd\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 vars\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 docker\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 site.yml\n\u251c\u2500\u2500 setup_ansible.sh\n\u2514\u2500\u2500 setup_share.sh  The first time  only , you have to setup the shared folders and generate the ssh key needed by ansible to access all nodes executing  ./setup_share.sh  Start the boxes with  vagrant up  The first time it could take a while  Verify status of the boxes with  vagrant status  Verify access to the boxes with  vagrant ssh ansible\nvagrant ssh node-1  From inside the boxes you should be able to communicate with the others  ping ansible.local\nping ip-192-168-100-11.local\nping 192.168.100.12  The following paths are shared with the boxes   /vagrant  provision-tool  /local  host $HOME  /ansible  data  (ansible only)  /data  .share  (node only)", 
            "title": "Setup"
        }, 
        {
            "location": "/ansible/#ad-hoc-commands", 
            "text": "Access the ansible box with  vagrant ssh ansible  Below a list of examples  \n# ping all nodes (default inventory /etc/ansible/hosts)\nansible all -m ping\nansible ansible -m ping\nansible cluster -m ping\n\n# ping all nodes (specify inventory)\nansible all -i  /vagrant/data/hosts  -m ping\n\n# gathering facts\nansible all -m setup\nansible ansible -m setup\n\n# specify host and user\nansible ip-192-168-100-11.local -m ping -u vagrant\n\n# execute command\nansible all -a  /bin/echo hello \nansible all -a  uptime \nansible all -a  /bin/date \n# do NOT reboot vagrant through ansible (use vagrant reload)\nansible cluster -a  /sbin/reboot  --become\n\n# shell module\nansible all -m shell -a  pwd \n# be carefull to quotes\nansible all -m shell -a 'echo $HOME'\n\n# update   upgrade\nansible all -m apt -a  update_cache=yes upgrade=dist  --become\n# restart after upgrade\nvagrant reload\n# install package\nansible all -m apt -a  name=tree state=present  --become", 
            "title": "Ad-Hoc Commands"
        }, 
        {
            "location": "/ansible/#playbooks", 
            "text": "Access the ansible box with  vagrant ssh ansible  Below a list of examples  # test uptime on all node\nansible-playbook /ansible/site.yml --tags=test --verbose\n\n# update   upgrade only cluster nodes\nansible-playbook /ansible/site.yml -t package --skip-tags=java --verbose\n\n# install packages on cluster nodes\nansible-playbook /ansible/site.yml -t package --verbose\n\n# run common task on cluster node\nansible-playbook /ansible/site.yml -t common\n\n# setup docker\nansible-playbook /ansible/site.yml -t docker\n# test docker\nvagrant ssh node-1\nsudo -i -u docker\ndocker ps -a\n\n# custom banner\nansible-playbook /ansible/site.yml -t motd\n\n# setup all infrastructure at once\nansible-playbook /ansible/site.yml\n\n# dry run\nansible-playbook -i /ansible/hosts /ansible/site.yml --check --diff", 
            "title": "Playbooks"
        }, 
        {
            "location": "/cassandra/", 
            "text": "Cassandra\n\n\n\n\nCassandra\n is a distributed database for managing large amounts of structured data across many commodity servers, while providing highly available service and no single point of failure\n\n\n\n\nDocumentation\n\n\n\n\nCassandra\n\n\n\n\nSetup\n\n\nSingle Node Cluster\n\n\n# change path\ncd devops/cassandra\n\n# start single node\ndocker-compose up\n\n# paths\n/etc/cassandra\n/var/lib/cassandra\n/var/log/cassandra\n\n# remove container and volume\ndocker rm -fv devops-cassandra\n\n\n\n\nMulti Node Cluster\n\n\n# change path\ncd devops/cassandra\n\n# start node\ndocker-compose -f docker-compose-cluster.yml up\n\n# optional mounted volumes\nmkdir -p \\\n  .cassandra/cassandra-seed/{data,log} \\\n  .cassandra/cassandra-node-1/{data,log} \\\n  .cassandra/cassandra-node-2/{data,log}\ntree .cassandra/\n\n# ISSUES releated to host permissions\n# \n Small commitlog volume detected at /var/lib/cassandra/commitlog\n# \n There is insufficient memory for the Java Runtime Environment to continue\n(cassandra) /var/lib/cassandra\n(root) /var/log/cassandra\n\n\n\n\nExample\n\n\n# execute cql from host\n(docker exec -i devops-cassandra bash \\\n  -c \ncat \n example.cql; cqlsh -f example.cql\n) \n cql/example.cql\n\n# access container\ndocker exec -it devops-cassandra bash\ndocker exec -it devops-cassandra bash -c cqlsh\ndocker exec -it devops-cassandra-seed bash\ndocker exec -it devops-cassandra-node-1 bash\n\nDESCRIBE keyspaces;\nDESCRIBE KEYSPACE example;\nDESCRIBE TABLE example.messages;\nSELECT * FROM example.messages;", 
            "title": "Cassandra"
        }, 
        {
            "location": "/cassandra/#cassandra", 
            "text": "Cassandra  is a distributed database for managing large amounts of structured data across many commodity servers, while providing highly available service and no single point of failure   Documentation   Cassandra", 
            "title": "Cassandra"
        }, 
        {
            "location": "/cassandra/#setup", 
            "text": "Single Node Cluster  # change path\ncd devops/cassandra\n\n# start single node\ndocker-compose up\n\n# paths\n/etc/cassandra\n/var/lib/cassandra\n/var/log/cassandra\n\n# remove container and volume\ndocker rm -fv devops-cassandra  Multi Node Cluster  # change path\ncd devops/cassandra\n\n# start node\ndocker-compose -f docker-compose-cluster.yml up\n\n# optional mounted volumes\nmkdir -p \\\n  .cassandra/cassandra-seed/{data,log} \\\n  .cassandra/cassandra-node-1/{data,log} \\\n  .cassandra/cassandra-node-2/{data,log}\ntree .cassandra/\n\n# ISSUES releated to host permissions\n#   Small commitlog volume detected at /var/lib/cassandra/commitlog\n#   There is insufficient memory for the Java Runtime Environment to continue\n(cassandra) /var/lib/cassandra\n(root) /var/log/cassandra  Example  # execute cql from host\n(docker exec -i devops-cassandra bash \\\n  -c  cat   example.cql; cqlsh -f example.cql )   cql/example.cql\n\n# access container\ndocker exec -it devops-cassandra bash\ndocker exec -it devops-cassandra bash -c cqlsh\ndocker exec -it devops-cassandra-seed bash\ndocker exec -it devops-cassandra-node-1 bash\n\nDESCRIBE keyspaces;\nDESCRIBE KEYSPACE example;\nDESCRIBE TABLE example.messages;\nSELECT * FROM example.messages;", 
            "title": "Setup"
        }, 
        {
            "location": "/zookeeper/", 
            "text": "ZooKeeper\n\n\n\n\nZooKeeper\n is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\n\n\n\n\nDocumentation\n\n\n\n\nZooKeeper\n\n\nCurator\n\n\n\n\nSetup\n\n\nRequirement\n\n\n\n\nBase\n image\n\n\n\n\nBuild \ndevops/zookeeper\n image\n\n\n# change path\ncd devops/zookeeper\n\n# build image\ndocker build -t devops/zookeeper:latest .\n# build image with specific version - see Dockerfile for version 3.5.x\ndocker build -t devops/zookeeper:3.4.10 --build-arg VERSION=3.4.10 .\n\n# temporary container [host:container]\ndocker run --rm --name zookeeper -p 12181:2181 devops/zookeeper\n# access container\ndocker exec -it zookeeper bash\n\n# paths\n/opt/zookeeper\n/var/log/zookeeper\n/var/lib/zookeeper\n/var/log/supervisord.log\n\n# logs\ntail -F /var/log/supervisord.log\n# check service status\nsupervisorctl status\nsupervisorctl restart zookeeper\n\n\n\n\nExample\n\n\ndocker exec -it zookeeper bash\n\n# (option 1) check zookeeper status\necho ruok | nc localhost 2181\n\n# (option 2) check zookeeper status\ntelnet localhost 2181\n# expect answer imok\n\n ruok\n\nzkCli.sh -server 127.0.0.1:2181\nhelp\n# list znodes\nls /\n# create znode and associate value\ncreate /zk_test my_data\n# verify data\nget /zk_test\n# change value\nset /zk_test junk\n# delete znode\ndelete /zk_test\n\n\n\n\nThe four-letter words\n\n\n\n\n\n\n\n\nCategory\n\n\nCommand\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nServer status\n\n\nruok\n\n\nPrints \nimok\n if the server is running and not in an error state\n\n\n\n\n\n\n\n\nconf\n\n\nPrints the server configuration (from zoo.cfg)\n\n\n\n\n\n\n\n\nenvi\n\n\nPrints the server environment, including ZooKeeper version, Java version, and other system properties\n\n\n\n\n\n\n\n\nsrvr\n\n\nPrints server statistics, including latency statistics, the number of znodes, and the server mode (standalone, leader, or follower)\n\n\n\n\n\n\n\n\nstat\n\n\nPrints server statistics and connected clients\n\n\n\n\n\n\n\n\nsrst\n\n\nResets server statistics\n\n\n\n\n\n\n\n\nisro\n\n\nShows whether the server is in read-only ( ro ) mode (due to a network partition) or read/write mode (rw)\n\n\n\n\n\n\nClient connections\n\n\ndump\n\n\nLists all the sessions and ephemeral znodes for the ensemble. You must connect to the leader (see srvr) for this command\n\n\n\n\n\n\n\n\ncons\n\n\nLists connection statistics for all the server's clients\n\n\n\n\n\n\n\n\ncrst\n\n\nResets connection statistics\n\n\n\n\n\n\nWatches\n\n\nwchs\n\n\nLists summary information for the server's watches\n\n\n\n\n\n\n\n\nwchc\n\n\nLists all the server's watches by connection, may impact server performance for a large number of watches\n\n\n\n\n\n\n\n\nwchp\n\n\nLists all the server\u2019s watches by znode path, may impact server performance for a large number of watches\n\n\n\n\n\n\nMonitoring\n\n\nmntr\n\n\nLists server statistics in Java properties format, suitable as a source for monitoring systems such as Ganglia and Nagios", 
            "title": "ZooKeeper"
        }, 
        {
            "location": "/zookeeper/#zookeeper", 
            "text": "ZooKeeper  is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services   Documentation   ZooKeeper  Curator", 
            "title": "ZooKeeper"
        }, 
        {
            "location": "/zookeeper/#setup", 
            "text": "Requirement   Base  image   Build  devops/zookeeper  image  # change path\ncd devops/zookeeper\n\n# build image\ndocker build -t devops/zookeeper:latest .\n# build image with specific version - see Dockerfile for version 3.5.x\ndocker build -t devops/zookeeper:3.4.10 --build-arg VERSION=3.4.10 .\n\n# temporary container [host:container]\ndocker run --rm --name zookeeper -p 12181:2181 devops/zookeeper\n# access container\ndocker exec -it zookeeper bash\n\n# paths\n/opt/zookeeper\n/var/log/zookeeper\n/var/lib/zookeeper\n/var/log/supervisord.log\n\n# logs\ntail -F /var/log/supervisord.log\n# check service status\nsupervisorctl status\nsupervisorctl restart zookeeper  Example  docker exec -it zookeeper bash\n\n# (option 1) check zookeeper status\necho ruok | nc localhost 2181\n\n# (option 2) check zookeeper status\ntelnet localhost 2181\n# expect answer imok  ruok\n\nzkCli.sh -server 127.0.0.1:2181\nhelp\n# list znodes\nls /\n# create znode and associate value\ncreate /zk_test my_data\n# verify data\nget /zk_test\n# change value\nset /zk_test junk\n# delete znode\ndelete /zk_test", 
            "title": "Setup"
        }, 
        {
            "location": "/zookeeper/#the-four-letter-words", 
            "text": "Category  Command  Description      Server status  ruok  Prints  imok  if the server is running and not in an error state     conf  Prints the server configuration (from zoo.cfg)     envi  Prints the server environment, including ZooKeeper version, Java version, and other system properties     srvr  Prints server statistics, including latency statistics, the number of znodes, and the server mode (standalone, leader, or follower)     stat  Prints server statistics and connected clients     srst  Resets server statistics     isro  Shows whether the server is in read-only ( ro ) mode (due to a network partition) or read/write mode (rw)    Client connections  dump  Lists all the sessions and ephemeral znodes for the ensemble. You must connect to the leader (see srvr) for this command     cons  Lists connection statistics for all the server's clients     crst  Resets connection statistics    Watches  wchs  Lists summary information for the server's watches     wchc  Lists all the server's watches by connection, may impact server performance for a large number of watches     wchp  Lists all the server\u2019s watches by znode path, may impact server performance for a large number of watches    Monitoring  mntr  Lists server statistics in Java properties format, suitable as a source for monitoring systems such as Ganglia and Nagios", 
            "title": "The four-letter words"
        }, 
        {
            "location": "/kafka/", 
            "text": "Kafka\n\n\n\n\nKafka\n is a distributed streaming platform\n\n\n\n\nDocumentation\n\n\n\n\nKafka\n\n\n\n\nSetup\n\n\nRequirement\n\n\n\n\nBase\n docker image \n\n\nZooKeeper\n docker image\n\n\n\n\nBuild \ndevops/kafka\n image\n\n\n# change path\ncd devops/kafka\n\n# build image\ndocker build -t devops/kafka .\n\n# create network\ndocker network create --driver bridge my_network\ndocker network ls\ndocker network inspect my_network\n\n# start temporary zookeeper container [host:container]\ndocker run --rm \\\n  --name zookeeper \\\n  -p 12181:2181 \\\n  --network=my_network \\\n  devops/zookeeper\n# access container\ndocker exec -it zookeeper bash\n\n# start temporary kafka container [host:container]\ndocker run --rm \\\n  --name kafka \\\n  -p 19092:9092 \\\n  --network=my_network \\\n  -e ZOOKEEPER_HOSTS=\nzookeeper:2181\n \\\n  devops/kafka\n# access container\ndocker exec -it kafka bash\n\n# paths\n/opt/kafka\n/opt/kafka/logs\n/var/log/kafka\n/var/lib/kafka/data\n\n\n\n\nAlternatively use \ndocker-compose\n\n\n# change path\ncd devops/kafka\n\n# build base image\ndocker build -t devops/base ../base\n# build + start zookeeper and kafka\ndocker-compose up\n\n# access container\ndocker exec -it devops-zookeeper bash\ndocker exec -it devops-kafka bash\n\n\n\n\nExample\n\n\ndocker exec -it kafka bash\ncd /opt/kafka/bin\n\n./kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic test\n./kafka-topics.sh --list --zookeeper zookeeper:2181\n./kafka-console-producer.sh --broker-list kafka:9092 --topic test\n./kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic test --from-beginning", 
            "title": "Kafka"
        }, 
        {
            "location": "/kafka/#kafka", 
            "text": "Kafka  is a distributed streaming platform   Documentation   Kafka", 
            "title": "Kafka"
        }, 
        {
            "location": "/kafka/#setup", 
            "text": "Requirement   Base  docker image   ZooKeeper  docker image   Build  devops/kafka  image  # change path\ncd devops/kafka\n\n# build image\ndocker build -t devops/kafka .\n\n# create network\ndocker network create --driver bridge my_network\ndocker network ls\ndocker network inspect my_network\n\n# start temporary zookeeper container [host:container]\ndocker run --rm \\\n  --name zookeeper \\\n  -p 12181:2181 \\\n  --network=my_network \\\n  devops/zookeeper\n# access container\ndocker exec -it zookeeper bash\n\n# start temporary kafka container [host:container]\ndocker run --rm \\\n  --name kafka \\\n  -p 19092:9092 \\\n  --network=my_network \\\n  -e ZOOKEEPER_HOSTS= zookeeper:2181  \\\n  devops/kafka\n# access container\ndocker exec -it kafka bash\n\n# paths\n/opt/kafka\n/opt/kafka/logs\n/var/log/kafka\n/var/lib/kafka/data  Alternatively use  docker-compose  # change path\ncd devops/kafka\n\n# build base image\ndocker build -t devops/base ../base\n# build + start zookeeper and kafka\ndocker-compose up\n\n# access container\ndocker exec -it devops-zookeeper bash\ndocker exec -it devops-kafka bash  Example  docker exec -it kafka bash\ncd /opt/kafka/bin\n\n./kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic test\n./kafka-topics.sh --list --zookeeper zookeeper:2181\n./kafka-console-producer.sh --broker-list kafka:9092 --topic test\n./kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic test --from-beginning", 
            "title": "Setup"
        }, 
        {
            "location": "/hadoop/", 
            "text": "Hadoop\n\n\nThe following guide explains how to provision a Multi Node Hadoop Cluster locally and play with it. Checkout the \nVagrantfile\n and the Vagrant \nguide\n for more details.\n\n\nDocumentation\n\n\n\n\nHadoop\n\n\nThe Hadoop Ecosystem Table\n\n\n\n\nSetup\n\n\nRequirements\n\n\n\n\nVagrant\n\n\nVirtualBox\n\n\n\n\nDirectory structure\n\n\ntree -a hadoop/\nhadoop/\n\u251c\u2500\u2500 .data # mounted volume\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop_rsa\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop_rsa.pub\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 master\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 mapred\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 yarn\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 namenode\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 secondary\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 oozie\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0  \u00a0\u00a0 \u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0  \u00a0\u00a0 \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 datanode\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 mapred\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u2514\u2500\u2500 yarn\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-3\n\u251c\u2500\u2500 example\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 map-reduce\n\u251c\u2500\u2500 file\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 core-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fair-scheduler.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hdfs-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 mapred-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 masters\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 slaves\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 yarn-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-hadoop.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hosts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 motd\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie-env.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 oozie-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-oozie.sh\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ssh\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 config\n\u251c\u2500\u2500 script\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bootstrap.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_hadoop.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_oozie.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_spark.sh\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 setup_ubuntu.sh\n\u251c\u2500\u2500 Vagrantfile\n\u2514\u2500\u2500 vagrant_hadoop.sh\n\n\n\n\nImport the script\n\n\nsource vagrant_hadoop.sh\n\n\n\n\nCreate and start a Multi Node Hadoop Cluster\n\n\nhadoop-start\n\n\n\n\nThe first time it might take a while\n\n\nAccess the cluster via ssh, check also the \n/etc/hosts\n file\n\n\nvagrant ssh master\nssh hadoop@172.16.0.10 -i .data/hadoop_rsa\n\n# 3 nodes\nvagrant ssh node-1\nssh hadoop@172.16.0.101 -i .data/hadoop_rsa\n\n\n\n\nDestroy the cluster\n\n\nhadoop-destroy\n\n\n\n\nFor convenience add to the host machine\n\n\ncat hadoop/file/hosts | sudo tee --append /etc/hosts\n\n\n\n\nWeb UI links\n\n\n\n\nNameNode: \nhttp://namenode.local:50070\n\n\nNameNode metrics: \nhttp://namenode.local:50070/jmx\n\n\nResourceManager: \nhttp://resource-manager.local:8088\n\n\nLog Level: \nhttp://resource-manager.local:8088/logLevel\n\n\nWeb Application Proxy Server: \nhttp://web-proxy.local:8100/proxy/application_XXX_0000\n\n\nMapReduce Job History Server: \nhttp://history.local:19888\n\n\nDataNode/NodeManager (1): \nhttp://node-1.local:8042/node\n\n\nDataNode/NodeManager (2): \nhttp://node-2.local:8042/node\n\n\nDataNode/NodeManager (3): \nhttp://node-3.local:8042/node\n\n\nSpark: \nhttp://spark.local:4040\n\n\nOozie*: \nhttp://oozie.local:11000\n\n\n\n\n\n\nHDFS and MapReduce\n\n\n\n\nHDFS\n is a distributed file system that provides high-throughput access to application data\n\n\nYARN\n is a framework for job scheduling and cluster resource management\n\n\nMapReduce\n is a YARN-based system for parallel processing of large data sets\n\n\n\n\nDocumentation\n\n\n\n\nHadoop v2.7.5\n\n\nUntangling Apache Hadoop YARN\n series\n\n\n\n\nHDFS Admin\n\n\n# filesystem statistics\nhdfs dfsadmin -report\n\n# filesystem check\nhdfs fsck /\n\n\n\n\nUseful paths\n\n\n# data and logs\ndevops/hadoop/.data/master/hadoop # host\n/vol/hadoop # guest\n\n# (guest) config\n/usr/local/hadoop/etc/hadoop\n\n# (hdfs) map-reduce history\n/mr-history/history/done_intermediate/hadoop\n\n# (hdfs) aggregated app logs\n/yarn/app/hadoop/logs/application_XXX\n\n\n\n\nMapReduce WordCount Job\n\n\n# build jar on the host machine\ncd devops/hadoop/example/map-reduce\n./gradlew clean build\n\ncd devops/hadoop\nvagrant ssh master\n\n# create base directory using hdfs\nhdfs dfs -mkdir -p /user/ubuntu\n\n# create example directory\nhadoop fs -mkdir -p /user/ubuntu/word-count/input\n\n# list directory\nhadoop fs -ls -h -R /\nhadoop fs -ls -h -R /user/ubuntu\n\n# create sample files\necho \nHello World Bye World\n \n file01\necho \nHello Hadoop Goodbye Hadoop\n \n file02\n\n# copy from local to hdfs\nhadoop fs -copyFromLocal file01 /user/ubuntu/word-count/input\nhadoop fs -put file02 /user/ubuntu/word-count/input\n\n# verify copied files\nhadoop fs -ls -h -R /user/ubuntu\nhadoop fs -cat /user/ubuntu/word-count/input/file01\nhadoop fs -cat /user/ubuntu/word-count/input/file02\nhadoop fs -cat /user/ubuntu/word-count/input/*\n\n# run application\nhadoop jar /vagrant/example/map-reduce/build/libs/map-reduce.jar \\\n  /user/ubuntu/word-count/input \\\n  /user/ubuntu/word-count/output\n\n# check output\nhadoop fs -cat /user/ubuntu/word-count/output/part-r-00000\n\n# delete directory to run it again\nhadoop fs -rm -R /user/ubuntu/word-count/output\n\n# run sample job in a different queue\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  wordcount \\\n  -Dmapreduce.job.queuename=root.priority_queue \\\n  /user/ubuntu/word-count/input \\\n  /user/ubuntu/word-count/output\n\n# well known WARN issue\n# https://issues.apache.org/jira/browse/HDFS-10429\n\n\n\n\nBenchmarking MapReduce with TeraSort\n\n\n# generate random data\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  teragen 1000 random-data\n\n# run terasort benchmark\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  terasort random-data sorted-data\n\n# validate data\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  teravalidate sorted-data report\n\n# useful commands\nhadoop fs -ls -h -R .\nhadoop fs -rm -r random-data\nhadoop fs -cat random-data/part-m-00000\nhadoop fs -cat sorted-data/part-r-00000\n\n\n\n\n\n\nSpark\n\n\n\n\nSpark\n is an open-source cluster-computing framework\n\n\n\n\nDocumentation\n\n\n\n\nSpark\n\n\nHow-to: Tune Your Apache Spark Jobs\n series\n\n\nUnderstanding Resource Allocation configurations for a Spark application\n\n\n\n\n# start REPL\nspark-shell\npyspark\n\n\n\n\nInteractive Analysis example\n\n\nspark-shell\n\nval licenceLines = sc.textFile(\nfile:/usr/local/spark/LICENSE\n)\nval lineCount = licenceLines.count\nval isBsd = (line: String) =\n line.contains(\nBSD\n)\nval bsdLines = licenceLines.filter(isBsd)\nbsdLines.count\nbsdLines.foreach(println)\n\n\n\n\nSpark Job example\n\n\n# GitHub event documentation\n# https://developer.github.com/v3/activity/events/types\n\n# build jar on the host machine\ncd devops/hadoop/example/spark\nsbt clean package\n\ncd devops/hadoop\nvagrant ssh master\n\n# sample dataset\nmkdir -p github-archive \n \\\n  cd $_ \n \\\n  wget http://data.githubarchive.org/2018-01-01-{0..10}.json.gz \n \\\n  gunzip -k *\n# sample line\nhead -n 1 2018-01-01-0.json | jq '.'\n\n# run job\nspark-submit \\\n  --class \ncom.github.niqdev.App\n \\\n  --master local[*] \\\n  /vagrant/example/spark/target/scala-2.11/spark-github_2.11-0.1.0-SNAPSHOT.jar\n\n\n\n\n\n\nOozie\n\n\n\n\nOozie\n is a workflow scheduler system to manage Hadoop jobs\n\n\n\n\nDocumentation\n\n\n\n\nOozie\n\n\n\n\nSetup\n\n\n\n\nOozie is not installed by default\n\n\n\n\nOptional PostgreSQL configuration\n - By default Oozie is configured to use Embedded Derby\n\n\n# access master node\nvagrant ssh master\n\n# install docker\ncurl -fsSL get.docker.com -o get-docker.sh \n \\\n  chmod u+x $_ \n \\\n  ./$_ \n \\\n  sudo usermod -aG docker hadoop\n\n# logout and login again to verify docker installation\nexit\nvagrant ssh master\nwhoami # hadoop\ndocker ps -a\n\n# uncomment PostgreSQL configurations\nvim devops/hadoop/file/oozie/config/oozie-site.xml # from host\nvim /vagrant/file/oozie/config/oozie-site.xml # from guest\n\n# start postgres on guest machine \ndocker run \\\n  --detach \\\n  --name oozie-postgres \\\n  -p 5432:5432 \\\n  -e POSTGRES_DB=\noozie-db\n \\\n  -e POSTGRES_USER=\npostgres\n \\\n  -e POSTGRES_PASSWORD=\npassword\n \\\n  postgres\n\n# permission issue\n# https://github.com/docker-library/postgres/issues/116\n# --volume /vol/postgres:/var/lib/postgresql/data\n\n# access container\ndocker exec -it oozie-postgres bash\npsql --username=postgres\n# list all databases\n\\list\n\\connect oozie-db\n# list all tables\n\\dt\n# describe table\n\\d+ wf_jobs\n# list workflow\nselect * from wf_jobs;\n\n\n\n\nInstall and start Oozie\n\n\n# access master node\nvagrant ssh master\n\n# login as root\nsudo su -\n\n# build, install and init\n/vagrant/script/setup_oozie.sh\n\n# start oozie manually (first time only)\nsu --login hadoop /vagrant/script/bootstrap.sh oozie\n\n\n\n\nIt might take a while to build the sources\n\n\nUseful paths\n\n\n# data and logs\ndevops/hadoop/.data/master/oozie # host\n/vol/oozie # guest\n\n# (guest) config\n/usr/local/oozie/conf\n\n# (hdfs) examples\n/user/hadoop/examples\n\n\n\n\nExamples\n\n\nRun bundled examples within distribution\n\n\n# examples path\n.data/master/oozie/examples # host\n/vol/oozie/examples # guest\n\n# access master node as hadoop user\nvagrant ssh master\n\nexport OOZIE_EXAMPLE_PATH=/vol/oozie/examples\nexport OOZIE_HDFS_PATH=/user/$(whoami)/examples\n\n# open map-reduce job.properties\nvim $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties\n\n# edit the following properties\nnameNode=hdfs://namenode.local:9000 # fs.defaultFS @ core-site.xml\njobTracker=resource-manager.local:8032 # yarn.resourcemanager.address @ yarn-site.xml\nqueueName=priority_queue # or default @ fair-scheduler.xml\n\n# upload all the examples\nhadoop fs -put $OOZIE_EXAMPLE_PATH $OOZIE_HDFS_PATH\n\n# verify uploaded files\nhadoop fs -ls -h -R /user/$(whoami)\n\n# run the map-reduce workflow example\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -config $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties \\\n  -run\n\n# verify status\noozie job -oozie http://oozie.local:11000/oozie -info WORKFLOW_ID\n\n# verify result\nhadoop fs -cat $OOZIE_HDFS_PATH/output-data/map-reduce/part-00000\n\n# remove all the examples\nhadoop fs -rm -R $OOZIE_HDFS_PATH\n\n\n\n\nUseful commands\n\n\n\n\nWorkflow requires \noozie.wf.application.path\n property\n\n\nCoordinator requires \noozie.coord.application.path\n property\n\n\n\n\n# verify oozie status\noozie admin \\\n  -oozie http://oozie.local:11000/oozie \\\n  -status\n\n# verify workflow or coordinator status\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -info JOB_ID \\\n  -verbose\n\n# poll workflow or coordinator status\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -poll JOB_ID \\\n  -interval 10 \\\n  -timeout 60 \\\n  -verbose", 
            "title": "Hadoop"
        }, 
        {
            "location": "/hadoop/#hadoop", 
            "text": "The following guide explains how to provision a Multi Node Hadoop Cluster locally and play with it. Checkout the  Vagrantfile  and the Vagrant  guide  for more details.  Documentation   Hadoop  The Hadoop Ecosystem Table", 
            "title": "Hadoop"
        }, 
        {
            "location": "/hadoop/#setup", 
            "text": "Requirements   Vagrant  VirtualBox   Directory structure  tree -a hadoop/\nhadoop/\n\u251c\u2500\u2500 .data # mounted volume\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop_rsa\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop_rsa.pub\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 master\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 mapred\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 yarn\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 namenode\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 secondary\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 oozie\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0  \u00a0\u00a0 \u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0  \u00a0\u00a0 \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 datanode\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 mapred\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u2514\u2500\u2500 yarn\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-3\n\u251c\u2500\u2500 example\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 map-reduce\n\u251c\u2500\u2500 file\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 core-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fair-scheduler.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hdfs-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 mapred-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 masters\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 slaves\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 yarn-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-hadoop.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hosts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 motd\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie-env.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 oozie-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-oozie.sh\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ssh\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 config\n\u251c\u2500\u2500 script\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bootstrap.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_hadoop.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_oozie.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_spark.sh\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 setup_ubuntu.sh\n\u251c\u2500\u2500 Vagrantfile\n\u2514\u2500\u2500 vagrant_hadoop.sh  Import the script  source vagrant_hadoop.sh  Create and start a Multi Node Hadoop Cluster  hadoop-start  The first time it might take a while  Access the cluster via ssh, check also the  /etc/hosts  file  vagrant ssh master\nssh hadoop@172.16.0.10 -i .data/hadoop_rsa\n\n# 3 nodes\nvagrant ssh node-1\nssh hadoop@172.16.0.101 -i .data/hadoop_rsa  Destroy the cluster  hadoop-destroy  For convenience add to the host machine  cat hadoop/file/hosts | sudo tee --append /etc/hosts  Web UI links   NameNode:  http://namenode.local:50070  NameNode metrics:  http://namenode.local:50070/jmx  ResourceManager:  http://resource-manager.local:8088  Log Level:  http://resource-manager.local:8088/logLevel  Web Application Proxy Server:  http://web-proxy.local:8100/proxy/application_XXX_0000  MapReduce Job History Server:  http://history.local:19888  DataNode/NodeManager (1):  http://node-1.local:8042/node  DataNode/NodeManager (2):  http://node-2.local:8042/node  DataNode/NodeManager (3):  http://node-3.local:8042/node  Spark:  http://spark.local:4040  Oozie*:  http://oozie.local:11000", 
            "title": "Setup"
        }, 
        {
            "location": "/hadoop/#hdfs-and-mapreduce", 
            "text": "HDFS  is a distributed file system that provides high-throughput access to application data  YARN  is a framework for job scheduling and cluster resource management  MapReduce  is a YARN-based system for parallel processing of large data sets   Documentation   Hadoop v2.7.5  Untangling Apache Hadoop YARN  series", 
            "title": "HDFS and MapReduce"
        }, 
        {
            "location": "/hadoop/#hdfs-admin", 
            "text": "# filesystem statistics\nhdfs dfsadmin -report\n\n# filesystem check\nhdfs fsck /  Useful paths  # data and logs\ndevops/hadoop/.data/master/hadoop # host\n/vol/hadoop # guest\n\n# (guest) config\n/usr/local/hadoop/etc/hadoop\n\n# (hdfs) map-reduce history\n/mr-history/history/done_intermediate/hadoop\n\n# (hdfs) aggregated app logs\n/yarn/app/hadoop/logs/application_XXX", 
            "title": "HDFS Admin"
        }, 
        {
            "location": "/hadoop/#mapreduce-wordcount-job", 
            "text": "# build jar on the host machine\ncd devops/hadoop/example/map-reduce\n./gradlew clean build\n\ncd devops/hadoop\nvagrant ssh master\n\n# create base directory using hdfs\nhdfs dfs -mkdir -p /user/ubuntu\n\n# create example directory\nhadoop fs -mkdir -p /user/ubuntu/word-count/input\n\n# list directory\nhadoop fs -ls -h -R /\nhadoop fs -ls -h -R /user/ubuntu\n\n# create sample files\necho  Hello World Bye World    file01\necho  Hello Hadoop Goodbye Hadoop    file02\n\n# copy from local to hdfs\nhadoop fs -copyFromLocal file01 /user/ubuntu/word-count/input\nhadoop fs -put file02 /user/ubuntu/word-count/input\n\n# verify copied files\nhadoop fs -ls -h -R /user/ubuntu\nhadoop fs -cat /user/ubuntu/word-count/input/file01\nhadoop fs -cat /user/ubuntu/word-count/input/file02\nhadoop fs -cat /user/ubuntu/word-count/input/*\n\n# run application\nhadoop jar /vagrant/example/map-reduce/build/libs/map-reduce.jar \\\n  /user/ubuntu/word-count/input \\\n  /user/ubuntu/word-count/output\n\n# check output\nhadoop fs -cat /user/ubuntu/word-count/output/part-r-00000\n\n# delete directory to run it again\nhadoop fs -rm -R /user/ubuntu/word-count/output\n\n# run sample job in a different queue\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  wordcount \\\n  -Dmapreduce.job.queuename=root.priority_queue \\\n  /user/ubuntu/word-count/input \\\n  /user/ubuntu/word-count/output\n\n# well known WARN issue\n# https://issues.apache.org/jira/browse/HDFS-10429", 
            "title": "MapReduce WordCount Job"
        }, 
        {
            "location": "/hadoop/#benchmarking-mapreduce-with-terasort", 
            "text": "# generate random data\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  teragen 1000 random-data\n\n# run terasort benchmark\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  terasort random-data sorted-data\n\n# validate data\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  teravalidate sorted-data report\n\n# useful commands\nhadoop fs -ls -h -R .\nhadoop fs -rm -r random-data\nhadoop fs -cat random-data/part-m-00000\nhadoop fs -cat sorted-data/part-r-00000", 
            "title": "Benchmarking MapReduce with TeraSort"
        }, 
        {
            "location": "/hadoop/#spark", 
            "text": "Spark  is an open-source cluster-computing framework   Documentation   Spark  How-to: Tune Your Apache Spark Jobs  series  Understanding Resource Allocation configurations for a Spark application   # start REPL\nspark-shell\npyspark", 
            "title": "Spark"
        }, 
        {
            "location": "/hadoop/#interactive-analysis-example", 
            "text": "spark-shell\n\nval licenceLines = sc.textFile( file:/usr/local/spark/LICENSE )\nval lineCount = licenceLines.count\nval isBsd = (line: String) =  line.contains( BSD )\nval bsdLines = licenceLines.filter(isBsd)\nbsdLines.count\nbsdLines.foreach(println)", 
            "title": "Interactive Analysis example"
        }, 
        {
            "location": "/hadoop/#spark-job-example", 
            "text": "# GitHub event documentation\n# https://developer.github.com/v3/activity/events/types\n\n# build jar on the host machine\ncd devops/hadoop/example/spark\nsbt clean package\n\ncd devops/hadoop\nvagrant ssh master\n\n# sample dataset\nmkdir -p github-archive   \\\n  cd $_   \\\n  wget http://data.githubarchive.org/2018-01-01-{0..10}.json.gz   \\\n  gunzip -k *\n# sample line\nhead -n 1 2018-01-01-0.json | jq '.'\n\n# run job\nspark-submit \\\n  --class  com.github.niqdev.App  \\\n  --master local[*] \\\n  /vagrant/example/spark/target/scala-2.11/spark-github_2.11-0.1.0-SNAPSHOT.jar", 
            "title": "Spark Job example"
        }, 
        {
            "location": "/hadoop/#oozie", 
            "text": "Oozie  is a workflow scheduler system to manage Hadoop jobs   Documentation   Oozie", 
            "title": "Oozie"
        }, 
        {
            "location": "/hadoop/#setup_1", 
            "text": "Oozie is not installed by default   Optional PostgreSQL configuration  - By default Oozie is configured to use Embedded Derby  # access master node\nvagrant ssh master\n\n# install docker\ncurl -fsSL get.docker.com -o get-docker.sh   \\\n  chmod u+x $_   \\\n  ./$_   \\\n  sudo usermod -aG docker hadoop\n\n# logout and login again to verify docker installation\nexit\nvagrant ssh master\nwhoami # hadoop\ndocker ps -a\n\n# uncomment PostgreSQL configurations\nvim devops/hadoop/file/oozie/config/oozie-site.xml # from host\nvim /vagrant/file/oozie/config/oozie-site.xml # from guest\n\n# start postgres on guest machine \ndocker run \\\n  --detach \\\n  --name oozie-postgres \\\n  -p 5432:5432 \\\n  -e POSTGRES_DB= oozie-db  \\\n  -e POSTGRES_USER= postgres  \\\n  -e POSTGRES_PASSWORD= password  \\\n  postgres\n\n# permission issue\n# https://github.com/docker-library/postgres/issues/116\n# --volume /vol/postgres:/var/lib/postgresql/data\n\n# access container\ndocker exec -it oozie-postgres bash\npsql --username=postgres\n# list all databases\n\\list\n\\connect oozie-db\n# list all tables\n\\dt\n# describe table\n\\d+ wf_jobs\n# list workflow\nselect * from wf_jobs;  Install and start Oozie  # access master node\nvagrant ssh master\n\n# login as root\nsudo su -\n\n# build, install and init\n/vagrant/script/setup_oozie.sh\n\n# start oozie manually (first time only)\nsu --login hadoop /vagrant/script/bootstrap.sh oozie  It might take a while to build the sources  Useful paths  # data and logs\ndevops/hadoop/.data/master/oozie # host\n/vol/oozie # guest\n\n# (guest) config\n/usr/local/oozie/conf\n\n# (hdfs) examples\n/user/hadoop/examples", 
            "title": "Setup"
        }, 
        {
            "location": "/hadoop/#examples", 
            "text": "Run bundled examples within distribution  # examples path\n.data/master/oozie/examples # host\n/vol/oozie/examples # guest\n\n# access master node as hadoop user\nvagrant ssh master\n\nexport OOZIE_EXAMPLE_PATH=/vol/oozie/examples\nexport OOZIE_HDFS_PATH=/user/$(whoami)/examples\n\n# open map-reduce job.properties\nvim $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties\n\n# edit the following properties\nnameNode=hdfs://namenode.local:9000 # fs.defaultFS @ core-site.xml\njobTracker=resource-manager.local:8032 # yarn.resourcemanager.address @ yarn-site.xml\nqueueName=priority_queue # or default @ fair-scheduler.xml\n\n# upload all the examples\nhadoop fs -put $OOZIE_EXAMPLE_PATH $OOZIE_HDFS_PATH\n\n# verify uploaded files\nhadoop fs -ls -h -R /user/$(whoami)\n\n# run the map-reduce workflow example\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -config $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties \\\n  -run\n\n# verify status\noozie job -oozie http://oozie.local:11000/oozie -info WORKFLOW_ID\n\n# verify result\nhadoop fs -cat $OOZIE_HDFS_PATH/output-data/map-reduce/part-00000\n\n# remove all the examples\nhadoop fs -rm -R $OOZIE_HDFS_PATH", 
            "title": "Examples"
        }, 
        {
            "location": "/hadoop/#useful-commands", 
            "text": "Workflow requires  oozie.wf.application.path  property  Coordinator requires  oozie.coord.application.path  property   # verify oozie status\noozie admin \\\n  -oozie http://oozie.local:11000/oozie \\\n  -status\n\n# verify workflow or coordinator status\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -info JOB_ID \\\n  -verbose\n\n# poll workflow or coordinator status\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -poll JOB_ID \\\n  -interval 10 \\\n  -timeout 60 \\\n  -verbose", 
            "title": "Useful commands"
        }, 
        {
            "location": "/papers/", 
            "text": "Papers\n\n\n\n\n\n\nThe Google File System\n\n\n\n\n\n\nMapReduce\n\n\n\n\n\n\nRaft\n\n\n\n\n\n\nPaxos\n\n\n\n\n\n\nZab\n\n\n\n\n\n\nChubby\n\n\n\n\n\n\nSpanner\n\n\n\n\n\n\nDynamo\n\n\n\n\n\n\nHyperLogLog\n\n\n\n\n\n\nDapper\n\n\n\n\n\n\nHarvest, Yield, and Scalable Tolerant Systems\n\n\n\n\n\n\nCassandra\n\n\n\n\n\n\nA Big Data Modeling Methodology for Apache Cassandra\n\n\n\n\n\n\nLife beyond Distributed Transactions\n\n\n\n\n\n\nConflict-free Replicated Data Types\n\n\n\n\n\n\nMerkle Hash Tree", 
            "title": "Papers"
        }, 
        {
            "location": "/papers/#papers", 
            "text": "The Google File System    MapReduce    Raft    Paxos    Zab    Chubby    Spanner    Dynamo    HyperLogLog    Dapper    Harvest, Yield, and Scalable Tolerant Systems    Cassandra    A Big Data Modeling Methodology for Apache Cassandra    Life beyond Distributed Transactions    Conflict-free Replicated Data Types    Merkle Hash Tree", 
            "title": "Papers"
        }, 
        {
            "location": "/other/", 
            "text": "Vagrant\n\n\n\n\nVagrant\n is a tool for building and managing virtual machine environments in a single workflow\n\n\n\n\nDocumentation\n\n\n\n\nVagrant\n\n\nVirtualBox\n\n\n\n\nSetup project creating a Vagrantfile\n\n\nvagrant init\n\n\n\n\nBoot and connect to the default virtual machine\n\n\nvagrant up\nvagrant status\nvagrant ssh\n\n\n\n\nUseful commands\n\n\n# shut down gracefully\nvagrant halt\n\n# reload (halt + up) + re-provision\nvagrant reload --provision\n\n# update box\nvagrant box update\n\n# delete virtual machine without prompt\nvagrant destory -f\n\n\n\n\n\n\nMkDocs\n\n\n\n\nMkDocs\n is a static site generator\n\n\n\n\nDocumentation\n\n\n\n\nMkDocs\n\n\n\n\nInstall\n\n\npip install mkdocs\n\n\n\n\nUseful commands\n\n\n# setup in current directory\nmkdocs new .\n\n# start dev server with hot reload @ http://127.0.0.1:8000\nmkdocs serve\n\n# build static site\nmkdocs build --clean\n\n# deploy to github\nmkdocs gh-deploy\n\n\n\n\n\n\nSDKMAN!\n\n\n\n\nSDKMAN!\n is a tool for managing parallel versions of multiple Software Development Kits on most Unix based systems\n\n\n\n\nDocumentation\n\n\n\n\nSDKMAN!\n\n\n\n\nSetup\n\n\ncurl -s \nhttps://get.sdkman.io\n | bash\nsource \n$HOME/.sdkman/bin/sdkman-init.sh\n\nsdk version\n\n\n\n\nGradle\n\n\n# setup\nsdk list gradle\nsdk install gradle 4.4.1\ngradle -version\n\n# create Gradle project\nmkdir -p PROJECT_NAME \n cd $_\ngradle init --type java-library\n\n./gradlew clean build\n\n\n\n\nScala\n\n\n# setup sbt\nsdk list sbt\nsdk install sbt\nsbt sbtVersion\nsbt about\n\n# setup scala\nsdk list scala\nsdk install scala 2.11.8\nscala -version\n\n# sample project\nsbt new sbt/scala-seed.g8\n\n\n\n\n\n\nGiter8\n\n\n\n\nGiter8\n is a command line tool to generate files and directories from templates published on GitHub or any other git repository\n\n\n\n\nDocumentation\n\n\n\n\nGiter8\n\n\nTemplates\n\n\n\n\nSetup\n\n\n# install conscript\ncurl https://raw.githubusercontent.com/foundweekends/conscript/master/setup.sh | sh\nsource ~/.bashrc\n\n# install g8\ncs foundweekends/giter8\n\n\n\n\nExample\n\n\n# interactive\ng8 sbt/scala-seed.g8\n# non-interactive\ng8 sbt/scala-seed.g8 --name=my-new-website\n\n\n\n\n\n\nPython\n\n\nDocumentation\n\n\n\n\npip\n\n\nvirtualenv\n\n\nWhat is the difference between virtualenv | pyenv | virtualenvwrapper | venv ?\n\n\n\n\nSetup\n\n\n# search\napt-get update \n apt-cache search python | grep python2\n\n# setup python\napt-get install -y python2.7\napt-get install -y python3\n\n# install pip + setuptools\ncurl https://bootstrap.pypa.io/get-pip.py | python2.7 -\ncurl https://bootstrap.pypa.io/get-pip.py | python3 -\n\n# upgrade pip\npip install -U pip\n\n# install virtualenv globally \npip install virtualenv\n\n\n\n\nvirtualenv\n\n\n# create virtualenv\nvirtualenv venv\nvirtualenv -p python3 venv\nvirtualenv -p $(which python3) venv\n\n# activate virtualenv\nsource venv/bin/activate\n\n# verify virtualenv\nwhich python\npython --version\n\n# deactivate virtualenv\ndeactivate\n\n\n\n\npip\n\n\n# search package\npip search \npackage\n\n\n# install new package\npip install \npackage\n\n\n# update requirements with new packages\npip freeze \n requirements.txt\n\n# install all requirements\npip install -r requirements.txt\n\n\n\n\nOther\n\n\n# generate rc file\npylint --generate-rcfile \n .pylintrc\n\n# create module\ntouch app/{__init__,main}.py\n\n\n\n\n\n\nFurther reading\n\n\n\n\nHow Linux Works\n (2014)(2nd) by Brian Ward\n\n\nDocker in Action\n (2016) by Jeff Nickoloff\n\n\nDesigning Data-Intensive Applications\n (2017) by Martin Kleppmann\n\n\nHadoop: The Definitive Guide\n (2015)(4th) by Tom White\n\n\nSpark in Action\n (2016) by Petar Ze\u010devi\u0107 and Marko Bona\u0107i\n\n\nCassandra: The Definitive Guide\n (2016)(4th) by Eben Hewitt, Jeff Carpenter\n\n\nKafka: The Definitive Guide\n (2017) by Gwen Shapira, Neha Narkhede, Todd Palino\n\n\n\n\nScala\n\n\n\n\nProgramming in Scala\n (2016)(3rd) by Martin Odersky, Lex Spoon, and Bill Venners\n\n\nFunctional Programming in Scala\n (2014) by Paul Chiusano and Runar Bjarnason\n\n\nAkka in Action\n (2016) by Raymond Roestenburg, Rob Bakker, and Rob Williams", 
            "title": "Other"
        }, 
        {
            "location": "/other/#vagrant", 
            "text": "Vagrant  is a tool for building and managing virtual machine environments in a single workflow   Documentation   Vagrant  VirtualBox   Setup project creating a Vagrantfile  vagrant init  Boot and connect to the default virtual machine  vagrant up\nvagrant status\nvagrant ssh  Useful commands  # shut down gracefully\nvagrant halt\n\n# reload (halt + up) + re-provision\nvagrant reload --provision\n\n# update box\nvagrant box update\n\n# delete virtual machine without prompt\nvagrant destory -f", 
            "title": "Vagrant"
        }, 
        {
            "location": "/other/#mkdocs", 
            "text": "MkDocs  is a static site generator   Documentation   MkDocs   Install  pip install mkdocs  Useful commands  # setup in current directory\nmkdocs new .\n\n# start dev server with hot reload @ http://127.0.0.1:8000\nmkdocs serve\n\n# build static site\nmkdocs build --clean\n\n# deploy to github\nmkdocs gh-deploy", 
            "title": "MkDocs"
        }, 
        {
            "location": "/other/#sdkman", 
            "text": "SDKMAN!  is a tool for managing parallel versions of multiple Software Development Kits on most Unix based systems   Documentation   SDKMAN!   Setup  curl -s  https://get.sdkman.io  | bash\nsource  $HOME/.sdkman/bin/sdkman-init.sh \nsdk version  Gradle  # setup\nsdk list gradle\nsdk install gradle 4.4.1\ngradle -version\n\n# create Gradle project\nmkdir -p PROJECT_NAME   cd $_\ngradle init --type java-library\n\n./gradlew clean build  Scala  # setup sbt\nsdk list sbt\nsdk install sbt\nsbt sbtVersion\nsbt about\n\n# setup scala\nsdk list scala\nsdk install scala 2.11.8\nscala -version\n\n# sample project\nsbt new sbt/scala-seed.g8", 
            "title": "SDKMAN!"
        }, 
        {
            "location": "/other/#giter8", 
            "text": "Giter8  is a command line tool to generate files and directories from templates published on GitHub or any other git repository   Documentation   Giter8  Templates   Setup  # install conscript\ncurl https://raw.githubusercontent.com/foundweekends/conscript/master/setup.sh | sh\nsource ~/.bashrc\n\n# install g8\ncs foundweekends/giter8  Example  # interactive\ng8 sbt/scala-seed.g8\n# non-interactive\ng8 sbt/scala-seed.g8 --name=my-new-website", 
            "title": "Giter8"
        }, 
        {
            "location": "/other/#python", 
            "text": "Documentation   pip  virtualenv  What is the difference between virtualenv | pyenv | virtualenvwrapper | venv ?   Setup  # search\napt-get update   apt-cache search python | grep python2\n\n# setup python\napt-get install -y python2.7\napt-get install -y python3\n\n# install pip + setuptools\ncurl https://bootstrap.pypa.io/get-pip.py | python2.7 -\ncurl https://bootstrap.pypa.io/get-pip.py | python3 -\n\n# upgrade pip\npip install -U pip\n\n# install virtualenv globally \npip install virtualenv  virtualenv  # create virtualenv\nvirtualenv venv\nvirtualenv -p python3 venv\nvirtualenv -p $(which python3) venv\n\n# activate virtualenv\nsource venv/bin/activate\n\n# verify virtualenv\nwhich python\npython --version\n\n# deactivate virtualenv\ndeactivate  pip  # search package\npip search  package \n\n# install new package\npip install  package \n\n# update requirements with new packages\npip freeze   requirements.txt\n\n# install all requirements\npip install -r requirements.txt  Other  # generate rc file\npylint --generate-rcfile   .pylintrc\n\n# create module\ntouch app/{__init__,main}.py", 
            "title": "Python"
        }, 
        {
            "location": "/other/#further-reading", 
            "text": "How Linux Works  (2014)(2nd) by Brian Ward  Docker in Action  (2016) by Jeff Nickoloff  Designing Data-Intensive Applications  (2017) by Martin Kleppmann  Hadoop: The Definitive Guide  (2015)(4th) by Tom White  Spark in Action  (2016) by Petar Ze\u010devi\u0107 and Marko Bona\u0107i  Cassandra: The Definitive Guide  (2016)(4th) by Eben Hewitt, Jeff Carpenter  Kafka: The Definitive Guide  (2017) by Gwen Shapira, Neha Narkhede, Todd Palino   Scala   Programming in Scala  (2016)(3rd) by Martin Odersky, Lex Spoon, and Bill Venners  Functional Programming in Scala  (2014) by Paul Chiusano and Runar Bjarnason  Akka in Action  (2016) by Raymond Roestenburg, Rob Bakker, and Rob Williams", 
            "title": "Further reading"
        }
    ]
}