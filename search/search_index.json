{
    "docs": [
        {
            "location": "/", 
            "text": "DevOps Lab\n\n\nA collection of Docker and Vagrant images mainly to provision distributed systems for local development, learning purposes and quick prototyping.\n\n\nDocumentations\n\n\n\n\nAnsible\n\n\nHadoop\n\n\nCassandra\n\n\nZookeeper\n\n\nKafka\n\n\nOther", 
            "title": "Home"
        }, 
        {
            "location": "/#devops-lab", 
            "text": "A collection of Docker and Vagrant images mainly to provision distributed systems for local development, learning purposes and quick prototyping.  Documentations   Ansible  Hadoop  Cassandra  Zookeeper  Kafka  Other", 
            "title": "DevOps Lab"
        }, 
        {
            "location": "/ansible/", 
            "text": "Ansible\n\n\nAnsible is an open source automation platform that can help with config management, deployment and task automation.\n\n\nDocumentation\n\n\n\n\nAnsible\n\n\nTutorial\n\n\nPlaybook example\n\n\n\n\nRequirement\n\n\n\n\nVagrant\n\n\nVirtualBox\n\n\n\n\nThe following guide explains how to provision Ansible locally and play with it. Checkout the \nVagrantfile\n and the Vagrant \nguide\n for more details.\n\n\nDirectory structure\n\n\nAll the commands are executed in this directory \ncd ansible\n\n\nansible/\n\u251c\u2500\u2500 .share\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-3\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ssh\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ansible_rsa\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 ansible_rsa.pub\n\u251c\u2500\u2500 Vagrantfile\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 group_vars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 host_vars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hosts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 roles\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 common\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 defaults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 files\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 handlers\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 motd.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 package.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 templates\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 motd\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 vars\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 docker\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 site.yml\n\u251c\u2500\u2500 setup_ansible.sh\n\u2514\u2500\u2500 setup_share.sh\n\n\n\n\nSetup\n\n\nThe first time \nonly\n, you have to setup the shared folders and generate the ssh key needed by ansible to access all nodes executing\n\n\n./setup_share.sh\n\n\n\n\nFrom now on start the boxes with\n\n\nvagrant up\n\n\n\n\nNote that the first time it could take a while\n\n\nVerify status of the boxes with\n\n\nvagrant status\n\n\n\n\nVerify access to the boxes with\n\n\nvagrant ssh ansible\nvagrant ssh node-1\n\n\n\n\nFrom inside the boxes you should be able to communicate with the others\n\n\nping ansible.local\nping ip-192-168-100-11.local\nping 192.168.100.12\n\n\n\n\nThe following paths are shared with the boxes\n\n\n\n\n/vagrant\n provision-tool\n\n\n/local\n host $HOME\n\n\n/ansible\n data \n(ansible only)\n\n\n/data\n .share \n(node only)\n\n\n\n\nAd-Hoc Commands\n\n\nAccess the ansible box with\n\n\nvagrant ssh ansible\n\n\n\n\nBelow a list of examples\n\n\n\n# ping all nodes (default inventory /etc/ansible/hosts)\nansible all -m ping\nansible ansible -m ping\nansible cluster -m ping\n\n# ping all nodes (specify inventory)\nansible all -i \n/vagrant/data/hosts\n -m ping\n\n# gathering facts\nansible all -m setup\nansible ansible -m setup\n\n# specify host and user\nansible ip-192-168-100-11.local -m ping -u vagrant\n\n# execute command\nansible all -a \n/bin/echo hello\n\nansible all -a \nuptime\n\nansible all -a \n/bin/date\n\n# do NOT reboot vagrant through ansible (use vagrant reload)\nansible cluster -a \n/sbin/reboot\n --become\n\n# shell module\nansible all -m shell -a \npwd\n\n# be carefull to quotes\nansible all -m shell -a 'echo $HOME'\n\n# update \n upgrade\nansible all -m apt -a \nupdate_cache=yes upgrade=dist\n --become\n# restart after upgrade\nvagrant reload\n# install package\nansible all -m apt -a \nname=tree state=present\n --become\n\n\n\n\nPlaybooks\n\n\nAccess the ansible box with\n\n\nvagrant ssh ansible\n\n\n\n\nBelow a list of examples\n\n\n# test uptime on all node\nansible-playbook /ansible/site.yml --tags=test --verbose\n\n# update \n upgrade only cluster nodes\nansible-playbook /ansible/site.yml -t package --skip-tags=java --verbose\n\n# install packages on cluster nodes\nansible-playbook /ansible/site.yml -t package --verbose\n\n# run common task on cluster node\nansible-playbook /ansible/site.yml -t common\n\n# setup docker\nansible-playbook /ansible/site.yml -t docker\n# test docker\nvagrant ssh node-1\nsudo -i -u docker\ndocker ps -a\n\n# custom banner\nansible-playbook /ansible/site.yml -t motd\n\n# setup all infrastructure at once\nansible-playbook /ansible/site.yml\n\n# dry run\nansible-playbook -i /ansible/hosts /ansible/site.yml --check --diff", 
            "title": "Ansible"
        }, 
        {
            "location": "/ansible/#ansible", 
            "text": "Ansible is an open source automation platform that can help with config management, deployment and task automation.  Documentation   Ansible  Tutorial  Playbook example   Requirement   Vagrant  VirtualBox   The following guide explains how to provision Ansible locally and play with it. Checkout the  Vagrantfile  and the Vagrant  guide  for more details.", 
            "title": "Ansible"
        }, 
        {
            "location": "/ansible/#directory-structure", 
            "text": "All the commands are executed in this directory  cd ansible  ansible/\n\u251c\u2500\u2500 .share\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-3\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ssh\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ansible_rsa\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 ansible_rsa.pub\n\u251c\u2500\u2500 Vagrantfile\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 group_vars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 host_vars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hosts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 roles\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 common\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 defaults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 files\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 handlers\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 motd.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 package.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 templates\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 motd\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 vars\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 docker\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 site.yml\n\u251c\u2500\u2500 setup_ansible.sh\n\u2514\u2500\u2500 setup_share.sh", 
            "title": "Directory structure"
        }, 
        {
            "location": "/ansible/#setup", 
            "text": "The first time  only , you have to setup the shared folders and generate the ssh key needed by ansible to access all nodes executing  ./setup_share.sh  From now on start the boxes with  vagrant up  Note that the first time it could take a while  Verify status of the boxes with  vagrant status  Verify access to the boxes with  vagrant ssh ansible\nvagrant ssh node-1  From inside the boxes you should be able to communicate with the others  ping ansible.local\nping ip-192-168-100-11.local\nping 192.168.100.12  The following paths are shared with the boxes   /vagrant  provision-tool  /local  host $HOME  /ansible  data  (ansible only)  /data  .share  (node only)", 
            "title": "Setup"
        }, 
        {
            "location": "/ansible/#ad-hoc-commands", 
            "text": "Access the ansible box with  vagrant ssh ansible  Below a list of examples  \n# ping all nodes (default inventory /etc/ansible/hosts)\nansible all -m ping\nansible ansible -m ping\nansible cluster -m ping\n\n# ping all nodes (specify inventory)\nansible all -i  /vagrant/data/hosts  -m ping\n\n# gathering facts\nansible all -m setup\nansible ansible -m setup\n\n# specify host and user\nansible ip-192-168-100-11.local -m ping -u vagrant\n\n# execute command\nansible all -a  /bin/echo hello \nansible all -a  uptime \nansible all -a  /bin/date \n# do NOT reboot vagrant through ansible (use vagrant reload)\nansible cluster -a  /sbin/reboot  --become\n\n# shell module\nansible all -m shell -a  pwd \n# be carefull to quotes\nansible all -m shell -a 'echo $HOME'\n\n# update   upgrade\nansible all -m apt -a  update_cache=yes upgrade=dist  --become\n# restart after upgrade\nvagrant reload\n# install package\nansible all -m apt -a  name=tree state=present  --become", 
            "title": "Ad-Hoc Commands"
        }, 
        {
            "location": "/ansible/#playbooks", 
            "text": "Access the ansible box with  vagrant ssh ansible  Below a list of examples  # test uptime on all node\nansible-playbook /ansible/site.yml --tags=test --verbose\n\n# update   upgrade only cluster nodes\nansible-playbook /ansible/site.yml -t package --skip-tags=java --verbose\n\n# install packages on cluster nodes\nansible-playbook /ansible/site.yml -t package --verbose\n\n# run common task on cluster node\nansible-playbook /ansible/site.yml -t common\n\n# setup docker\nansible-playbook /ansible/site.yml -t docker\n# test docker\nvagrant ssh node-1\nsudo -i -u docker\ndocker ps -a\n\n# custom banner\nansible-playbook /ansible/site.yml -t motd\n\n# setup all infrastructure at once\nansible-playbook /ansible/site.yml\n\n# dry run\nansible-playbook -i /ansible/hosts /ansible/site.yml --check --diff", 
            "title": "Playbooks"
        }, 
        {
            "location": "/hadoop/", 
            "text": "Hadoop\n\n\nThe following guide explains how to provision a Multi Node Hadoop Cluster locally and play with it. Checkout the \nVagrantfile\n and the Vagrant \nguide\n for more details.\n\n\nSetup\n\n\nRequirements\n\n\n\n\nVagrant\n\n\nVirtualBox\n\n\n\n\nDirectory structure\n\n\ncd devops-lab/hadoop\n\ntree -a hadoop/\nhadoop/\n\u251c\u2500\u2500 .data # mounted volume\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop_rsa\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop_rsa.pub\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 master\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 mapred\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 yarn\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 namenode\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 secondary\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 oozie\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0  \u00a0\u00a0 \u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0  \u00a0\u00a0 \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 datanode\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 mapred\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u2514\u2500\u2500 yarn\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-3\n\u251c\u2500\u2500 example\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 map-reduce\n\u251c\u2500\u2500 file\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 core-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fair-scheduler.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hdfs-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 mapred-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 masters\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 slaves\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 yarn-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-hadoop.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hosts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 motd\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie-env.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 oozie-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-oozie.sh\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ssh\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 config\n\u251c\u2500\u2500 script\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bootstrap.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_hadoop.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_oozie.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_spark.sh\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 setup_ubuntu.sh\n\u251c\u2500\u2500 Vagrantfile\n\u2514\u2500\u2500 vagrant_hadoop.sh\n\n\n\n\nImport the script\n\n\nsource vagrant_hadoop.sh\n\n\n\n\nCreate and start a Multi Node Hadoop Cluster\n\n\nhadoop-start\n\n\n\n\nThe first time it might take a while\n\n\nAccess the cluster via ssh, check also the \n/etc/hosts\n file\n\n\nvagrant ssh master\nssh hadoop@172.16.0.10 -i .data/hadoop_rsa\n\n# 3 nodes\nvagrant ssh node-1\nssh hadoop@172.16.0.101 -i .data/hadoop_rsa\n\n\n\n\nDestroy the cluster\n\n\nhadoop-destroy\n\n\n\n\nFor convenience add to the host machine\n\n\ncat hadoop/file/hosts | sudo tee --append /etc/hosts\n\n\n\n\nWeb UI links\n\n\n\n\nNameNode: \nhttp://namenode.local:50070\n\n\nNameNode metrics: \nhttp://namenode.local:50070/jmx\n\n\nResourceManager: \nhttp://resource-manager.local:8088\n\n\nLog Level: \nhttp://resource-manager.local:8088/logLevel\n\n\nWeb Application Proxy Server: \nhttp://web-proxy.local:8100/proxy/application_XXX_0000\n\n\nMapReduce Job History Server: \nhttp://history.local:19888\n\n\nDataNode/NodeManager (1): \nhttp://node-1.local:8042/node\n\n\nDataNode/NodeManager (2): \nhttp://node-2.local:8042/node\n\n\nDataNode/NodeManager (3): \nhttp://node-3.local:8042/node\n\n\nOozie*: \nhttp://oozie.local:11000\n\n\n\n\nHDFS and MapReduce\n\n\n\n\nHDFS\n is a distributed file system that provides high-throughput access to application data\n\n\nYARN\n is a framework for job scheduling and cluster resource management\n\n\nMapReduce\n is a YARN-based system for parallel processing of large data sets\n\n\n\n\nDocumentation\n\n\n\n\nHadoop v2.7.5\n\n\nUntangling Apache Hadoop YARN\n series\n\n\n\n\nHDFS Admin\n\n\n# filesystem statistics\nhdfs dfsadmin -report\n\n# filesystem check\nhdfs fsck /\n\n\n\n\nUseful paths\n\n\n# data and logs\ndevops-lab/hadoop/.data/master/hadoop # host\n/vol/hadoop # guest\n\n# (guest) config\n/usr/local/hadoop/etc/hadoop\n\n# (hdfs) map-reduce history\n/mr-history/history/done_intermediate/hadoop\n\n# (hdfs) aggregated app logs\n/yarn/app/hadoop/logs/application_XXX\n\n\n\n\nMapReduce WordCount Job\n\n\n# create base directory using hdfs\nhdfs dfs -mkdir -p /user/ubuntu\n\n# create example directory\nhadoop fs -mkdir -p /user/ubuntu/word-count/input\n\n# list directory\nhadoop fs -ls -h -R /\nhadoop fs -ls -h -R /user/ubuntu\n\n# create sample files\necho \nHello World Bye World\n \n file01\necho \nHello Hadoop Goodbye Hadoop\n \n file02\n\n# copy from local to hdfs\nhadoop fs -copyFromLocal file01 /user/ubuntu/word-count/input\nhadoop fs -put file02 /user/ubuntu/word-count/input\n\n# verify copied files\nhadoop fs -ls -h -R /user/ubuntu\nhadoop fs -cat /user/ubuntu/word-count/input/file01\nhadoop fs -cat /user/ubuntu/word-count/input/file02\nhadoop fs -cat /user/ubuntu/word-count/input/*\n\n# build the jar (outside the machine to avoid permission issues)\ncd devops-lab/hadoop/example/map-reduce\n./gradlew clean build\n\n# run application\nhadoop jar /vagrant/example/map-reduce/build/libs/map-reduce.jar \\\n  /user/ubuntu/word-count/input \\\n  /user/ubuntu/word-count/output\n\n# check output\nhadoop fs -cat /user/ubuntu/word-count/output/part-r-00000\n\n# delete directory to run it again\nhadoop fs -rm -R /user/ubuntu/word-count/output\n\n# run sample job in a different queue\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  wordcount \\\n  -Dmapreduce.job.queuename=root.priority_queue \\\n  /user/ubuntu/word-count/input \\\n  /user/ubuntu/word-count/output\n\n# well known WARN issue\n# https://issues.apache.org/jira/browse/HDFS-10429\n\n\n\n\nBenchmarking MapReduce with TeraSort\n\n\n# generate random data\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  teragen 1000 random-data\n\n# run terasort benchmark\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  terasort random-data sorted-data\n\n# validate data\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  teravalidate sorted-data report\n\n# useful commands\nhadoop fs -ls -h -R .\nhadoop fs -rm -r random-data\nhadoop fs -cat random-data/part-m-00000\nhadoop fs -cat sorted-data/part-r-00000\n\n\n\n\nSpark\n\n\n\n\nSpark\n is an open-source cluster-computing framework\n\n\n\n\nTODO\n\n\nFlink\n\n\nTODO\n\n\nAvro\n\n\n\n\nAvro\n is a data serialization system\n\n\n\n\nTODO\n\n\nParquet\n\n\n\n\nParquet\n is a columnar storage format that can efficiently store nested data\n\n\n\n\nTODO\n\n\nFlume\n\n\nTODO\n\n\nSqoop\n\n\nTODO\n\n\nPig\n\n\nTODO\n\n\nHive\n\n\nTODO\n\n\nCrunch\n\n\nTODO\n\n\nHBase\n\n\nTODO\n\n\nOozie\n\n\n\n\nOozie\n is a workflow scheduler system to manage Hadoop jobs\n\n\n\n\nDocumentation\n\n\n\n\nOozie\n\n\n\n\nSetup\n\n\n\n\nOozie is not installed by default\n\n\n\n\nOptional PostgreSQL configuration\n - By default Oozie is configured to use Embedded Derby\n\n\n# access master node\nvagrant ssh master\n\n# install docker\ncurl -fsSL get.docker.com -o get-docker.sh \n \\\n  chmod u+x $_ \n \\\n  ./$_ \n \\\n  sudo usermod -aG docker hadoop\n\n# logout and login again to verify docker installation\nexit\nvagrant ssh master\nwhoami # whoami\ndocker ps -a\n\n# uncomment PostgreSQL configurations\nvim devops-lab/hadoop/file/oozie/config/oozie-site.xml # from host\nvim /vagrant/file/oozie/config/oozie-site.xml # from guest\n\n# start postgres on guest machine \ndocker run \\\n  --detach \\\n  --name oozie-postgres \\\n  -p 5432:5432 \\\n  -e POSTGRES_DB=\noozie-db\n \\\n  -e POSTGRES_USER=\npostgres\n \\\n  -e POSTGRES_PASSWORD=\npassword\n \\\n  postgres\n\n# permission issue\n# https://github.com/docker-library/postgres/issues/116\n# --volume /vol/postgres:/var/lib/postgresql/data\n\n# access container\ndocker exec -it oozie-postgres bash\npsql --username=postgres\n# list all databases\n\\list\n\\connect oozie-db\n# list all tables\n\\dt\n# describe table\n\\d+ wf_jobs\n# list workflow\nselect * from wf_jobs;\n\n\n\n\nInstall and start Oozie\n\n\n# access master node\nvagrant ssh master\n\n# login as root\nsudo su -\n\n# build, install and init\n/vagrant/script/setup_oozie.sh\n\n# start oozie manually (first time only)\nsu --login hadoop /vagrant/script/bootstrap.sh oozie\n\n\n\n\nIt might take a while to build the sources\n\n\nUseful paths\n\n\n# data and logs\ndevops-lab/hadoop/.data/master/oozie # host\n/vol/oozie # guest\n\n# (guest) config\n/usr/local/oozie/conf\n\n# (hdfs) examples\n/user/hadoop/examples\n\n\n\n\nExamples\n\n\nRun bundled examples within distribution\n\n\n# examples path\n.data/master/oozie/examples # host\n/vol/oozie/examples # guest\n\n# access master node as hadoop user\nvagrant ssh master\n\nexport OOZIE_EXAMPLE_PATH=/vol/oozie/examples\nexport OOZIE_HDFS_PATH=/user/$(whoami)/examples\n\n# open map-reduce job.properties\nvim $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties\n\n# edit the following properties\nnameNode=hdfs://namenode.local:9000 # fs.defaultFS @ core-site.xml\njobTracker=resource-manager.local:8032 # yarn.resourcemanager.address @ yarn-site.xml\nqueueName=priority_queue # or default @ fair-scheduler.xml\n\n# upload all the examples\nhadoop fs -put $OOZIE_EXAMPLE_PATH $OOZIE_HDFS_PATH\n\n# verify uploaded files\nhadoop fs -ls -h -R /user/$(whoami)\n\n# run the map-reduce workflow example\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -config $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties \\\n  -run\n\n# verify status\noozie job -oozie http://oozie.local:11000/oozie -info WORKFLOW_ID\n\n# verify result\nhadoop fs -cat $OOZIE_HDFS_PATH/output-data/map-reduce/part-00000\n\n# remove all the examples\nhadoop fs -rm -R $OOZIE_HDFS_PATH\n\n\n\n\nUseful commands\n\n\n\n\nWorkflow requires \noozie.wf.application.path\n property\n\n\nCoordinator requires \noozie.coord.application.path\n property\n\n\n\n\n# verify oozie status\noozie admin \\\n  -oozie http://oozie.local:11000/oozie \\\n  -status\n\n# verify workflow or coordinator status\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -info JOB_ID \\\n  -verbose\n\n# poll workflow or coordinator status\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -poll JOB_ID \\\n  -interval 10 \\\n  -timeout 60 \\\n  -verbose\n\n\n\n\nGanglia\n\n\n\n\nGanglia\n is a monitoring system for Hadoop\n\n\n\n\nTODO\n\n\nZeppelin\n\n\nTODO\n\n\nKnox\n\n\nTODO", 
            "title": "Hadoop"
        }, 
        {
            "location": "/hadoop/#hadoop", 
            "text": "The following guide explains how to provision a Multi Node Hadoop Cluster locally and play with it. Checkout the  Vagrantfile  and the Vagrant  guide  for more details.", 
            "title": "Hadoop"
        }, 
        {
            "location": "/hadoop/#setup", 
            "text": "Requirements   Vagrant  VirtualBox   Directory structure  cd devops-lab/hadoop\n\ntree -a hadoop/\nhadoop/\n\u251c\u2500\u2500 .data # mounted volume\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop_rsa\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop_rsa.pub\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 master\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 mapred\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 yarn\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 namenode\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 secondary\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 oozie\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0  \u00a0\u00a0 \u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0  \u00a0\u00a0 \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 datanode\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 mapred\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u2514\u2500\u2500 yarn\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-3\n\u251c\u2500\u2500 example\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 map-reduce\n\u251c\u2500\u2500 file\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 core-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fair-scheduler.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hdfs-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 mapred-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 masters\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 slaves\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 yarn-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-hadoop.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hosts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 motd\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie-env.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 oozie-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-oozie.sh\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ssh\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 config\n\u251c\u2500\u2500 script\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bootstrap.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_hadoop.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_oozie.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_spark.sh\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 setup_ubuntu.sh\n\u251c\u2500\u2500 Vagrantfile\n\u2514\u2500\u2500 vagrant_hadoop.sh  Import the script  source vagrant_hadoop.sh  Create and start a Multi Node Hadoop Cluster  hadoop-start  The first time it might take a while  Access the cluster via ssh, check also the  /etc/hosts  file  vagrant ssh master\nssh hadoop@172.16.0.10 -i .data/hadoop_rsa\n\n# 3 nodes\nvagrant ssh node-1\nssh hadoop@172.16.0.101 -i .data/hadoop_rsa  Destroy the cluster  hadoop-destroy  For convenience add to the host machine  cat hadoop/file/hosts | sudo tee --append /etc/hosts  Web UI links   NameNode:  http://namenode.local:50070  NameNode metrics:  http://namenode.local:50070/jmx  ResourceManager:  http://resource-manager.local:8088  Log Level:  http://resource-manager.local:8088/logLevel  Web Application Proxy Server:  http://web-proxy.local:8100/proxy/application_XXX_0000  MapReduce Job History Server:  http://history.local:19888  DataNode/NodeManager (1):  http://node-1.local:8042/node  DataNode/NodeManager (2):  http://node-2.local:8042/node  DataNode/NodeManager (3):  http://node-3.local:8042/node  Oozie*:  http://oozie.local:11000", 
            "title": "Setup"
        }, 
        {
            "location": "/hadoop/#hdfs-and-mapreduce", 
            "text": "HDFS  is a distributed file system that provides high-throughput access to application data  YARN  is a framework for job scheduling and cluster resource management  MapReduce  is a YARN-based system for parallel processing of large data sets   Documentation   Hadoop v2.7.5  Untangling Apache Hadoop YARN  series", 
            "title": "HDFS and MapReduce"
        }, 
        {
            "location": "/hadoop/#hdfs-admin", 
            "text": "# filesystem statistics\nhdfs dfsadmin -report\n\n# filesystem check\nhdfs fsck /  Useful paths  # data and logs\ndevops-lab/hadoop/.data/master/hadoop # host\n/vol/hadoop # guest\n\n# (guest) config\n/usr/local/hadoop/etc/hadoop\n\n# (hdfs) map-reduce history\n/mr-history/history/done_intermediate/hadoop\n\n# (hdfs) aggregated app logs\n/yarn/app/hadoop/logs/application_XXX", 
            "title": "HDFS Admin"
        }, 
        {
            "location": "/hadoop/#mapreduce-wordcount-job", 
            "text": "# create base directory using hdfs\nhdfs dfs -mkdir -p /user/ubuntu\n\n# create example directory\nhadoop fs -mkdir -p /user/ubuntu/word-count/input\n\n# list directory\nhadoop fs -ls -h -R /\nhadoop fs -ls -h -R /user/ubuntu\n\n# create sample files\necho  Hello World Bye World    file01\necho  Hello Hadoop Goodbye Hadoop    file02\n\n# copy from local to hdfs\nhadoop fs -copyFromLocal file01 /user/ubuntu/word-count/input\nhadoop fs -put file02 /user/ubuntu/word-count/input\n\n# verify copied files\nhadoop fs -ls -h -R /user/ubuntu\nhadoop fs -cat /user/ubuntu/word-count/input/file01\nhadoop fs -cat /user/ubuntu/word-count/input/file02\nhadoop fs -cat /user/ubuntu/word-count/input/*\n\n# build the jar (outside the machine to avoid permission issues)\ncd devops-lab/hadoop/example/map-reduce\n./gradlew clean build\n\n# run application\nhadoop jar /vagrant/example/map-reduce/build/libs/map-reduce.jar \\\n  /user/ubuntu/word-count/input \\\n  /user/ubuntu/word-count/output\n\n# check output\nhadoop fs -cat /user/ubuntu/word-count/output/part-r-00000\n\n# delete directory to run it again\nhadoop fs -rm -R /user/ubuntu/word-count/output\n\n# run sample job in a different queue\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  wordcount \\\n  -Dmapreduce.job.queuename=root.priority_queue \\\n  /user/ubuntu/word-count/input \\\n  /user/ubuntu/word-count/output\n\n# well known WARN issue\n# https://issues.apache.org/jira/browse/HDFS-10429", 
            "title": "MapReduce WordCount Job"
        }, 
        {
            "location": "/hadoop/#benchmarking-mapreduce-with-terasort", 
            "text": "# generate random data\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  teragen 1000 random-data\n\n# run terasort benchmark\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  terasort random-data sorted-data\n\n# validate data\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  teravalidate sorted-data report\n\n# useful commands\nhadoop fs -ls -h -R .\nhadoop fs -rm -r random-data\nhadoop fs -cat random-data/part-m-00000\nhadoop fs -cat sorted-data/part-r-00000", 
            "title": "Benchmarking MapReduce with TeraSort"
        }, 
        {
            "location": "/hadoop/#spark", 
            "text": "Spark  is an open-source cluster-computing framework   TODO", 
            "title": "Spark"
        }, 
        {
            "location": "/hadoop/#flink", 
            "text": "TODO", 
            "title": "Flink"
        }, 
        {
            "location": "/hadoop/#avro", 
            "text": "Avro  is a data serialization system   TODO", 
            "title": "Avro"
        }, 
        {
            "location": "/hadoop/#parquet", 
            "text": "Parquet  is a columnar storage format that can efficiently store nested data   TODO", 
            "title": "Parquet"
        }, 
        {
            "location": "/hadoop/#flume", 
            "text": "TODO", 
            "title": "Flume"
        }, 
        {
            "location": "/hadoop/#sqoop", 
            "text": "TODO", 
            "title": "Sqoop"
        }, 
        {
            "location": "/hadoop/#pig", 
            "text": "TODO", 
            "title": "Pig"
        }, 
        {
            "location": "/hadoop/#hive", 
            "text": "TODO", 
            "title": "Hive"
        }, 
        {
            "location": "/hadoop/#crunch", 
            "text": "TODO", 
            "title": "Crunch"
        }, 
        {
            "location": "/hadoop/#hbase", 
            "text": "TODO", 
            "title": "HBase"
        }, 
        {
            "location": "/hadoop/#oozie", 
            "text": "Oozie  is a workflow scheduler system to manage Hadoop jobs   Documentation   Oozie", 
            "title": "Oozie"
        }, 
        {
            "location": "/hadoop/#setup_1", 
            "text": "Oozie is not installed by default   Optional PostgreSQL configuration  - By default Oozie is configured to use Embedded Derby  # access master node\nvagrant ssh master\n\n# install docker\ncurl -fsSL get.docker.com -o get-docker.sh   \\\n  chmod u+x $_   \\\n  ./$_   \\\n  sudo usermod -aG docker hadoop\n\n# logout and login again to verify docker installation\nexit\nvagrant ssh master\nwhoami # whoami\ndocker ps -a\n\n# uncomment PostgreSQL configurations\nvim devops-lab/hadoop/file/oozie/config/oozie-site.xml # from host\nvim /vagrant/file/oozie/config/oozie-site.xml # from guest\n\n# start postgres on guest machine \ndocker run \\\n  --detach \\\n  --name oozie-postgres \\\n  -p 5432:5432 \\\n  -e POSTGRES_DB= oozie-db  \\\n  -e POSTGRES_USER= postgres  \\\n  -e POSTGRES_PASSWORD= password  \\\n  postgres\n\n# permission issue\n# https://github.com/docker-library/postgres/issues/116\n# --volume /vol/postgres:/var/lib/postgresql/data\n\n# access container\ndocker exec -it oozie-postgres bash\npsql --username=postgres\n# list all databases\n\\list\n\\connect oozie-db\n# list all tables\n\\dt\n# describe table\n\\d+ wf_jobs\n# list workflow\nselect * from wf_jobs;  Install and start Oozie  # access master node\nvagrant ssh master\n\n# login as root\nsudo su -\n\n# build, install and init\n/vagrant/script/setup_oozie.sh\n\n# start oozie manually (first time only)\nsu --login hadoop /vagrant/script/bootstrap.sh oozie  It might take a while to build the sources  Useful paths  # data and logs\ndevops-lab/hadoop/.data/master/oozie # host\n/vol/oozie # guest\n\n# (guest) config\n/usr/local/oozie/conf\n\n# (hdfs) examples\n/user/hadoop/examples", 
            "title": "Setup"
        }, 
        {
            "location": "/hadoop/#examples", 
            "text": "Run bundled examples within distribution  # examples path\n.data/master/oozie/examples # host\n/vol/oozie/examples # guest\n\n# access master node as hadoop user\nvagrant ssh master\n\nexport OOZIE_EXAMPLE_PATH=/vol/oozie/examples\nexport OOZIE_HDFS_PATH=/user/$(whoami)/examples\n\n# open map-reduce job.properties\nvim $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties\n\n# edit the following properties\nnameNode=hdfs://namenode.local:9000 # fs.defaultFS @ core-site.xml\njobTracker=resource-manager.local:8032 # yarn.resourcemanager.address @ yarn-site.xml\nqueueName=priority_queue # or default @ fair-scheduler.xml\n\n# upload all the examples\nhadoop fs -put $OOZIE_EXAMPLE_PATH $OOZIE_HDFS_PATH\n\n# verify uploaded files\nhadoop fs -ls -h -R /user/$(whoami)\n\n# run the map-reduce workflow example\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -config $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties \\\n  -run\n\n# verify status\noozie job -oozie http://oozie.local:11000/oozie -info WORKFLOW_ID\n\n# verify result\nhadoop fs -cat $OOZIE_HDFS_PATH/output-data/map-reduce/part-00000\n\n# remove all the examples\nhadoop fs -rm -R $OOZIE_HDFS_PATH", 
            "title": "Examples"
        }, 
        {
            "location": "/hadoop/#useful-commands", 
            "text": "Workflow requires  oozie.wf.application.path  property  Coordinator requires  oozie.coord.application.path  property   # verify oozie status\noozie admin \\\n  -oozie http://oozie.local:11000/oozie \\\n  -status\n\n# verify workflow or coordinator status\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -info JOB_ID \\\n  -verbose\n\n# poll workflow or coordinator status\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -poll JOB_ID \\\n  -interval 10 \\\n  -timeout 60 \\\n  -verbose", 
            "title": "Useful commands"
        }, 
        {
            "location": "/hadoop/#ganglia", 
            "text": "Ganglia  is a monitoring system for Hadoop   TODO", 
            "title": "Ganglia"
        }, 
        {
            "location": "/hadoop/#zeppelin", 
            "text": "TODO", 
            "title": "Zeppelin"
        }, 
        {
            "location": "/hadoop/#knox", 
            "text": "TODO", 
            "title": "Knox"
        }, 
        {
            "location": "/cassandra/", 
            "text": "Cassandra\n\n\nTODO\n\n\nRequirements\n\n\n\n\nDocker", 
            "title": "Cassandra"
        }, 
        {
            "location": "/cassandra/#cassandra", 
            "text": "TODO  Requirements   Docker", 
            "title": "Cassandra"
        }, 
        {
            "location": "/zookeeper/", 
            "text": "Zookeeper\n\n\nTODO", 
            "title": "Zookeeper"
        }, 
        {
            "location": "/zookeeper/#zookeeper", 
            "text": "TODO", 
            "title": "Zookeeper"
        }, 
        {
            "location": "/kafka/", 
            "text": "Kafka\n\n\nTODO", 
            "title": "Kafka"
        }, 
        {
            "location": "/kafka/#kafka", 
            "text": "TODO", 
            "title": "Kafka"
        }, 
        {
            "location": "/other/", 
            "text": "Other\n\n\nDocker\n\n\nDocker is an open platform for developers and sysadmins to build, ship, and run distributed applications.\n\n\nOffical documentation\n\n\n\n\nDocker\n\n\n\n\nBasic Docker commands\n\n\nTODO\n\n\nTODO\n\n\n\n\nDocker Machine\n\n\nTODO\n\n\nTODO\n\n\n\n\nVagrant\n\n\nVagrant is a tool for building and managing virtual machine environments in a single workflow.\n\n\nOffical documentation\n\n\n\n\nVagrant\n\n\nVirtualBox\n\n\n\n\nBasic Vagrant commands\n\n\nSetup project creating a Vagrantfile\n\n\nvagrant init\n\n\n\n\nBoot and connect to the default virtual machine\n\n\nvagrant up\nvagrant status\nvagrant ssh\n\n\n\n\nUseful commands\n\n\n# shut down gracefully\nvagrant halt\n\n# reload (halt + up) + re-provision\nvagrant reload --provision\n\n# update box\nvagrant box update\n\n# delete virtual machine without prompt\nvagrant destory -f\n\n\n\n\nMkDocs\n\n\nMkDocs is a static site generator.\n\n\nOffical documentation\n\n\n\n\nMkDocs\n\n\n\n\nBasic MkDocs commands\n\n\nInstall\n\n\npip install mkdocs\n\n\n\n\nSetup in current directory\n\n\nmkdocs new .\n\n\n\n\nStart dev server with hot reload on \nhttp://127.0.0.1:8000\n\n\nmkdocs serve\n\n\n\n\nBuild static site\n\n\nmkdocs build --clean\n\n\n\n\nDeploy to github\n\n\nmkdocs gh-deploy\n\n\n\n\nSDKMAN!\n\n\nSDKMAN! is a tool for managing parallel versions of multiple Software Development Kits on most Unix based systems.\n\n\nOffical documentation\n\n\n\n\nSDKMAN!\n\n\n\n\nSetup\n\n\ncurl -s \nhttps://get.sdkman.io\n | bash\nsource \n$HOME/.sdkman/bin/sdkman-init.sh\n\nsdk version\n\n\n\n\nGradle\n\n\nSetup\n\n\nsdk list gradle\nsdk install gradle 4.4.1\ngradle -version\n\n\n\n\nCreate Gradle project\n\n\nmkdir -p PROJECT_NAME \n cd $_\ngradle init --type java-library\n\n./gradlew clean build\n\n\n\n\nBooks\n\n\n\n\nDesigning Data-Intensive Applications\n (2017) by Martin Kleppmann\n\n\nHadoop: The Definitive Guide\n (4th)(2015) by Tom White\n\n\nSpark in Action\n (2016) by Petar Ze\u010devi\u0107 and Marko Bona\u0107i\n\n\nCassandra: The Definitive Guide\n (4th)(2016) By Eben Hewitt, Jeff Carpenter\n\n\nKafka: The Definitive Guide\n (2017) By Gwen Shapira, Neha Narkhede, Todd Palino", 
            "title": "Other"
        }, 
        {
            "location": "/other/#other", 
            "text": "", 
            "title": "Other"
        }, 
        {
            "location": "/other/#docker", 
            "text": "Docker is an open platform for developers and sysadmins to build, ship, and run distributed applications.  Offical documentation   Docker", 
            "title": "Docker"
        }, 
        {
            "location": "/other/#basic-docker-commands", 
            "text": "TODO  TODO", 
            "title": "Basic Docker commands"
        }, 
        {
            "location": "/other/#docker-machine", 
            "text": "TODO  TODO", 
            "title": "Docker Machine"
        }, 
        {
            "location": "/other/#vagrant", 
            "text": "Vagrant is a tool for building and managing virtual machine environments in a single workflow.  Offical documentation   Vagrant  VirtualBox", 
            "title": "Vagrant"
        }, 
        {
            "location": "/other/#basic-vagrant-commands", 
            "text": "Setup project creating a Vagrantfile  vagrant init  Boot and connect to the default virtual machine  vagrant up\nvagrant status\nvagrant ssh  Useful commands  # shut down gracefully\nvagrant halt\n\n# reload (halt + up) + re-provision\nvagrant reload --provision\n\n# update box\nvagrant box update\n\n# delete virtual machine without prompt\nvagrant destory -f", 
            "title": "Basic Vagrant commands"
        }, 
        {
            "location": "/other/#mkdocs", 
            "text": "MkDocs is a static site generator.  Offical documentation   MkDocs", 
            "title": "MkDocs"
        }, 
        {
            "location": "/other/#basic-mkdocs-commands", 
            "text": "Install  pip install mkdocs  Setup in current directory  mkdocs new .  Start dev server with hot reload on  http://127.0.0.1:8000  mkdocs serve  Build static site  mkdocs build --clean  Deploy to github  mkdocs gh-deploy", 
            "title": "Basic MkDocs commands"
        }, 
        {
            "location": "/other/#sdkman", 
            "text": "SDKMAN! is a tool for managing parallel versions of multiple Software Development Kits on most Unix based systems.  Offical documentation   SDKMAN!   Setup  curl -s  https://get.sdkman.io  | bash\nsource  $HOME/.sdkman/bin/sdkman-init.sh \nsdk version", 
            "title": "SDKMAN!"
        }, 
        {
            "location": "/other/#gradle", 
            "text": "Setup  sdk list gradle\nsdk install gradle 4.4.1\ngradle -version  Create Gradle project  mkdir -p PROJECT_NAME   cd $_\ngradle init --type java-library\n\n./gradlew clean build", 
            "title": "Gradle"
        }, 
        {
            "location": "/other/#books", 
            "text": "Designing Data-Intensive Applications  (2017) by Martin Kleppmann  Hadoop: The Definitive Guide  (4th)(2015) by Tom White  Spark in Action  (2016) by Petar Ze\u010devi\u0107 and Marko Bona\u0107i  Cassandra: The Definitive Guide  (4th)(2016) By Eben Hewitt, Jeff Carpenter  Kafka: The Definitive Guide  (2017) By Gwen Shapira, Neha Narkhede, Todd Palino", 
            "title": "Books"
        }
    ]
}