{
    "docs": [
        {
            "location": "/",
            "text": "DevOps\n\n\nA collection of Docker and Vagrant images, scripts and documentation mainly related to distributed systems for local development, learning purposes and quick prototyping.\n\n\n\n\nLinux\n\n\nDocker\n\n\nAnsible\n\n\nCassandra\n\n\nZookeeper\n\n\nKafka\n\n\nHadoop\n\n\nKubernetes\n\n\nJVM\n\n\nScala\n\n\nReadings\n\n\nOther",
            "title": "Home"
        },
        {
            "location": "/#devops",
            "text": "A collection of Docker and Vagrant images, scripts and documentation mainly related to distributed systems for local development, learning purposes and quick prototyping.   Linux  Docker  Ansible  Cassandra  Zookeeper  Kafka  Hadoop  Kubernetes  JVM  Scala  Readings  Other",
            "title": "DevOps"
        },
        {
            "location": "/linux/",
            "text": "Linux\n\n\nResources\n\n\n\n\nHow Linux Works\n (2014)(2nd) by Brian Ward (Book)\n\n\n\n\nUseful commands\n\n\n# create nested directories\nmkdir -p parent/child1/child2 && cd $_\n\n# scroll file from bottom\nless +G /var/log/auth.log\n# follow also if doesn't exist\ntail -F /var/log/auth.log\n\n# find files\nfind /etc -name '*shadow'\n\n# prints lines that match regexp\n# -i case insensitive\n# -v inverts the search\n# -c count lines\ngrep -E '^root' /etc/passwd\n# password encryption\ngrep password.*unix /etc/pam.d/*\n\n# sed = stream editor\n# example substitution\necho -e \"a='1st' b='2nd' c='3rd'\\na='4th' b='5th' c='6th'\" > test.txt\ncat test.txt | sed -nE \"s/^.*a='([^']*).*c='([^']*).*$/\\2\\n\\1/gp\" | sort -r | uniq\n# delete lines three through six\nsed 3,6d /etc/passwd\n\n# pick a single field out of an input stream\nls -l | awk '{print $9}'\n# extract 2nd string\necho \"aaa bbb ccc\" > test.txt\ncat test.txt | awk '{printf(\"2nd: %s\\n\",$2)}'\n\n# pack archive\ntar cvf archive.tar file1 file2\n# table-of-content\ntar tvf archive.tar\n# unpack archive\ntar xvf archive.tar -C output\n# compress and pack archive\ntar zcvf archive.tar.gz /path/to/images/*.jpg\n# unpack compressed archive\ntar zxvf archive.tar.gz\n\n# pack archive\nzip -r backup.zip file-name directory-name\n# zip with password (prompt)\nzip -e backup.zip file-name\n# unpack jar\nunzip my-lib.jar -d /tmp/my-lib\n\n# count lines\nwc -l file\n\n# lowercase random uuid\nuuidgen | tr \"[:upper:]\" \"[:lower:]\"\n\n# number of bytes\nstat --printf=\"%s\" file\n\n# calculator\necho 1+2 | bc\n# print number in binary base 2 format\necho 'obase=2; 240' | bc\n# reverse-polish calculator\necho '1 2 + p' | dc\n# evaluate expressions\nexpr 1 + 2\n\n# unix timestamp\ndate +%s\n# timestamp in microsecond\ndate +%s%N\n\n# calendar\ncal -3\n\n# configure kernel parameters at runtime\nsysctl\n\n# test conditions ([)\ntest a = a && echo equal\n\n# create temporary file\nmktemp\n# X is a template\nmktemp /tmp/my-tmp.XXXXXX\n# signal handler to catch the signal that CTRL-C generates and remove the temporary files\nTMPFILE=$(mktemp /tmp/my-tmp.XXXXXX)\ntrap \"rm -f $TMPFILE; exit 1\" INT\n\n# compare files\ndiff FILE1 FILE2\n\n# here document\nDATE=$(date)\ncat <<EOF\nDate: $DATE\nline1\nline2\nEOF\n\n# strip full path and extension if specified e.g. mail\nbasename /var/log/mail.log .log\n\n# image conversion\ngiftopnm\npnmtopng\n\n# when operating on huge number of files to avoid buffer issues\n# e.g. verify file's type\n# INSECURE find . -name '*.md' -print | xargs file\n# change the find output separator and the xargs argument delimiter from a newline to a NULL character\n# two dashes if there is a chance that any of the target files start with a single dash\nfind . -name '*.md' -print0 | xargs -0 file --\n# supply a {} to substitute the filename and a literal ; to indicate the end of the command\nfind . -name '*.md' -exec file {} \\;\n\n# replaces current shell process with the program you name after exec system call\n# after you press CTRL-D or CTRL-C to terminate the cat program,\n# your window should disappear because its child process no longer exists\nexec cat\n\n# subshell example () e.g. path remains the same outside\n(PATH=/bad/invalid:$PATH; echo $PATH)\n# fast way to copy and preserve permissions\ntar cf - orig | (cd target; tar xvf -)\n\n# X Window System\nxwininfo\nxlsclients -l\nxev\nxinput --list\ndbus-monitor --system\ndbus-monitor --session\n\n# compile C program\ncc -o hello hello.c\n# list shared library (so)\nldd /bin/bash\n\n\n\n\nScript templates\n\n\n# shebang\n#!/bin/sh\n#!/bin/bash\n\n# unofficial bash strict mode\nset -euo pipefail\nIFS=$'\\n\\t'\n\n# run from any directory (no symlink allowed)\nCURRENT_PATH=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\"; pwd -P)\ncd ${CURRENT_PATH}\n\n# import\n. imported_file.sh\nsource imported_file.sh\n\n# read and store in a variable\nread MY_VAR\necho $MY_VAR\n# read stdin\nread -p \"Are you sure? [y/n]\" -n 1 -r\n\n\n\n\nDiagnostic\n\n\n# sysfs info\nudevadm info --query=all --name=/dev/xvda\n# monitor kernel uevents\nudevadm monitor\n\n# view kernel's boot and runtime diagnostic messages\ndmesg | less\n\n# system logs paths configuration\nvim /etc/rsyslog.conf\nvim /etc/rsyslog.d/50-default.conf\n# test system logger\nlogger -p mail.info mail-message\ntail -n 1 /var/log/syslog\n\n\n\n\nFilesystem\n\n\n# copy data in blocks of a fixed size\n# /dev/zero is a continuous stream of zero bytes\ndd if=/dev/zero of=DUMP_FILE bs=1024 count=1\n\n# view partition table\n# use (g)parted only for partioning disk (supports MBR and GPT)\nsudo parted -l\n\n# create filesystem\nmkfs -t ext4 /dev/PARTITION_NAME\nls -l /sbin/mkfs.*\n\n# list devices and corresponding filesystems UUID\nblkid\n# list attached filesystems\nmount\n# mount device on mount point\nmount -t ext4 /dev/PARTITION_NAME /MOUNT/POINT\n# mount filesystem by its UUID\nmount UUID=xxx-yyy-zzz /MOUNT/POINT\n# make changes permanent after reboot\necho \"UUID=$(sudo blkid -s UUID -o value /dev/PARTITION_NAME)      /MOUNT/POINT   ext4    defaults,nofail        0       2\" | sudo tee -a /etc/fstab\n# mount all filesystems\nmount -a\n# unmount (detach) a filesystem\numount /dev/PARTITION_NAME\n\n# view size and utilization of mounted filesystems\ndf -h\n# disk usage\ndu -sh /* | sort -g\n# disk size\nfdisk --list\n\n# check memory and swap size\nfree -h\n\n# (1) create swap file (~1GB)\ndd if=/dev/zero of=/dev/SWAP_FILE bs=1024 count=1024000\n# (2) create swap file (2GB)\nfallocate -l 2G /dev/SWAP_FILE\n# change owner and permissions\nchown root:root /dev/SWAP_FILE\nchmod 0600 /dev/SWAP_FILE\n# put swap signature on partition\nmkswap /dev/SWAP_FILE\n# register space with the kernel\nswapon /dev/SWAP_FILE\n# make changes permanent after reboot\necho \"/dev/SWAP_FILE    none    swap    sw    0   0\" | tee -a /etc/fstab\n# list swap partitions\nswapon --show\n\n# simple static server on port 8000\npython -m SimpleHTTPServer\n\n# copy directory to remote host\nscp -r directory remote_host:~/new-directory\ntar cBvf - directory | ssh remote_host tar xBvpf -\nrsync -az directory remote_host:~/new-\n# equivalent to /*\n# -nv dry run\nrsync -a directory/ remote_host:~/new-directory\n\n\n\n\nMonitoring\n\n\n# list processes\n# m show threads\nps aux\n\n# display current system status\n# Spacebar Updates the display immediately\n# M Sorts by current resident memory usage\n# T Sorts by total (cumulative) CPU usage\n# P Sorts by current CPU usage (the default)\n# u Displays only one user\u2019s processes\n# f Selects different statistics to display\n# ? Displays a usage summary for all top commands\ntop\ntop -p PID1 PID2\n# alternatives\nhtop\natop\n\n# monitor system performance\nvmstat 2\n\n# list open files and the processes using them\nlsof | less\nlsof /dev\n\n# print all the system calls that a process makes\nstrace cat /dev/null\nstrace uptime\n# track shared library calls\nltrace ls /\n\n# CPU usage\n/usr/bin/time ls\n\n# change process priority (-20 < nice value < +20)\nrenice 20 PID\n\n# load average: for the past 1 minute, 5 minutes and 15 minutes\nuptime\n\n# check memory status\nfree\ncat /proc/meminfo\n\n# check major/minor page faults\n/usr/bin/time cal > /dev/null\n\n# show statistics for machine\u2019s current uptime (install sysstat)\niostat\n# show partition information\niostat -p ALL\n\n# show I/O resources used by individual processes\niotop\n\n# see the resource consumption of a process over time\npidstat -p PID 1\n\n# reports CPU and IO stats\niostat -mt 2\n\n# system resource statistics\ndstat\n\n\n\n\nNetwork\n\n\n# active network interfaces\nifconfig\n# enable/disable network interface\nifconfig NETWORK_INTERFACE up\nifconfig NETWORK_INTERFACE down\n\n# show routing table\n# Destination: network prefix e.g. 0.0.0.0/0 matches every address (default route)\n# flag U: up\n# flag G: gateway\n# convention: the router is usually at address 1 of the subnet\nroute -n\n\n# ICMP echo request\n# icmp_req: verify order and no gap\n# time: round-trip time\nping -c 3 8.8.8.8\n\n# show path packets take to a remote host\ntraceroute 8.8.8.8\n\n# (DNS) find the IP address behind a domain name\nhost www.github.com\n\n# network manager\nnmcli\nnmcli device show\n# returns zero as its exit code if network is up\nnm-online\n# network details e.g. ssid/password\ncat /etc/NetworkManager/system-connections/NETWORK_NAME\n\n# override hostname lookups\nvim /etc/hosts\n\n# traditional configuration file for DNS servers\ncat /etc/resolv.conf\n# DNS settings\ncat /etc/nsswitch.conf\n\n# static IP\n/etc/network/interfaces\n\n# -t Prints TCP port information\n# -u Prints UDP port information\n# -l Prints listening ports\n# -a Prints every active port\n# -n Disables name lookups (speeds things up; also useful if DNS isn\u2019t working)\n# list open TCP connections\nnetstat -nt\n# print listening TCP ports\nnetstat -ntl\n# list running services\nnetstat -plunt\n\n# processes listening on open TCP ports\nlsof -i -n -P | grep TCP\nlsof -iTCP -sTCP:LISTEN\n# process running on specific port\nlsof -n -i:PORT_NUMBER\n# list unix domain socket\nlsof -U\n\n# well-known ports\ncat /etc/services\n\n# release IP with DHCP\ndhclient -r NETWORK_INTERFACE_NAME\n# renew IP\ndhclient -v NETWORK_INTERFACE_NAME\n\n# public IP via external services\nhttp ident.me\nhttp ipv4.ident.me\nhttp ipv6.ident.me\nhttp icanhazip.com\nhttp ipv4.icanhazip.com\nhttp ipv6.icanhazip.com\n\n# Linux kernel does not automatically move packets from one subnet to another\n# enable temporary IP forwarding in the router's kernel\nsysctl -w net.ipv4.ip_forward\n# change permanent configs upon reboot\nvim /etc/sysctl.conf\n\n# example NAT (IP masquerading)\nsysctl -w net.ipv4.ip_forward\niptables -P FORWARD DROP\niptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE\niptables -A FORWARD -i eth0 -o eth1 -m state --state ESTABLISHED,RELATED -j ACCEPT\niptables -A FORWARD -i eth1 -o eth0 -j ACCEPT\n\n# firewalling on individual machines is sometimes called IP filtering\n# firewall rules in series or chain make up a table\n# INPUT chain: protect individual machine\n# FORWARD chain: protect a network of machines\n# show iptable configuration\niptables -L\n# block IP\niptables -A INPUT -s BLOCKED_IP -j DROP\n# block IP/port\niptables -A INPUT -s BLOCKED_IP/CIDR -p tcp --destination-port BLOCKED_PORT -j DROP\n# allow IP (insert at the bottom)\niptables -A INPUT -s ALLOWED_IP -j ACCEPT\n# allow IP (insert at the top)\niptables -I INPUT -s ALLOWED_IP -j ACCEPT\n# allow IP (specify order)\niptables -I INPUT RULE_NUMBER -s ALLOWED_IP -j ACCEPT\n# delete rule #number in chain\niptables -D INPUT RULE_NUMBER\n\n# show ARP kernel cache\narp -n\n\n# list wireless network\niw dev NETWORK_INTERFACE scan\n# show details of connected network\niw dev NETWORK_INTERFACE link\n\n# manage both authentication and encryption for a wireless network interface\nwpa_supplicant\n\n\n\n\nApplications\n\n\n# old insecure\ntelnet www.wikipedia.org 80\n# press enter twice after\nGET / HTTP/1.0\n\n# details about communication\ncurl --trace-ascii trace_file https://www.wikipedia.org > /dev/null\nvim trace_file\n\n# sshd server configs\nvim /etc/ssh/sshd_config\n# generate key pair\nssh-keygen -t rsa -b 4096 -C KEY_NAME -N \"PASSPHRASE\" -f KEY_PATH\n\n# list network interfaces\ntcpdump -D\n# sniff hex and ascii (-A) by interface/host/port\ntcpdump -XX -n -i INTERFACE_NAME tcp and host IP_ADDRESS and port PORT_NUMBER\n\n# Swiss Army knife\n# banner grabbing\ncat <(echo HEAD / HTTP/1.0) - | netcat IP_ADDRESS PORT_NUMBER\n# install traditional (with -e option)\napt-get install netcat -y\n# choose /bin/nc.traditional\nupdate-alternatives --config nc\n# listen on server\nnetcat -l -p 6996 -e /bin/bash\n# run client\ncat <(echo ls -la) - | netcat IP_ADDRESS 6996\n\n# scan open ports\nnmap -Pn IP_ADDRESS\n\n# other\n# * ssh/scp/sftp/rsync\n# * curl/wget\n\n# system calls\nman recv\nman send\n\n\n\n\nUseful links\n\n\n\n\nSubnetting\n\n\nOpenWrt\n\n\nBusyBox\n\n\nNetfilter\n\n\nIptables Essentials\n\n\niptables vs nftables\n\n\nShorewall\n\n\nNmap\n\n\ntshark\n\n\nPostfix\n\n\nHTTPie\n\n\njq\n\n\nLinux Performance\n\n\nSamba\n\n\nProgram Library HOWTO",
            "title": "Linux"
        },
        {
            "location": "/linux/#linux",
            "text": "Resources   How Linux Works  (2014)(2nd) by Brian Ward (Book)",
            "title": "Linux"
        },
        {
            "location": "/linux/#useful-commands",
            "text": "# create nested directories\nmkdir -p parent/child1/child2 && cd $_\n\n# scroll file from bottom\nless +G /var/log/auth.log\n# follow also if doesn't exist\ntail -F /var/log/auth.log\n\n# find files\nfind /etc -name '*shadow'\n\n# prints lines that match regexp\n# -i case insensitive\n# -v inverts the search\n# -c count lines\ngrep -E '^root' /etc/passwd\n# password encryption\ngrep password.*unix /etc/pam.d/*\n\n# sed = stream editor\n# example substitution\necho -e \"a='1st' b='2nd' c='3rd'\\na='4th' b='5th' c='6th'\" > test.txt\ncat test.txt | sed -nE \"s/^.*a='([^']*).*c='([^']*).*$/\\2\\n\\1/gp\" | sort -r | uniq\n# delete lines three through six\nsed 3,6d /etc/passwd\n\n# pick a single field out of an input stream\nls -l | awk '{print $9}'\n# extract 2nd string\necho \"aaa bbb ccc\" > test.txt\ncat test.txt | awk '{printf(\"2nd: %s\\n\",$2)}'\n\n# pack archive\ntar cvf archive.tar file1 file2\n# table-of-content\ntar tvf archive.tar\n# unpack archive\ntar xvf archive.tar -C output\n# compress and pack archive\ntar zcvf archive.tar.gz /path/to/images/*.jpg\n# unpack compressed archive\ntar zxvf archive.tar.gz\n\n# pack archive\nzip -r backup.zip file-name directory-name\n# zip with password (prompt)\nzip -e backup.zip file-name\n# unpack jar\nunzip my-lib.jar -d /tmp/my-lib\n\n# count lines\nwc -l file\n\n# lowercase random uuid\nuuidgen | tr \"[:upper:]\" \"[:lower:]\"\n\n# number of bytes\nstat --printf=\"%s\" file\n\n# calculator\necho 1+2 | bc\n# print number in binary base 2 format\necho 'obase=2; 240' | bc\n# reverse-polish calculator\necho '1 2 + p' | dc\n# evaluate expressions\nexpr 1 + 2\n\n# unix timestamp\ndate +%s\n# timestamp in microsecond\ndate +%s%N\n\n# calendar\ncal -3\n\n# configure kernel parameters at runtime\nsysctl\n\n# test conditions ([)\ntest a = a && echo equal\n\n# create temporary file\nmktemp\n# X is a template\nmktemp /tmp/my-tmp.XXXXXX\n# signal handler to catch the signal that CTRL-C generates and remove the temporary files\nTMPFILE=$(mktemp /tmp/my-tmp.XXXXXX)\ntrap \"rm -f $TMPFILE; exit 1\" INT\n\n# compare files\ndiff FILE1 FILE2\n\n# here document\nDATE=$(date)\ncat <<EOF\nDate: $DATE\nline1\nline2\nEOF\n\n# strip full path and extension if specified e.g. mail\nbasename /var/log/mail.log .log\n\n# image conversion\ngiftopnm\npnmtopng\n\n# when operating on huge number of files to avoid buffer issues\n# e.g. verify file's type\n# INSECURE find . -name '*.md' -print | xargs file\n# change the find output separator and the xargs argument delimiter from a newline to a NULL character\n# two dashes if there is a chance that any of the target files start with a single dash\nfind . -name '*.md' -print0 | xargs -0 file --\n# supply a {} to substitute the filename and a literal ; to indicate the end of the command\nfind . -name '*.md' -exec file {} \\;\n\n# replaces current shell process with the program you name after exec system call\n# after you press CTRL-D or CTRL-C to terminate the cat program,\n# your window should disappear because its child process no longer exists\nexec cat\n\n# subshell example () e.g. path remains the same outside\n(PATH=/bad/invalid:$PATH; echo $PATH)\n# fast way to copy and preserve permissions\ntar cf - orig | (cd target; tar xvf -)\n\n# X Window System\nxwininfo\nxlsclients -l\nxev\nxinput --list\ndbus-monitor --system\ndbus-monitor --session\n\n# compile C program\ncc -o hello hello.c\n# list shared library (so)\nldd /bin/bash  Script templates  # shebang\n#!/bin/sh\n#!/bin/bash\n\n# unofficial bash strict mode\nset -euo pipefail\nIFS=$'\\n\\t'\n\n# run from any directory (no symlink allowed)\nCURRENT_PATH=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\"; pwd -P)\ncd ${CURRENT_PATH}\n\n# import\n. imported_file.sh\nsource imported_file.sh\n\n# read and store in a variable\nread MY_VAR\necho $MY_VAR\n# read stdin\nread -p \"Are you sure? [y/n]\" -n 1 -r",
            "title": "Useful commands"
        },
        {
            "location": "/linux/#diagnostic",
            "text": "# sysfs info\nudevadm info --query=all --name=/dev/xvda\n# monitor kernel uevents\nudevadm monitor\n\n# view kernel's boot and runtime diagnostic messages\ndmesg | less\n\n# system logs paths configuration\nvim /etc/rsyslog.conf\nvim /etc/rsyslog.d/50-default.conf\n# test system logger\nlogger -p mail.info mail-message\ntail -n 1 /var/log/syslog",
            "title": "Diagnostic"
        },
        {
            "location": "/linux/#filesystem",
            "text": "# copy data in blocks of a fixed size\n# /dev/zero is a continuous stream of zero bytes\ndd if=/dev/zero of=DUMP_FILE bs=1024 count=1\n\n# view partition table\n# use (g)parted only for partioning disk (supports MBR and GPT)\nsudo parted -l\n\n# create filesystem\nmkfs -t ext4 /dev/PARTITION_NAME\nls -l /sbin/mkfs.*\n\n# list devices and corresponding filesystems UUID\nblkid\n# list attached filesystems\nmount\n# mount device on mount point\nmount -t ext4 /dev/PARTITION_NAME /MOUNT/POINT\n# mount filesystem by its UUID\nmount UUID=xxx-yyy-zzz /MOUNT/POINT\n# make changes permanent after reboot\necho \"UUID=$(sudo blkid -s UUID -o value /dev/PARTITION_NAME)      /MOUNT/POINT   ext4    defaults,nofail        0       2\" | sudo tee -a /etc/fstab\n# mount all filesystems\nmount -a\n# unmount (detach) a filesystem\numount /dev/PARTITION_NAME\n\n# view size and utilization of mounted filesystems\ndf -h\n# disk usage\ndu -sh /* | sort -g\n# disk size\nfdisk --list\n\n# check memory and swap size\nfree -h\n\n# (1) create swap file (~1GB)\ndd if=/dev/zero of=/dev/SWAP_FILE bs=1024 count=1024000\n# (2) create swap file (2GB)\nfallocate -l 2G /dev/SWAP_FILE\n# change owner and permissions\nchown root:root /dev/SWAP_FILE\nchmod 0600 /dev/SWAP_FILE\n# put swap signature on partition\nmkswap /dev/SWAP_FILE\n# register space with the kernel\nswapon /dev/SWAP_FILE\n# make changes permanent after reboot\necho \"/dev/SWAP_FILE    none    swap    sw    0   0\" | tee -a /etc/fstab\n# list swap partitions\nswapon --show\n\n# simple static server on port 8000\npython -m SimpleHTTPServer\n\n# copy directory to remote host\nscp -r directory remote_host:~/new-directory\ntar cBvf - directory | ssh remote_host tar xBvpf -\nrsync -az directory remote_host:~/new-\n# equivalent to /*\n# -nv dry run\nrsync -a directory/ remote_host:~/new-directory",
            "title": "Filesystem"
        },
        {
            "location": "/linux/#monitoring",
            "text": "# list processes\n# m show threads\nps aux\n\n# display current system status\n# Spacebar Updates the display immediately\n# M Sorts by current resident memory usage\n# T Sorts by total (cumulative) CPU usage\n# P Sorts by current CPU usage (the default)\n# u Displays only one user\u2019s processes\n# f Selects different statistics to display\n# ? Displays a usage summary for all top commands\ntop\ntop -p PID1 PID2\n# alternatives\nhtop\natop\n\n# monitor system performance\nvmstat 2\n\n# list open files and the processes using them\nlsof | less\nlsof /dev\n\n# print all the system calls that a process makes\nstrace cat /dev/null\nstrace uptime\n# track shared library calls\nltrace ls /\n\n# CPU usage\n/usr/bin/time ls\n\n# change process priority (-20 < nice value < +20)\nrenice 20 PID\n\n# load average: for the past 1 minute, 5 minutes and 15 minutes\nuptime\n\n# check memory status\nfree\ncat /proc/meminfo\n\n# check major/minor page faults\n/usr/bin/time cal > /dev/null\n\n# show statistics for machine\u2019s current uptime (install sysstat)\niostat\n# show partition information\niostat -p ALL\n\n# show I/O resources used by individual processes\niotop\n\n# see the resource consumption of a process over time\npidstat -p PID 1\n\n# reports CPU and IO stats\niostat -mt 2\n\n# system resource statistics\ndstat",
            "title": "Monitoring"
        },
        {
            "location": "/linux/#network",
            "text": "# active network interfaces\nifconfig\n# enable/disable network interface\nifconfig NETWORK_INTERFACE up\nifconfig NETWORK_INTERFACE down\n\n# show routing table\n# Destination: network prefix e.g. 0.0.0.0/0 matches every address (default route)\n# flag U: up\n# flag G: gateway\n# convention: the router is usually at address 1 of the subnet\nroute -n\n\n# ICMP echo request\n# icmp_req: verify order and no gap\n# time: round-trip time\nping -c 3 8.8.8.8\n\n# show path packets take to a remote host\ntraceroute 8.8.8.8\n\n# (DNS) find the IP address behind a domain name\nhost www.github.com\n\n# network manager\nnmcli\nnmcli device show\n# returns zero as its exit code if network is up\nnm-online\n# network details e.g. ssid/password\ncat /etc/NetworkManager/system-connections/NETWORK_NAME\n\n# override hostname lookups\nvim /etc/hosts\n\n# traditional configuration file for DNS servers\ncat /etc/resolv.conf\n# DNS settings\ncat /etc/nsswitch.conf\n\n# static IP\n/etc/network/interfaces\n\n# -t Prints TCP port information\n# -u Prints UDP port information\n# -l Prints listening ports\n# -a Prints every active port\n# -n Disables name lookups (speeds things up; also useful if DNS isn\u2019t working)\n# list open TCP connections\nnetstat -nt\n# print listening TCP ports\nnetstat -ntl\n# list running services\nnetstat -plunt\n\n# processes listening on open TCP ports\nlsof -i -n -P | grep TCP\nlsof -iTCP -sTCP:LISTEN\n# process running on specific port\nlsof -n -i:PORT_NUMBER\n# list unix domain socket\nlsof -U\n\n# well-known ports\ncat /etc/services\n\n# release IP with DHCP\ndhclient -r NETWORK_INTERFACE_NAME\n# renew IP\ndhclient -v NETWORK_INTERFACE_NAME\n\n# public IP via external services\nhttp ident.me\nhttp ipv4.ident.me\nhttp ipv6.ident.me\nhttp icanhazip.com\nhttp ipv4.icanhazip.com\nhttp ipv6.icanhazip.com\n\n# Linux kernel does not automatically move packets from one subnet to another\n# enable temporary IP forwarding in the router's kernel\nsysctl -w net.ipv4.ip_forward\n# change permanent configs upon reboot\nvim /etc/sysctl.conf\n\n# example NAT (IP masquerading)\nsysctl -w net.ipv4.ip_forward\niptables -P FORWARD DROP\niptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE\niptables -A FORWARD -i eth0 -o eth1 -m state --state ESTABLISHED,RELATED -j ACCEPT\niptables -A FORWARD -i eth1 -o eth0 -j ACCEPT\n\n# firewalling on individual machines is sometimes called IP filtering\n# firewall rules in series or chain make up a table\n# INPUT chain: protect individual machine\n# FORWARD chain: protect a network of machines\n# show iptable configuration\niptables -L\n# block IP\niptables -A INPUT -s BLOCKED_IP -j DROP\n# block IP/port\niptables -A INPUT -s BLOCKED_IP/CIDR -p tcp --destination-port BLOCKED_PORT -j DROP\n# allow IP (insert at the bottom)\niptables -A INPUT -s ALLOWED_IP -j ACCEPT\n# allow IP (insert at the top)\niptables -I INPUT -s ALLOWED_IP -j ACCEPT\n# allow IP (specify order)\niptables -I INPUT RULE_NUMBER -s ALLOWED_IP -j ACCEPT\n# delete rule #number in chain\niptables -D INPUT RULE_NUMBER\n\n# show ARP kernel cache\narp -n\n\n# list wireless network\niw dev NETWORK_INTERFACE scan\n# show details of connected network\niw dev NETWORK_INTERFACE link\n\n# manage both authentication and encryption for a wireless network interface\nwpa_supplicant",
            "title": "Network"
        },
        {
            "location": "/linux/#applications",
            "text": "# old insecure\ntelnet www.wikipedia.org 80\n# press enter twice after\nGET / HTTP/1.0\n\n# details about communication\ncurl --trace-ascii trace_file https://www.wikipedia.org > /dev/null\nvim trace_file\n\n# sshd server configs\nvim /etc/ssh/sshd_config\n# generate key pair\nssh-keygen -t rsa -b 4096 -C KEY_NAME -N \"PASSPHRASE\" -f KEY_PATH\n\n# list network interfaces\ntcpdump -D\n# sniff hex and ascii (-A) by interface/host/port\ntcpdump -XX -n -i INTERFACE_NAME tcp and host IP_ADDRESS and port PORT_NUMBER\n\n# Swiss Army knife\n# banner grabbing\ncat <(echo HEAD / HTTP/1.0) - | netcat IP_ADDRESS PORT_NUMBER\n# install traditional (with -e option)\napt-get install netcat -y\n# choose /bin/nc.traditional\nupdate-alternatives --config nc\n# listen on server\nnetcat -l -p 6996 -e /bin/bash\n# run client\ncat <(echo ls -la) - | netcat IP_ADDRESS 6996\n\n# scan open ports\nnmap -Pn IP_ADDRESS\n\n# other\n# * ssh/scp/sftp/rsync\n# * curl/wget\n\n# system calls\nman recv\nman send",
            "title": "Applications"
        },
        {
            "location": "/linux/#useful-links",
            "text": "Subnetting  OpenWrt  BusyBox  Netfilter  Iptables Essentials  iptables vs nftables  Shorewall  Nmap  tshark  Postfix  HTTPie  jq  Linux Performance  Samba  Program Library HOWTO",
            "title": "Useful links"
        },
        {
            "location": "/docker/",
            "text": "Docker\n\n\n\n\nDocker\n is an open platform for developers and sysadmins to build, ship, and run distributed applications\n\n\n\n\nResources\n\n\n\n\n\n\nDocumentation\n\n\n\n\n\n\nDocker in Action\n (2016) by Jeff Nickoloff (Book)\n\n\n\n\n\n\nHow-To\n\n\nSetup\n\n\n# install docker\ncurl -fsSL get.docker.com -o get-docker.sh && \\\n  chmod u+x $_ && \\\n  ./$_ && \\\n  sudo usermod -aG docker hadoop\n\ndocker --version\n\n# install docker-compose\nsudo curl -L https://github.com/docker/compose/releases/download/1.19.0/docker-compose-`uname -s`-`uname -m` \\\n  -o /usr/local/bin/docker-compose && \\\n  sudo chmod +x /usr/local/bin/docker-compose\n\ndocker-compose --version\n\n# install docker-machine (VirtualBox required)\ncurl -L https://github.com/docker/machine/releases/download/v0.13.0/docker-machine-`uname -s`-`uname -m` >/tmp/docker-machine && \\\n  sudo install /tmp/docker-machine /usr/local/bin/docker-machine\n\ndocker-machine --version\n\n\n\n\nUseful commands\n\n\n# list images\ndocker images\n# list containers\ndocker ps -a\n# list volumes\ndocker volume ls\n\n# run temporary container\ndocker run --rm --name phusion phusion/baseimage:latest\n# access container from another shell\ndocker exec -it phusion bash\n\n# remove container by name\ndocker ps -a -q -f name=CONTAINER_NAME | xargs --no-run-if-empty docker rm -f\n# delete dangling images <none>\ndocker images -q -f dangling=true | xargs --no-run-if-empty docker rmi\n# delete dangling volumes\ndocker volume ls -q -f dangling=true | xargs --no-run-if-empty docker volume rm\n\n\n\n\nDocker Machine\n\n\n# create local machine\ndocker-machine create --driver virtualbox default\n\n# list\ndocker-machine ls\ndocker-machine ls --filter name=default\ndocker-machine ls --filter state=Running\ndocker-machine ls --format \"{{.Name}}: {{.DriverName}} - {{.State}}\"\n\n# info\ndocker-machine inspect default\ndocker-machine inspect --format='{{.Driver.IPAddress}}' default\ndocker-machine status default\ndocker-machine ip default\n\n# management\ndocker-machine start default\ndocker-machine stop default\ndocker-machine restart default\ndocker-machine rm default\n\n# mount volume\n#https://docs.docker.com/machine/reference/mount\n\n# show command to connect to machine\ndocker-machine env default\n# check if variables are set\nenv | grep DOCKER\n\n# connect to machine\neval \"$(docker-machine env default)\"\ndocker ps -a\n\n# show command to disconnect from machine\ndocker-machine env -u\n# unset all\neval $(docker-machine env -u)\n\n# access\ndocker-machine ssh default\n# execute command and exit\ndocker-machine ssh default uptime\n# copy files from host to guest\ndocker-machine scp -r /FROM default:/TO\n\n# start nginx on default machine\ndocker run -d -p 8000:80 nginx\n# verify from host\ncurl $(docker-machine ip default):8000\n# forward to port 8080\ndocker-machine ssh default -L 8080:localhost:8000\n# verify tunnel from host\ncurl localhost:8080\n\n# disable error crash reporting\nmkdir -p ~/.docker/machine && touch ~/.docker/machine/no-error-report\n\n\n\n\nBase image\n\n\n\n\nSupervisor\n\n\n\n\nBuild \ndevops/base\n image\n\n\n# change path\ncd devops/base\n\n# build image\ndocker build -t devops/base .\n\n# temporary container\ndocker run --rm --name devops-base devops/base\n# access container\ndocker exec -it devops-base bash\n\n# configurations\n/etc/supervisor/conf.d\n\n# supervisor actions\nsupervisorctl status\nsupervisorctl start SERVICE_NAME\nsupervisorctl stop SERVICE_NAME\n\n\n\n\nDocker Hub\n\n\n\n\nniqdev/phusion-base\n\n\nniqdev/zookeeper\n\n\nniqdev/kafka\n\n\n\n\ndocker login\n\n# phusion-base\n# https://github.com/phusion/baseimage-docker\ndocker build -t devops/base:latest ./base\ndocker tag devops/base niqdev/phusion-base:0.11\ndocker tag devops/base niqdev/phusion-base:latest\ndocker push niqdev/phusion-base:0.11\ndocker push niqdev/phusion-base:latest\n\n# zookeeper\ndocker build -t devops/zookeeper:latest ./zookeeper\ndocker tag devops/zookeeper niqdev/zookeeper:3.4.13\ndocker tag devops/zookeeper niqdev/zookeeper\ndocker push niqdev/zookeeper:3.4.13\ndocker push niqdev/zookeeper:latest\n\n# kafka\ndocker build -t devops/kafka:latest ./kafka\ndocker tag devops/kafka niqdev/kafka:2.0.0\ndocker tag devops/kafka niqdev/kafka\ndocker push niqdev/kafka:2.0.0\ndocker push niqdev/kafka:latest\n\ndocker-compose -f kafka/docker-compose-hub.yml up",
            "title": "Docker"
        },
        {
            "location": "/docker/#docker",
            "text": "Docker  is an open platform for developers and sysadmins to build, ship, and run distributed applications   Resources    Documentation    Docker in Action  (2016) by Jeff Nickoloff (Book)",
            "title": "Docker"
        },
        {
            "location": "/docker/#how-to",
            "text": "Setup  # install docker\ncurl -fsSL get.docker.com -o get-docker.sh && \\\n  chmod u+x $_ && \\\n  ./$_ && \\\n  sudo usermod -aG docker hadoop\n\ndocker --version\n\n# install docker-compose\nsudo curl -L https://github.com/docker/compose/releases/download/1.19.0/docker-compose-`uname -s`-`uname -m` \\\n  -o /usr/local/bin/docker-compose && \\\n  sudo chmod +x /usr/local/bin/docker-compose\n\ndocker-compose --version\n\n# install docker-machine (VirtualBox required)\ncurl -L https://github.com/docker/machine/releases/download/v0.13.0/docker-machine-`uname -s`-`uname -m` >/tmp/docker-machine && \\\n  sudo install /tmp/docker-machine /usr/local/bin/docker-machine\n\ndocker-machine --version  Useful commands  # list images\ndocker images\n# list containers\ndocker ps -a\n# list volumes\ndocker volume ls\n\n# run temporary container\ndocker run --rm --name phusion phusion/baseimage:latest\n# access container from another shell\ndocker exec -it phusion bash\n\n# remove container by name\ndocker ps -a -q -f name=CONTAINER_NAME | xargs --no-run-if-empty docker rm -f\n# delete dangling images <none>\ndocker images -q -f dangling=true | xargs --no-run-if-empty docker rmi\n# delete dangling volumes\ndocker volume ls -q -f dangling=true | xargs --no-run-if-empty docker volume rm  Docker Machine  # create local machine\ndocker-machine create --driver virtualbox default\n\n# list\ndocker-machine ls\ndocker-machine ls --filter name=default\ndocker-machine ls --filter state=Running\ndocker-machine ls --format \"{{.Name}}: {{.DriverName}} - {{.State}}\"\n\n# info\ndocker-machine inspect default\ndocker-machine inspect --format='{{.Driver.IPAddress}}' default\ndocker-machine status default\ndocker-machine ip default\n\n# management\ndocker-machine start default\ndocker-machine stop default\ndocker-machine restart default\ndocker-machine rm default\n\n# mount volume\n#https://docs.docker.com/machine/reference/mount\n\n# show command to connect to machine\ndocker-machine env default\n# check if variables are set\nenv | grep DOCKER\n\n# connect to machine\neval \"$(docker-machine env default)\"\ndocker ps -a\n\n# show command to disconnect from machine\ndocker-machine env -u\n# unset all\neval $(docker-machine env -u)\n\n# access\ndocker-machine ssh default\n# execute command and exit\ndocker-machine ssh default uptime\n# copy files from host to guest\ndocker-machine scp -r /FROM default:/TO\n\n# start nginx on default machine\ndocker run -d -p 8000:80 nginx\n# verify from host\ncurl $(docker-machine ip default):8000\n# forward to port 8080\ndocker-machine ssh default -L 8080:localhost:8000\n# verify tunnel from host\ncurl localhost:8080\n\n# disable error crash reporting\nmkdir -p ~/.docker/machine && touch ~/.docker/machine/no-error-report",
            "title": "How-To"
        },
        {
            "location": "/docker/#base-image",
            "text": "Supervisor   Build  devops/base  image  # change path\ncd devops/base\n\n# build image\ndocker build -t devops/base .\n\n# temporary container\ndocker run --rm --name devops-base devops/base\n# access container\ndocker exec -it devops-base bash\n\n# configurations\n/etc/supervisor/conf.d\n\n# supervisor actions\nsupervisorctl status\nsupervisorctl start SERVICE_NAME\nsupervisorctl stop SERVICE_NAME",
            "title": "Base image"
        },
        {
            "location": "/docker/#docker-hub",
            "text": "niqdev/phusion-base  niqdev/zookeeper  niqdev/kafka   docker login\n\n# phusion-base\n# https://github.com/phusion/baseimage-docker\ndocker build -t devops/base:latest ./base\ndocker tag devops/base niqdev/phusion-base:0.11\ndocker tag devops/base niqdev/phusion-base:latest\ndocker push niqdev/phusion-base:0.11\ndocker push niqdev/phusion-base:latest\n\n# zookeeper\ndocker build -t devops/zookeeper:latest ./zookeeper\ndocker tag devops/zookeeper niqdev/zookeeper:3.4.13\ndocker tag devops/zookeeper niqdev/zookeeper\ndocker push niqdev/zookeeper:3.4.13\ndocker push niqdev/zookeeper:latest\n\n# kafka\ndocker build -t devops/kafka:latest ./kafka\ndocker tag devops/kafka niqdev/kafka:2.0.0\ndocker tag devops/kafka niqdev/kafka\ndocker push niqdev/kafka:2.0.0\ndocker push niqdev/kafka:latest\n\ndocker-compose -f kafka/docker-compose-hub.yml up",
            "title": "Docker Hub"
        },
        {
            "location": "/ansible/",
            "text": "Ansible\n\n\n\n\nAnsible\n is an open source automation platform that can help with config management, deployment and task automation\n\n\n\n\nResources\n\n\n\n\n\n\nDocumentation\n\n\n\n\n\n\nAnsible - Up and Running\n (2017) by Lorin Hochstein and Rene Moser (Book)\n\n\n\n\n\n\nTutorial\n\n\n\n\n\n\nPlaybook example\n\n\n\n\n\n\nThe following guide explains how to provision Ansible locally and play with it. Checkout the \nVagrantfile\n and the Vagrant \nguide\n for more details.\n\n\nSetup\n\n\nRequirements\n\n\n\n\nVagrant\n\n\nVirtualBox\n\n\n\n\nDirectory structure\n\n\ntree -a ansible/\nansible/\n\u251c\u2500\u2500 .share\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-3\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ssh\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ansible_rsa\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 ansible_rsa.pub\n\u251c\u2500\u2500 Vagrantfile\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 group_vars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 host_vars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hosts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 roles\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 common\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 defaults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 files\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 handlers\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 motd.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 package.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 templates\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 motd\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 vars\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 docker\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 site.yml\n\u251c\u2500\u2500 setup_ansible.sh\n\u2514\u2500\u2500 setup_share.sh\n\n\n\n\nThe first time \nonly\n, you have to setup the shared folders and generate the ssh key needed by ansible to access all nodes executing\n\n\n./setup_share.sh\n\n\n\n\nStart the boxes with\n\n\nvagrant up\n\n\n\n\nThe first time it could take a while\n\n\nVerify status of the boxes with\n\n\nvagrant status\n\n\n\n\nVerify access to the boxes with\n\n\nvagrant ssh ansible\nvagrant ssh node-1\n\n\n\n\nFrom inside the boxes you should be able to communicate with the others\n\n\nping ansible.local\nping ip-192-168-100-11.local\nping 192.168.100.12\n\n\n\n\nThe following paths are shared with the boxes\n\n\n\n\n/vagrant\n provision-tool\n\n\n/local\n host $HOME\n\n\n/ansible\n data \n(ansible only)\n\n\n/data\n .share \n(node only)\n\n\n\n\nAd-Hoc Commands\n\n\nAccess the ansible box with\n\n\nvagrant ssh ansible\n\n\n\n\nBelow a list of examples\n\n\n\n# ping all nodes (default inventory /etc/ansible/hosts)\nansible all -m ping\nansible ansible -m ping\nansible cluster -m ping\n\n# ping all nodes (specify inventory)\nansible all -i \"/vagrant/data/hosts\" -m ping\n\n# gathering facts\nansible all -m setup\nansible ansible -m setup\n\n# specify host and user\nansible ip-192-168-100-11.local -m ping -u vagrant\n\n# execute command\nansible all -a \"/bin/echo hello\"\nansible all -a \"uptime\"\nansible all -a \"/bin/date\"\n# do NOT reboot vagrant through ansible (use vagrant reload)\nansible cluster -a \"/sbin/reboot\" --become\n\n# shell module\nansible all -m shell -a \"pwd\"\n# be carefull to quotes\nansible all -m shell -a 'echo $HOME'\n\n# update && upgrade\nansible all -m apt -a \"update_cache=yes upgrade=dist\" --become\n# restart after upgrade\nvagrant reload\n# install package\nansible all -m apt -a \"name=tree state=present\" --become\n\n\n\n\nPlaybooks\n\n\nAccess the ansible box with\n\n\nvagrant ssh ansible\n\n\n\n\nBelow a list of examples\n\n\n# test uptime on all node\nansible-playbook /ansible/site.yml --tags=test --verbose\n\n# update & upgrade only cluster nodes\nansible-playbook /ansible/site.yml -t package --skip-tags=java --verbose\n\n# install packages on cluster nodes\nansible-playbook /ansible/site.yml -t package --verbose\n\n# run common task on cluster node\nansible-playbook /ansible/site.yml -t common\n\n# setup docker\nansible-playbook /ansible/site.yml -t docker\n# test docker\nvagrant ssh node-1\nsudo -i -u docker\ndocker ps -a\n\n# custom banner\nansible-playbook /ansible/site.yml -t motd\n\n# setup all infrastructure at once\nansible-playbook /ansible/site.yml\n\n# dry run\nansible-playbook -i /ansible/hosts /ansible/site.yml --check --diff",
            "title": "Ansible"
        },
        {
            "location": "/ansible/#ansible",
            "text": "Ansible  is an open source automation platform that can help with config management, deployment and task automation   Resources    Documentation    Ansible - Up and Running  (2017) by Lorin Hochstein and Rene Moser (Book)    Tutorial    Playbook example    The following guide explains how to provision Ansible locally and play with it. Checkout the  Vagrantfile  and the Vagrant  guide  for more details.",
            "title": "Ansible"
        },
        {
            "location": "/ansible/#setup",
            "text": "Requirements   Vagrant  VirtualBox   Directory structure  tree -a ansible/\nansible/\n\u251c\u2500\u2500 .share\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-1\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-3\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 ssh\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 ansible_rsa\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 ansible_rsa.pub\n\u251c\u2500\u2500 Vagrantfile\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 group_vars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 host_vars\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hosts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 roles\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 common\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 defaults\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 files\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 handlers\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 motd.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 package.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 templates\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 motd\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 vars\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 docker\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 meta\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 tasks\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0         \u2514\u2500\u2500 main.yml\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 site.yml\n\u251c\u2500\u2500 setup_ansible.sh\n\u2514\u2500\u2500 setup_share.sh  The first time  only , you have to setup the shared folders and generate the ssh key needed by ansible to access all nodes executing  ./setup_share.sh  Start the boxes with  vagrant up  The first time it could take a while  Verify status of the boxes with  vagrant status  Verify access to the boxes with  vagrant ssh ansible\nvagrant ssh node-1  From inside the boxes you should be able to communicate with the others  ping ansible.local\nping ip-192-168-100-11.local\nping 192.168.100.12  The following paths are shared with the boxes   /vagrant  provision-tool  /local  host $HOME  /ansible  data  (ansible only)  /data  .share  (node only)",
            "title": "Setup"
        },
        {
            "location": "/ansible/#ad-hoc-commands",
            "text": "Access the ansible box with  vagrant ssh ansible  Below a list of examples  \n# ping all nodes (default inventory /etc/ansible/hosts)\nansible all -m ping\nansible ansible -m ping\nansible cluster -m ping\n\n# ping all nodes (specify inventory)\nansible all -i \"/vagrant/data/hosts\" -m ping\n\n# gathering facts\nansible all -m setup\nansible ansible -m setup\n\n# specify host and user\nansible ip-192-168-100-11.local -m ping -u vagrant\n\n# execute command\nansible all -a \"/bin/echo hello\"\nansible all -a \"uptime\"\nansible all -a \"/bin/date\"\n# do NOT reboot vagrant through ansible (use vagrant reload)\nansible cluster -a \"/sbin/reboot\" --become\n\n# shell module\nansible all -m shell -a \"pwd\"\n# be carefull to quotes\nansible all -m shell -a 'echo $HOME'\n\n# update && upgrade\nansible all -m apt -a \"update_cache=yes upgrade=dist\" --become\n# restart after upgrade\nvagrant reload\n# install package\nansible all -m apt -a \"name=tree state=present\" --become",
            "title": "Ad-Hoc Commands"
        },
        {
            "location": "/ansible/#playbooks",
            "text": "Access the ansible box with  vagrant ssh ansible  Below a list of examples  # test uptime on all node\nansible-playbook /ansible/site.yml --tags=test --verbose\n\n# update & upgrade only cluster nodes\nansible-playbook /ansible/site.yml -t package --skip-tags=java --verbose\n\n# install packages on cluster nodes\nansible-playbook /ansible/site.yml -t package --verbose\n\n# run common task on cluster node\nansible-playbook /ansible/site.yml -t common\n\n# setup docker\nansible-playbook /ansible/site.yml -t docker\n# test docker\nvagrant ssh node-1\nsudo -i -u docker\ndocker ps -a\n\n# custom banner\nansible-playbook /ansible/site.yml -t motd\n\n# setup all infrastructure at once\nansible-playbook /ansible/site.yml\n\n# dry run\nansible-playbook -i /ansible/hosts /ansible/site.yml --check --diff",
            "title": "Playbooks"
        },
        {
            "location": "/cassandra/",
            "text": "Cassandra\n\n\n\n\nCassandra\n is a distributed database for managing large amounts of structured data across many commodity servers, while providing highly available service and no single point of failure\n\n\n\n\nResources\n\n\n\n\n\n\nDocumentation\n\n\n\n\n\n\nCassandra: The Definitive Guide\n (2016)(4th) by Eben Hewitt, Jeff Carpenter (Book)\n\n\n\n\n\n\nA Decentralized Structured Storage System\n (Paper)\n\n\n\n\n\n\nA Big Data Modeling Methodology for Apache Cassandra\n (Paper)\n\n\n\n\n\n\nFacebook\u2019s Cassandra paper\n\n\n\n\n\n\nCassandra Data Modeling Best Practices\n\n\n\n\n\n\nDifference between partition key, composite key and clustering key\n\n\n\n\n\n\nCassandra Cluster Manager\n\n\n\n\n\n\nNetflix Priam\n\n\n\n\n\n\ncstar_perf\n\n\n\n\n\n\nAmy's Cassandra 2.1 tuning guide\n\n\n\n\n\n\nRepair in Cassandra\n\n\n\n\n\n\n\n\n\nCassandra uses a tick-tock release model, even-numbered releases are feature releases, while odd-numbered releases are focused on bug fixes\n\n\nArchitecture\n\n\n\n\n\n\nA \nrack\n is a logical set of nodes in close proximity to each other\n\n\n\n\n\n\nA \ndata center\n is a logical set of racks\n\n\n\n\n\n\nCassandra uses a \ngossip protocol\n (called epidemic protocol) that allows each node to keep track of state information about the other nodes in the cluster implementing an algorithm called \nPhi Accrual Failure Detection\n instead of simple heartbeats\n\n\n\n\n\n\nThe job of a \nsnitch\n is to determine relative host proximity for each node in a cluster, which is used to determine which nodes to read and write from\n\n\n\n\n\n\nCassandra represents the data managed by a cluster as a \nring\n. Each node in the ring is assigned one or more ranges of data described by a \ntoken\n, which determines its position in the ring and is used to identify each partition\n\n\n\n\n\n\n\n\n\n\n\n\nvirtual nodes\n allow to break a token range and assign multiple tokens to a single physical node\n\n\n\n\n\n\nA \npartitioner\n is a hash function for computing the token of a partition key and determines how a (wide) row or partition of data is distributed within the ring\n\n\n\n\n\n\nThe \nreplication factor\n is the number of nodes in a cluster that will receive copies of the same row and the replication strategy is set independently for each keyspace\n\n\n\n\n\n\nCassandra provides tuneable \nconsistency\n levels and must be specified on each read or write\n\n\n\n\n\n\nA client may connect to any node in the cluster, named \ncoordinator node\n, to initiate a read or write query. The coordinator identifies which nodes are replicas for the data and forwards the queries to them\n\n\n\n\n\n\n\n\n\n\n\n\nWhen a write operation is performed, it's immediately written to a \ncommit log\n to ensure that data is not lost. It is a crash-recovery mechanism only, clients never read from it\n\n\n\n\n\n\nAfter it's written to the commit log, the value is written (already ordered) to a memory-resident data structure called the \nmemtable\n divided by Column Family (table)\n\n\n\n\n\n\nWhen the number of objects stored in the memtable or in the commit log reaches a threshold, the contents of the memtable are flushed (non-blocking operation) to disk in a file called \nSSTable\n and a new memtable or commit log is then created/recycled\n\n\n\n\n\n\nNo reads or seeks of any kind are required for writing a value to Cassandra because all writes are append operations to immutable SSTables. However, periodic \ncompaction\n operations in Cassandra are performed in order to support fast read performance: the keys are merged, columns are combined, tombstones are discarded, and a new index is created\n\n\n\n\n\n\nThe \nkey cache\n stores a map of partition keys to row index entries, facilitating faster read access into SSTables stored on disk. The key cache is stored on the JVM heap\n\n\n\n\n\n\nThe \nrow cache\n caches entire rows and can greatly speed up read access for frequently accessed rows, at the cost of more memory usage. The row cache is stored in off-heap memory\n\n\n\n\n\n\nThe \ncounter cache\n is used to improve counter performance by reducing lock contention for the most frequently accessed counters\n\n\n\n\n\n\nIn a scenario in which a write request is sent to Cassandra, but a replica node where the write properly belongs is not available due to network partition, hardware failure, or some other reason, to ensure general availability Cassandra implements a feature called \nhinted handoff\n. The coordinator node while store temporarily the data until it detects that the node is available again\n\n\n\n\n\n\nWrite Path\n\n\n\n\nRead Path\n\n\n\n\n\n\n\n\nTo provide \nlinearizable consistency\n e.g. read-before-write, Cassandra supports a \nlightweight transaction\n or LWT. The implementation is based on \npaxos\n and is limited to a single partition\n\n\n\n\n\n\nA \ntombstone\n is a deletion marker that is required to suppress older data in SSTables until compaction or garbage collection run. Data is not immediately deleted but it's treated as an update operation\n\n\n\n\n\n\nBloom filters\n are very fast, non-deterministic algorithms for testing whether an element is a member of a set. It is possible to get a false-positive read, but not a false-negative. When a read is performed, the filter is checked first before accessing disk, if it indicates that the element does not exist in the set, it certainly doesn't, but if the filter thinks that the element is in the set, the disk is accessed to make sure\n\n\n\n\n\n\nReplica Synchronization (1)\n Cassandra reads data from multiple replicas in order to achieve the requested consistency level and detects if any replicas have out of date values. If an insufficient number of nodes have the latest value, a \nread repair\n is performed immediately to update the out of date replicas\n\n\n\n\n\n\nReplica Synchronization (2)\n \nAnti-entropy repair\n is a manually initiated operation performed on nodes as part of a regular maintenance process executed with \nnodetool\n causing a \nmajor compaction\n during which a node exchange \nMerkle trees\n with neighboring nodes\n\n\n\n\n\n\nSetup\n\n\nSingle Node Cluster\n\n\n# change path\ncd devops/cassandra\n\n# start single node\ndocker-compose up\n\n# paths\n/etc/cassandra\n/var/lib/cassandra\n/var/log/cassandra\n\n# remove container and volume\ndocker rm -fv devops-cassandra\n\n\n\n\nMulti Node Cluster\n\n\n# change path\ncd devops/cassandra\n\n# start node\ndocker-compose -f docker-compose-cluster.yml up\n\n# optional mounted volumes\nmkdir -p \\\n  .cassandra/cassandra-seed/{data,log} \\\n  .cassandra/cassandra-node-1/{data,log} \\\n  .cassandra/cassandra-node-2/{data,log}\ntree .cassandra/\n\n# ISSUES releated to host permissions\n# > Small commitlog volume detected at /var/lib/cassandra/commitlog\n# > There is insufficient memory for the Java Runtime Environment to continue\n(cassandra) /var/lib/cassandra\n(root) /var/log/cassandra\n\n\n\n\nAccess container\n\n\n# access container\ndocker exec -it devops-cassandra bash\ndocker exec -it devops-cassandra bash -c cqlsh\ndocker exec -it devops-cassandra-seed bash\ndocker exec -it devops-cassandra-node-1 bash\n\n# execute cql script from host\n(docker exec -i devops-cassandra bash \\\n  -c \"cat > example.cql; cqlsh -f example.cql\") < cql/example_create.cql\n\n\n\n\nCQL\n\n\ncqlsh\n script \nexamples\n\n\n# connect\ncqlsh localhost 9042\ncqlsh localhost 9042 -u cassandra -p cassandra\n\n# execute cql script\ncqlsh -f cql/example_create.cql\n\n# info\nSHOW VERSION;\nDESCRIBE CLUSTER;\nDESCRIBE KEYSPACES;\nDESCRIBE KEYSPACE example;\nDESCRIBE TABLE example.messages;\n\n# nice format\nEXPAND ON;\n# trace query\nTRACING ON;\n\n# bulk loading\nCOPY example.users TO '/cql/users.csv' WITH HEADER=TRUE;\nCOPY example.users FROM '/cql/all_users.csv' WITH DELIMITER = ';';\nCOPY example.users (first_name,last_name,addresses,emails,enable) FROM '/cql/column_users.csv' WITH HEADER=TRUE;\n\n# automatic paging\nPAGING;\nPAGING ON;\nPAGING 100;\n# limit\nSELECT * FROM example.users LIMIT 1;\n\n\n\n\n\n\nBatch\n\n\nUser-Defined Type\n\n\nUser-Defined Function\n\n\nUser-Defined Aggregate Function\n\n\n\n\nOld \ncassandra-cli\n deprecated and removed in Cassandra 3.0\n\n\nUSE keyspace_name;\nLIST table_name;\nGET table_name[\"primary_key\"];\nSET table_name[\"primary_key\"][\"column_name\"];\n\n\n\n\nnodetool\n\n\n# help\nnodetool\n\n# cluster informations\nnodetool describecluster\nnodetool status\n\n# node informations\nnodetool -h xxx.xxx.xxx.xxx info\nnodetool -h xxx.xxx.xxx.xxx statusgossip|statusthrift|statusbinary|statushandoff\nnodetool gossipinfo\n\n# ring informations\nnodetool ring\nnodetool describering KEYSPACE\n\n# monitor network\nnodetool netstats\n\n# threadpool statistics\nnodetool tpstats\n\n# keyspace statistics\nnodetool tablestats KEYSPACE\n\n# dynamic logging via JMX\nnodetool getlogginglevels\n\n# force to write data from memtables to SSTables\nnodetool flush\n\n# gracefully shutdown\nnodetool drain\n\n# discards any data that is no longer owned by the node\n# e.g. after changing replication factor or token range\nnodetool cleanup\n\n# anti-entropy repair or manual repair: reconcile data exchanging Merkle trees among nodes\n# maintenance: incremental parallel repair on the primary token range (run on each node)\nnodetool repair -pr\n\n# create snapshot\nnodetool snapshot\nnodetool listsnapshots\n\n# restore snapshot (create schema or truncate table before)\n# 1) same cluster and configuration\n# copy SSTable \".db\" files into the data directory and on the running node execute refresh\nnodetool refresh\n# 2) different configuration (e.g. topology, token ranges, or replication)\nsstableloader\n\n# stress tool\ncassandra-stress write n=1000000\ncassandra-stress read n=200000",
            "title": "Cassandra"
        },
        {
            "location": "/cassandra/#cassandra",
            "text": "Cassandra  is a distributed database for managing large amounts of structured data across many commodity servers, while providing highly available service and no single point of failure   Resources    Documentation    Cassandra: The Definitive Guide  (2016)(4th) by Eben Hewitt, Jeff Carpenter (Book)    A Decentralized Structured Storage System  (Paper)    A Big Data Modeling Methodology for Apache Cassandra  (Paper)    Facebook\u2019s Cassandra paper    Cassandra Data Modeling Best Practices    Difference between partition key, composite key and clustering key    Cassandra Cluster Manager    Netflix Priam    cstar_perf    Amy's Cassandra 2.1 tuning guide    Repair in Cassandra     Cassandra uses a tick-tock release model, even-numbered releases are feature releases, while odd-numbered releases are focused on bug fixes",
            "title": "Cassandra"
        },
        {
            "location": "/cassandra/#architecture",
            "text": "A  rack  is a logical set of nodes in close proximity to each other    A  data center  is a logical set of racks    Cassandra uses a  gossip protocol  (called epidemic protocol) that allows each node to keep track of state information about the other nodes in the cluster implementing an algorithm called  Phi Accrual Failure Detection  instead of simple heartbeats    The job of a  snitch  is to determine relative host proximity for each node in a cluster, which is used to determine which nodes to read and write from    Cassandra represents the data managed by a cluster as a  ring . Each node in the ring is assigned one or more ranges of data described by a  token , which determines its position in the ring and is used to identify each partition       virtual nodes  allow to break a token range and assign multiple tokens to a single physical node    A  partitioner  is a hash function for computing the token of a partition key and determines how a (wide) row or partition of data is distributed within the ring    The  replication factor  is the number of nodes in a cluster that will receive copies of the same row and the replication strategy is set independently for each keyspace    Cassandra provides tuneable  consistency  levels and must be specified on each read or write    A client may connect to any node in the cluster, named  coordinator node , to initiate a read or write query. The coordinator identifies which nodes are replicas for the data and forwards the queries to them       When a write operation is performed, it's immediately written to a  commit log  to ensure that data is not lost. It is a crash-recovery mechanism only, clients never read from it    After it's written to the commit log, the value is written (already ordered) to a memory-resident data structure called the  memtable  divided by Column Family (table)    When the number of objects stored in the memtable or in the commit log reaches a threshold, the contents of the memtable are flushed (non-blocking operation) to disk in a file called  SSTable  and a new memtable or commit log is then created/recycled    No reads or seeks of any kind are required for writing a value to Cassandra because all writes are append operations to immutable SSTables. However, periodic  compaction  operations in Cassandra are performed in order to support fast read performance: the keys are merged, columns are combined, tombstones are discarded, and a new index is created    The  key cache  stores a map of partition keys to row index entries, facilitating faster read access into SSTables stored on disk. The key cache is stored on the JVM heap    The  row cache  caches entire rows and can greatly speed up read access for frequently accessed rows, at the cost of more memory usage. The row cache is stored in off-heap memory    The  counter cache  is used to improve counter performance by reducing lock contention for the most frequently accessed counters    In a scenario in which a write request is sent to Cassandra, but a replica node where the write properly belongs is not available due to network partition, hardware failure, or some other reason, to ensure general availability Cassandra implements a feature called  hinted handoff . The coordinator node while store temporarily the data until it detects that the node is available again    Write Path   Read Path     To provide  linearizable consistency  e.g. read-before-write, Cassandra supports a  lightweight transaction  or LWT. The implementation is based on  paxos  and is limited to a single partition    A  tombstone  is a deletion marker that is required to suppress older data in SSTables until compaction or garbage collection run. Data is not immediately deleted but it's treated as an update operation    Bloom filters  are very fast, non-deterministic algorithms for testing whether an element is a member of a set. It is possible to get a false-positive read, but not a false-negative. When a read is performed, the filter is checked first before accessing disk, if it indicates that the element does not exist in the set, it certainly doesn't, but if the filter thinks that the element is in the set, the disk is accessed to make sure    Replica Synchronization (1)  Cassandra reads data from multiple replicas in order to achieve the requested consistency level and detects if any replicas have out of date values. If an insufficient number of nodes have the latest value, a  read repair  is performed immediately to update the out of date replicas    Replica Synchronization (2)   Anti-entropy repair  is a manually initiated operation performed on nodes as part of a regular maintenance process executed with  nodetool  causing a  major compaction  during which a node exchange  Merkle trees  with neighboring nodes",
            "title": "Architecture"
        },
        {
            "location": "/cassandra/#setup",
            "text": "Single Node Cluster  # change path\ncd devops/cassandra\n\n# start single node\ndocker-compose up\n\n# paths\n/etc/cassandra\n/var/lib/cassandra\n/var/log/cassandra\n\n# remove container and volume\ndocker rm -fv devops-cassandra  Multi Node Cluster  # change path\ncd devops/cassandra\n\n# start node\ndocker-compose -f docker-compose-cluster.yml up\n\n# optional mounted volumes\nmkdir -p \\\n  .cassandra/cassandra-seed/{data,log} \\\n  .cassandra/cassandra-node-1/{data,log} \\\n  .cassandra/cassandra-node-2/{data,log}\ntree .cassandra/\n\n# ISSUES releated to host permissions\n# > Small commitlog volume detected at /var/lib/cassandra/commitlog\n# > There is insufficient memory for the Java Runtime Environment to continue\n(cassandra) /var/lib/cassandra\n(root) /var/log/cassandra  Access container  # access container\ndocker exec -it devops-cassandra bash\ndocker exec -it devops-cassandra bash -c cqlsh\ndocker exec -it devops-cassandra-seed bash\ndocker exec -it devops-cassandra-node-1 bash\n\n# execute cql script from host\n(docker exec -i devops-cassandra bash \\\n  -c \"cat > example.cql; cqlsh -f example.cql\") < cql/example_create.cql",
            "title": "Setup"
        },
        {
            "location": "/cassandra/#cql",
            "text": "cqlsh  script  examples  # connect\ncqlsh localhost 9042\ncqlsh localhost 9042 -u cassandra -p cassandra\n\n# execute cql script\ncqlsh -f cql/example_create.cql\n\n# info\nSHOW VERSION;\nDESCRIBE CLUSTER;\nDESCRIBE KEYSPACES;\nDESCRIBE KEYSPACE example;\nDESCRIBE TABLE example.messages;\n\n# nice format\nEXPAND ON;\n# trace query\nTRACING ON;\n\n# bulk loading\nCOPY example.users TO '/cql/users.csv' WITH HEADER=TRUE;\nCOPY example.users FROM '/cql/all_users.csv' WITH DELIMITER = ';';\nCOPY example.users (first_name,last_name,addresses,emails,enable) FROM '/cql/column_users.csv' WITH HEADER=TRUE;\n\n# automatic paging\nPAGING;\nPAGING ON;\nPAGING 100;\n# limit\nSELECT * FROM example.users LIMIT 1;   Batch  User-Defined Type  User-Defined Function  User-Defined Aggregate Function   Old  cassandra-cli  deprecated and removed in Cassandra 3.0  USE keyspace_name;\nLIST table_name;\nGET table_name[\"primary_key\"];\nSET table_name[\"primary_key\"][\"column_name\"];",
            "title": "CQL"
        },
        {
            "location": "/cassandra/#nodetool",
            "text": "# help\nnodetool\n\n# cluster informations\nnodetool describecluster\nnodetool status\n\n# node informations\nnodetool -h xxx.xxx.xxx.xxx info\nnodetool -h xxx.xxx.xxx.xxx statusgossip|statusthrift|statusbinary|statushandoff\nnodetool gossipinfo\n\n# ring informations\nnodetool ring\nnodetool describering KEYSPACE\n\n# monitor network\nnodetool netstats\n\n# threadpool statistics\nnodetool tpstats\n\n# keyspace statistics\nnodetool tablestats KEYSPACE\n\n# dynamic logging via JMX\nnodetool getlogginglevels\n\n# force to write data from memtables to SSTables\nnodetool flush\n\n# gracefully shutdown\nnodetool drain\n\n# discards any data that is no longer owned by the node\n# e.g. after changing replication factor or token range\nnodetool cleanup\n\n# anti-entropy repair or manual repair: reconcile data exchanging Merkle trees among nodes\n# maintenance: incremental parallel repair on the primary token range (run on each node)\nnodetool repair -pr\n\n# create snapshot\nnodetool snapshot\nnodetool listsnapshots\n\n# restore snapshot (create schema or truncate table before)\n# 1) same cluster and configuration\n# copy SSTable \".db\" files into the data directory and on the running node execute refresh\nnodetool refresh\n# 2) different configuration (e.g. topology, token ranges, or replication)\nsstableloader\n\n# stress tool\ncassandra-stress write n=1000000\ncassandra-stress read n=200000",
            "title": "nodetool"
        },
        {
            "location": "/zookeeper/",
            "text": "ZooKeeper\n\n\n\n\nZooKeeper\n is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services\n\n\n\n\nResources\n\n\n\n\n\n\nDocumentation\n\n\n\n\n\n\nCurator\n\n\n\n\n\n\nSetup\n\n\nRequirements\n\n\n\n\nBase\n image\n\n\n\n\nBuild \ndevops/zookeeper\n image\n\n\n# change path\ncd devops/zookeeper\n\n# build image\ndocker build -t devops/zookeeper:latest .\n# build image with specific version - see Dockerfile for version 3.5.x\ndocker build -t devops/zookeeper:3.4.10 --build-arg VERSION=3.4.10 .\n\n# temporary container [host:container]\ndocker run --rm --name zookeeper -p 12181:2181 devops/zookeeper\n# access container\ndocker exec -it zookeeper bash\n\n# paths\n/opt/zookeeper\n/var/log/zookeeper\n/var/lib/zookeeper\n/var/log/supervisord.log\n\n# logs\ntail -F /var/log/supervisord.log\n# check service status\nsupervisorctl status\nsupervisorctl restart zookeeper\n\n\n\n\nExample\n\n\ndocker exec -it zookeeper bash\n\n# (option 1) check zookeeper status\necho ruok | nc localhost 2181\n\n# (option 2) check zookeeper status\ntelnet localhost 2181\n# expect answer imok\n> ruok\n\nzkCli.sh -server 127.0.0.1:2181\nhelp\n# list znodes\nls /\n# create znode and associate value\ncreate /zk_test my_data\n# verify data\nget /zk_test\n# change value\nset /zk_test junk\n# delete znode\ndelete /zk_test\n\n\n\n\nThe four-letter words\n\n\n\n\n\n\n\n\nCategory\n\n\nCommand\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nServer status\n\n\nruok\n\n\nPrints \nimok\n if the server is running and not in an error state\n\n\n\n\n\n\n\n\nconf\n\n\nPrints the server configuration (from zoo.cfg)\n\n\n\n\n\n\n\n\nenvi\n\n\nPrints the server environment, including ZooKeeper version, Java version, and other system properties\n\n\n\n\n\n\n\n\nsrvr\n\n\nPrints server statistics, including latency statistics, the number of znodes, and the server mode (standalone, leader, or follower)\n\n\n\n\n\n\n\n\nstat\n\n\nPrints server statistics and connected clients\n\n\n\n\n\n\n\n\nsrst\n\n\nResets server statistics\n\n\n\n\n\n\n\n\nisro\n\n\nShows whether the server is in read-only ( ro ) mode (due to a network partition) or read/write mode (rw)\n\n\n\n\n\n\nClient connections\n\n\ndump\n\n\nLists all the sessions and ephemeral znodes for the ensemble. You must connect to the leader (see srvr) for this command\n\n\n\n\n\n\n\n\ncons\n\n\nLists connection statistics for all the server's clients\n\n\n\n\n\n\n\n\ncrst\n\n\nResets connection statistics\n\n\n\n\n\n\nWatches\n\n\nwchs\n\n\nLists summary information for the server's watches\n\n\n\n\n\n\n\n\nwchc\n\n\nLists all the server's watches by connection, may impact server performance for a large number of watches\n\n\n\n\n\n\n\n\nwchp\n\n\nLists all the server\u2019s watches by znode path, may impact server performance for a large number of watches\n\n\n\n\n\n\nMonitoring\n\n\nmntr\n\n\nLists server statistics in Java properties format, suitable as a source for monitoring systems such as Ganglia and Nagios",
            "title": "ZooKeeper"
        },
        {
            "location": "/zookeeper/#zookeeper",
            "text": "ZooKeeper  is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services   Resources    Documentation    Curator",
            "title": "ZooKeeper"
        },
        {
            "location": "/zookeeper/#setup",
            "text": "Requirements   Base  image   Build  devops/zookeeper  image  # change path\ncd devops/zookeeper\n\n# build image\ndocker build -t devops/zookeeper:latest .\n# build image with specific version - see Dockerfile for version 3.5.x\ndocker build -t devops/zookeeper:3.4.10 --build-arg VERSION=3.4.10 .\n\n# temporary container [host:container]\ndocker run --rm --name zookeeper -p 12181:2181 devops/zookeeper\n# access container\ndocker exec -it zookeeper bash\n\n# paths\n/opt/zookeeper\n/var/log/zookeeper\n/var/lib/zookeeper\n/var/log/supervisord.log\n\n# logs\ntail -F /var/log/supervisord.log\n# check service status\nsupervisorctl status\nsupervisorctl restart zookeeper  Example  docker exec -it zookeeper bash\n\n# (option 1) check zookeeper status\necho ruok | nc localhost 2181\n\n# (option 2) check zookeeper status\ntelnet localhost 2181\n# expect answer imok\n> ruok\n\nzkCli.sh -server 127.0.0.1:2181\nhelp\n# list znodes\nls /\n# create znode and associate value\ncreate /zk_test my_data\n# verify data\nget /zk_test\n# change value\nset /zk_test junk\n# delete znode\ndelete /zk_test",
            "title": "Setup"
        },
        {
            "location": "/zookeeper/#the-four-letter-words",
            "text": "Category  Command  Description      Server status  ruok  Prints  imok  if the server is running and not in an error state     conf  Prints the server configuration (from zoo.cfg)     envi  Prints the server environment, including ZooKeeper version, Java version, and other system properties     srvr  Prints server statistics, including latency statistics, the number of znodes, and the server mode (standalone, leader, or follower)     stat  Prints server statistics and connected clients     srst  Resets server statistics     isro  Shows whether the server is in read-only ( ro ) mode (due to a network partition) or read/write mode (rw)    Client connections  dump  Lists all the sessions and ephemeral znodes for the ensemble. You must connect to the leader (see srvr) for this command     cons  Lists connection statistics for all the server's clients     crst  Resets connection statistics    Watches  wchs  Lists summary information for the server's watches     wchc  Lists all the server's watches by connection, may impact server performance for a large number of watches     wchp  Lists all the server\u2019s watches by znode path, may impact server performance for a large number of watches    Monitoring  mntr  Lists server statistics in Java properties format, suitable as a source for monitoring systems such as Ganglia and Nagios",
            "title": "The four-letter words"
        },
        {
            "location": "/kafka/",
            "text": "Kafka\n\n\n\n\nKafka\n is a distributed streaming platform\n\n\n\n\nResources\n\n\n\n\n\n\nDocumentation\n\n\n\n\n\n\nKafka: The Definitive Guide\n (2017) by Gwen Shapira, Neha Narkhede, Todd Palino (Book)\n\n\n\n\n\n\nKafka: a Distributed Messaging System for Log Processing\n (Paper)\n\n\n\n\n\n\nSchema Registry\n\n\n\n\n\n\nKafkaProducer javadocs\n\n\n\n\n\n\nKafkaConsumer javadocs\n\n\n\n\n\n\nReactive Kafka\n\n\n\n\n\n\nkafkacat\n\n\n\n\n\n\nShould you put several event types in the same Kafka topic?\n\n\n\n\n\n\nKafka Partitioning\n\n\n\n\n\n\nArchitecture\n\n\n\n\n\n\nKafka is a publish/subscribe messaging system often described as a \ndistributed commit log\n or \ndistributing streaming platform\n\n\n\n\n\n\nThe unit of data is called a \nmessage\n, which is simply an array of bytes and it can have a \nkey\n used to assign partitions. A \nbatch\n is a collection of messages, all of which are being produced to the same topic and partition\n\n\n\n\n\n\nMessages are categorized into \ntopics\n which are additionally broken down into a number of \npartitions\n. Each partition is splitted into \nsegments\n for storage purposes and each segment is stored in a single data file which contains messages and their offsets\n\n\n\n\n\n\nMessages are written in an append-only fashion and are read in order from beginning to end. As a topic typically has multiple partitions, there is no guarantee of message time-ordering across the entire topic, just within a single partition\n\n\n\n\n\n\nIn order to help brokers quickly locate the message for a given offset, Kafka maintains an \nindex\n for each partition. The index maps offsets to segment files and positions within the file\n\n\n\n\n\n\nA \nstream\n is considered to be a single topic of data, regardless of the number of partitions\n\n\n\n\n\n\nProducers\n, publishers or writers, create new messages to a specific topic. By default, the producer does not care what partition a specific message is written to and will balance messages over all partitions of a topic evenly\n\n\n\n\n\n\n\n\n\n\nConsumers\n, subscribers or readers, read messages. The consumer subscribes to one or more topics and reads the messages in the order in which they were produced. The consumer keeps track of which messages it has already consumed by keeping track of the \noffset\n of messages i.e. an integer value that continually\nincreases. Each message in a given partition has a unique offset stored either in Zookeeper or in Kafka itself\n\n\n\n\n\n\n\n\n\n\nConsumers work as part of a \nconsumer group\n, which is one or more consumers that work together to consume a topic. The group assures that each partition is only consumed by one member. The mapping of a consumer to a partition is often called \nownership\n of the partition by the consumer\n\n\n\n\n\n\nWhen a new consumer is added to a group, or when a consumer shuts down or crashes leaving the group, it cause reassignment of partitions to other consumers. Moving partition ownership from one consumer to another is called a \nrebalance\n which provide high availability and scalability\n\n\n\n\n\n\nConsumers maintain membership in a consumer group and ownership of the partitions assigned to them by sending \nheartbeats\n to a Kafka broker designated as the \ngroup coordinator\n\n\n\n\n\n\nYou can't have multiple consumers that belong to the same group in one thread and you can't have multiple threads safely use the same consumer\n\n\n\n\n\n\n\n\n\n\nConsumers must keep polling or they will be considered dead and the partitions they are consuming will be handed to another consumer in the group to continue consuming. Consumers \ncommit\n (track) their offset (position) in each partition to a special \n__consumer_offsets\n topic. If a consumer crashes or a new consumer joins the consumer group, this will trigger a rebalance. After a rebalance, each consumer may be assigned a new set of partitions than the one it processed before. In order to know where to pick up the work, the consumer will read the latest committed offset of each partition and continue from there\n\n\n\n\n\n\n\n\n\n\n\n\nA single Kafka server is called a \nbroker\n. The broker receives messages from producers, assigns offsets to them, and commits the messages to storage on disk. It also services consumers, responding to fetch requests for partitions and responding with the messages that have been committed to disk\n\n\n\n\n\n\nKafka brokers are designed to operate as part of a \ncluster\n. A partition is owned by a single broker in the cluster and that broker is called the \nleader\n of the partition. A partition may be assigned to multiple brokers, which will result in the partition being replicated. All events are produced to and consumed from the \nleader\n replica. Other \nfollower\n replicas just need to stay \nin-sync\n with the leader and replicate all the recent events on time\n\n\n\n\n\n\nKafka uses \nZookeeper\n to maintain the list of brokers that are currently members of a cluster. Every time a broker process starts, it registers itself with a unique identifier by creating an \nephemeral node\n. Kafka uses Zookeeper's ephemeral node feature to elect a \ncontroller\n. The controller is responsible for electing leaders among the partitions and replicas whenever it notices nodes join and leave the cluster\n\n\n\n\n\n\n\n\n\n\n\n\nData in Kafka is organized by topics. Each topic is partitioned and each partition can have multiple \nreplicas\n. Those replicas are stored on brokers and each broker stores replicas belonging to different topics and partitions\n\n\n\n\n\n\nA key feature is that of \nretention\n. Brokers are configured with a default retention setting for topics, either retaining messages for some period of \ntime\n or until the topic reaches a certain \nsize\n in bytes. Once these limits are reached, messages are expired and deleted\n\n\n\n\n\n\nMirrorMaker\n is a tool to coordinates multiple clusters or datacenters and replicate data\n\n\n\n\n\n\nSetup\n\n\nRequirements\n\n\n\n\nBase\n docker image \n\n\nZooKeeper\n docker image\n\n\n\n\nBuild \ndevops/kafka\n image\n\n\n# change path\ncd devops/kafka\n\n# build image\ndocker build -t devops/kafka .\n\n# create network\ndocker network create --driver bridge my_network\ndocker network ls\ndocker network inspect my_network\n\n# start temporary zookeeper container [host:container]\ndocker run --rm \\\n  --name zookeeper \\\n  -p 12181:2181 \\\n  --network=my_network \\\n  devops/zookeeper\n# access container\ndocker exec -it zookeeper bash\n\n# start temporary kafka container [host:container]\ndocker run --rm \\\n  --name kafka \\\n  -p 19092:9092 \\\n  --network=my_network \\\n  -e ZOOKEEPER_HOSTS=\"zookeeper:2181\" \\\n  devops/kafka\n# access container\ndocker exec -it kafka bash\n\n# paths\n/opt/kafka\n/opt/kafka/logs\n/var/lib/kafka/data\n\n# supervisor logs\n/var/log/kafka\n/var/log/connect\ntail -F /var/log/kafka/stdout\nless +G /var/log/connect/stdout\n\n\n\n\nAlternatively use \ndocker-compose\n\n\n# change path\ncd devops/kafka\n\n# build base image\ndocker build -t devops/base ../base\n# build + start zookeeper and kafka\ndocker-compose up\n\n# access container\ndocker exec -it devops-zookeeper bash\ndocker exec -it devops-kafka bash\n\n\n\n\nHow-To\n\n\nKafka\n\n\ndocker exec -it devops-kafka bash\n\n# create topic\nkafka-topics.sh --zookeeper zookeeper:2181 \\\n  --create --if-not-exists --replication-factor 1 --partitions 1 --topic test\n\n# view topic\nkafka-topics.sh --zookeeper zookeeper:2181 --list \nkafka-topics.sh --zookeeper zookeeper:2181 --describe --topic test\nkafka-topics.sh --zookeeper zookeeper:2181 --describe --under-replicated-partitions\nkafka-topics.sh --zookeeper zookeeper:2181 --describe --unavailable-partitions\n\n# produce\nkafka-console-producer.sh --broker-list kafka:9092 --topic test\n# util\nkafkacat -P -b 0 -t test\n\n# consume\nkafka-console-consumer.sh --bootstrap-server kafka:9092 --topic test --from-beginning\n# util\nkafkacat -C -b 0 -t test\n\n# list consumers\nkafka-consumer-groups.sh --bootstrap-server kafka:9092 --list\n# view lag (GROUP_NAME from previous command)\nkafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group GROUP_NAME\n\n# delete\nkafka-topics.sh --zookeeper zookeeper:2181 --delete --topic test\n\n# verify log segment and index\nkafka-run-class.sh kafka.tools.DumpLogSegments \\\n  --files /var/lib/kafka/data/test-0/00000000000000000000.log\nkafka-run-class.sh kafka.tools.DumpLogSegments \\\n  --index-sanity-check \\\n  --files /var/lib/kafka/data/test-0/00000000000000000000.index\n\n# inspect __consumer_offsets\nkafka-console-consumer.sh --bootstrap-server kafka:9092 \\\n  --topic __consumer_offsets \\\n  --formatter \"kafka.coordinator.group.GroupMetadataManager\\$OffsetsMessageFormatter\" \\\n  --max-messages 1\n\n\n\n\nConnect\n\n\ndocker exec -it devops-kafka bash\n\n# verify connect\nhttp :8083\nhttp :8083/connector-plugins\n\n# write file to topic\nhttp POST :8083/connectors \\\n  name=load-kafka-config \\\n  config:='{\"connector.class\":\"FileStreamSource\",\"file\":\"/opt/kafka/config/server.properties\",\"topic\":\"kafka-config-topic\"}'\n\n# verify topic\nkafka-console-consumer.sh --bootstrap-server=kafka:9092 \\\n  --topic kafka-config-topic --from-beginning\n\n# write topic to file\nhttp POST :8083/connectors \\\n  name=dump-kafka-config \\\n  config:='{\"connector.class\":\"FileStreamSink\",\"file\":\"/tmp/copy-of-server-properties\",\"topics\":\"kafka-config-topic\"}'\n\n# verify file\nvim /tmp/copy-of-server-properties\n\n# manage connectors\nhttp :8083/connectors\nhttp DELETE :8083/connectors/dump-kafka-config\n\n\n\n\nZooKeeper\n\n\ndocker exec -it devops-zookeeper bash\n\n# start cli\nzkCli.sh\n\n# view ephemeral nodes\nls /brokers/ids\nget /brokers/ids/0\n\n# view topics\nls /brokers/topics\nget /brokers/topics/test\n\n\n\n\nSchema Registry\n\n\n# docker-hub images\ndocker-compose -f kafka/docker-compose-hub.yml up\ndocker exec -it devops-schema-registry bash\n\n# register new schema\nhttp -v POST :8081/subjects/ExampleSchema/versions \\\n  Accept:application/vnd.schemaregistry.v1+json \\\n  schema='{\"type\":\"string\"}'\n\n# list subjects and schema\nhttp -v :8081/subjects \\\n  Accept:application/vnd.schemaregistry.v1+json\nhttp -v :8081/subjects/ExampleSchema/versions \\\n  Accept:application/vnd.schemaregistry.v1+json\nhttp -v :8081/subjects/ExampleSchema/versions/1 \\\n  Accept:application/vnd.schemaregistry.v1+json\n\n# ui [mac|linux]\n[open|xdg-open] http://localhost:8082",
            "title": "Kafka"
        },
        {
            "location": "/kafka/#kafka",
            "text": "Kafka  is a distributed streaming platform   Resources    Documentation    Kafka: The Definitive Guide  (2017) by Gwen Shapira, Neha Narkhede, Todd Palino (Book)    Kafka: a Distributed Messaging System for Log Processing  (Paper)    Schema Registry    KafkaProducer javadocs    KafkaConsumer javadocs    Reactive Kafka    kafkacat    Should you put several event types in the same Kafka topic?    Kafka Partitioning",
            "title": "Kafka"
        },
        {
            "location": "/kafka/#architecture",
            "text": "Kafka is a publish/subscribe messaging system often described as a  distributed commit log  or  distributing streaming platform    The unit of data is called a  message , which is simply an array of bytes and it can have a  key  used to assign partitions. A  batch  is a collection of messages, all of which are being produced to the same topic and partition    Messages are categorized into  topics  which are additionally broken down into a number of  partitions . Each partition is splitted into  segments  for storage purposes and each segment is stored in a single data file which contains messages and their offsets    Messages are written in an append-only fashion and are read in order from beginning to end. As a topic typically has multiple partitions, there is no guarantee of message time-ordering across the entire topic, just within a single partition    In order to help brokers quickly locate the message for a given offset, Kafka maintains an  index  for each partition. The index maps offsets to segment files and positions within the file    A  stream  is considered to be a single topic of data, regardless of the number of partitions    Producers , publishers or writers, create new messages to a specific topic. By default, the producer does not care what partition a specific message is written to and will balance messages over all partitions of a topic evenly      Consumers , subscribers or readers, read messages. The consumer subscribes to one or more topics and reads the messages in the order in which they were produced. The consumer keeps track of which messages it has already consumed by keeping track of the  offset  of messages i.e. an integer value that continually\nincreases. Each message in a given partition has a unique offset stored either in Zookeeper or in Kafka itself      Consumers work as part of a  consumer group , which is one or more consumers that work together to consume a topic. The group assures that each partition is only consumed by one member. The mapping of a consumer to a partition is often called  ownership  of the partition by the consumer    When a new consumer is added to a group, or when a consumer shuts down or crashes leaving the group, it cause reassignment of partitions to other consumers. Moving partition ownership from one consumer to another is called a  rebalance  which provide high availability and scalability    Consumers maintain membership in a consumer group and ownership of the partitions assigned to them by sending  heartbeats  to a Kafka broker designated as the  group coordinator    You can't have multiple consumers that belong to the same group in one thread and you can't have multiple threads safely use the same consumer      Consumers must keep polling or they will be considered dead and the partitions they are consuming will be handed to another consumer in the group to continue consuming. Consumers  commit  (track) their offset (position) in each partition to a special  __consumer_offsets  topic. If a consumer crashes or a new consumer joins the consumer group, this will trigger a rebalance. After a rebalance, each consumer may be assigned a new set of partitions than the one it processed before. In order to know where to pick up the work, the consumer will read the latest committed offset of each partition and continue from there       A single Kafka server is called a  broker . The broker receives messages from producers, assigns offsets to them, and commits the messages to storage on disk. It also services consumers, responding to fetch requests for partitions and responding with the messages that have been committed to disk    Kafka brokers are designed to operate as part of a  cluster . A partition is owned by a single broker in the cluster and that broker is called the  leader  of the partition. A partition may be assigned to multiple brokers, which will result in the partition being replicated. All events are produced to and consumed from the  leader  replica. Other  follower  replicas just need to stay  in-sync  with the leader and replicate all the recent events on time    Kafka uses  Zookeeper  to maintain the list of brokers that are currently members of a cluster. Every time a broker process starts, it registers itself with a unique identifier by creating an  ephemeral node . Kafka uses Zookeeper's ephemeral node feature to elect a  controller . The controller is responsible for electing leaders among the partitions and replicas whenever it notices nodes join and leave the cluster       Data in Kafka is organized by topics. Each topic is partitioned and each partition can have multiple  replicas . Those replicas are stored on brokers and each broker stores replicas belonging to different topics and partitions    A key feature is that of  retention . Brokers are configured with a default retention setting for topics, either retaining messages for some period of  time  or until the topic reaches a certain  size  in bytes. Once these limits are reached, messages are expired and deleted    MirrorMaker  is a tool to coordinates multiple clusters or datacenters and replicate data",
            "title": "Architecture"
        },
        {
            "location": "/kafka/#setup",
            "text": "Requirements   Base  docker image   ZooKeeper  docker image   Build  devops/kafka  image  # change path\ncd devops/kafka\n\n# build image\ndocker build -t devops/kafka .\n\n# create network\ndocker network create --driver bridge my_network\ndocker network ls\ndocker network inspect my_network\n\n# start temporary zookeeper container [host:container]\ndocker run --rm \\\n  --name zookeeper \\\n  -p 12181:2181 \\\n  --network=my_network \\\n  devops/zookeeper\n# access container\ndocker exec -it zookeeper bash\n\n# start temporary kafka container [host:container]\ndocker run --rm \\\n  --name kafka \\\n  -p 19092:9092 \\\n  --network=my_network \\\n  -e ZOOKEEPER_HOSTS=\"zookeeper:2181\" \\\n  devops/kafka\n# access container\ndocker exec -it kafka bash\n\n# paths\n/opt/kafka\n/opt/kafka/logs\n/var/lib/kafka/data\n\n# supervisor logs\n/var/log/kafka\n/var/log/connect\ntail -F /var/log/kafka/stdout\nless +G /var/log/connect/stdout  Alternatively use  docker-compose  # change path\ncd devops/kafka\n\n# build base image\ndocker build -t devops/base ../base\n# build + start zookeeper and kafka\ndocker-compose up\n\n# access container\ndocker exec -it devops-zookeeper bash\ndocker exec -it devops-kafka bash",
            "title": "Setup"
        },
        {
            "location": "/kafka/#how-to",
            "text": "Kafka  docker exec -it devops-kafka bash\n\n# create topic\nkafka-topics.sh --zookeeper zookeeper:2181 \\\n  --create --if-not-exists --replication-factor 1 --partitions 1 --topic test\n\n# view topic\nkafka-topics.sh --zookeeper zookeeper:2181 --list \nkafka-topics.sh --zookeeper zookeeper:2181 --describe --topic test\nkafka-topics.sh --zookeeper zookeeper:2181 --describe --under-replicated-partitions\nkafka-topics.sh --zookeeper zookeeper:2181 --describe --unavailable-partitions\n\n# produce\nkafka-console-producer.sh --broker-list kafka:9092 --topic test\n# util\nkafkacat -P -b 0 -t test\n\n# consume\nkafka-console-consumer.sh --bootstrap-server kafka:9092 --topic test --from-beginning\n# util\nkafkacat -C -b 0 -t test\n\n# list consumers\nkafka-consumer-groups.sh --bootstrap-server kafka:9092 --list\n# view lag (GROUP_NAME from previous command)\nkafka-consumer-groups.sh --bootstrap-server kafka:9092 --describe --group GROUP_NAME\n\n# delete\nkafka-topics.sh --zookeeper zookeeper:2181 --delete --topic test\n\n# verify log segment and index\nkafka-run-class.sh kafka.tools.DumpLogSegments \\\n  --files /var/lib/kafka/data/test-0/00000000000000000000.log\nkafka-run-class.sh kafka.tools.DumpLogSegments \\\n  --index-sanity-check \\\n  --files /var/lib/kafka/data/test-0/00000000000000000000.index\n\n# inspect __consumer_offsets\nkafka-console-consumer.sh --bootstrap-server kafka:9092 \\\n  --topic __consumer_offsets \\\n  --formatter \"kafka.coordinator.group.GroupMetadataManager\\$OffsetsMessageFormatter\" \\\n  --max-messages 1  Connect  docker exec -it devops-kafka bash\n\n# verify connect\nhttp :8083\nhttp :8083/connector-plugins\n\n# write file to topic\nhttp POST :8083/connectors \\\n  name=load-kafka-config \\\n  config:='{\"connector.class\":\"FileStreamSource\",\"file\":\"/opt/kafka/config/server.properties\",\"topic\":\"kafka-config-topic\"}'\n\n# verify topic\nkafka-console-consumer.sh --bootstrap-server=kafka:9092 \\\n  --topic kafka-config-topic --from-beginning\n\n# write topic to file\nhttp POST :8083/connectors \\\n  name=dump-kafka-config \\\n  config:='{\"connector.class\":\"FileStreamSink\",\"file\":\"/tmp/copy-of-server-properties\",\"topics\":\"kafka-config-topic\"}'\n\n# verify file\nvim /tmp/copy-of-server-properties\n\n# manage connectors\nhttp :8083/connectors\nhttp DELETE :8083/connectors/dump-kafka-config  ZooKeeper  docker exec -it devops-zookeeper bash\n\n# start cli\nzkCli.sh\n\n# view ephemeral nodes\nls /brokers/ids\nget /brokers/ids/0\n\n# view topics\nls /brokers/topics\nget /brokers/topics/test  Schema Registry  # docker-hub images\ndocker-compose -f kafka/docker-compose-hub.yml up\ndocker exec -it devops-schema-registry bash\n\n# register new schema\nhttp -v POST :8081/subjects/ExampleSchema/versions \\\n  Accept:application/vnd.schemaregistry.v1+json \\\n  schema='{\"type\":\"string\"}'\n\n# list subjects and schema\nhttp -v :8081/subjects \\\n  Accept:application/vnd.schemaregistry.v1+json\nhttp -v :8081/subjects/ExampleSchema/versions \\\n  Accept:application/vnd.schemaregistry.v1+json\nhttp -v :8081/subjects/ExampleSchema/versions/1 \\\n  Accept:application/vnd.schemaregistry.v1+json\n\n# ui [mac|linux]\n[open|xdg-open] http://localhost:8082",
            "title": "How-To"
        },
        {
            "location": "/hadoop/",
            "text": "Hadoop\n\n\nThe following guide explains how to provision a Multi Node Hadoop Cluster locally and play with it. Checkout the \nVagrantfile\n and the Vagrant \nguide\n for more details.\n\n\nResources\n\n\n\n\n\n\nDocumentation\n\n\n\n\n\n\nHadoop: The Definitive Guide\n (2015)(4th) by Tom White (Book)\n\n\n\n\n\n\nThe Hadoop Ecosystem Table\n\n\n\n\n\n\nHadoop Internals\n\n\n\n\n\n\nSetup\n\n\nRequirements\n\n\n\n\nVagrant\n\n\nVirtualBox\n\n\n\n\nDirectory structure\n\n\ntree -a hadoop/\nhadoop/\n\u251c\u2500\u2500 .data # mounted volume\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop_rsa\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop_rsa.pub\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 master\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 mapred\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 yarn\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 namenode\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 secondary\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 spark\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 zeppelin\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 notebook\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 datanode\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 mapred\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u2514\u2500\u2500 yarn\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-3\n\u251c\u2500\u2500 example\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 map-reduce\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 spark\n\u251c\u2500\u2500 file\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 core-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fair-scheduler.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hdfs-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 mapred-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 masters\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 slaves\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 yarn-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-hadoop.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hosts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 motd\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie-env.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 oozie-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-oozie.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 spark\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 log4j.properties\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 spark-env.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-spark.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ssh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 zeppelin\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 zeppelin-env.sh\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 profile-zeppelin.sh\n\u251c\u2500\u2500 script\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bootstrap.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_hadoop.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_oozie.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_spark.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_ubuntu.sh\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 setup_zeppelin.sh\n\u251c\u2500\u2500 Vagrantfile\n\u2514\u2500\u2500 vagrant_hadoop.sh\n\n\n\n\nImport the script\n\n\nsource vagrant_hadoop.sh\n\n\n\n\nCreate and start a Multi Node Hadoop Cluster\n\n\nhadoop-start\n\n\n\n\nThe first time it might take a while\n\n\nAccess the cluster via ssh, check also the \n/etc/hosts\n file\n\n\nvagrant ssh master\nssh hadoop@172.16.0.10 -i .data/hadoop_rsa\n\n# 3 nodes\nvagrant ssh node-1\nssh hadoop@172.16.0.101 -i .data/hadoop_rsa\n\n\n\n\nDestroy the cluster\n\n\nhadoop-destroy\n\n\n\n\nFor convenience add to the host machine\n\n\ncat hadoop/file/hosts | sudo tee --append /etc/hosts\n\n\n\n\nWeb UI links\n\n\n\n\nNameNode: \nhttp://namenode.local:50070\n\n\nNameNode metrics: \nhttp://namenode.local:50070/jmx\n\n\nResourceManager: \nhttp://resource-manager.local:8088\n\n\nLog Level: \nhttp://resource-manager.local:8088/logLevel\n\n\nWeb Application Proxy Server: \nhttp://web-proxy.local:8100/proxy/application_XXX_0000\n\n\nMapReduce Job History Server: \nhttp://history.local:19888\n\n\nDataNode/NodeManager (1): \nhttp://node-1.local:8042/node\n\n\nDataNode/NodeManager (2): \nhttp://node-2.local:8042/node\n\n\nDataNode/NodeManager (3): \nhttp://node-3.local:8042/node\n\n\nSpark: \nhttp://spark.local:4040\n\n\nSpark History Server: \nhttp://spark-history.local:18080\n\n\nZeppelin (*): \nhttp://zeppelin.local:8080\n\n\nOozie (*): \nhttp://oozie.local:11000\n\n\n\n\n(*) Not installed by default\n\n\nHDFS and MapReduce\n\n\n\n\nHDFS\n is a distributed file system that provides high-throughput access to application data\n\n\nYARN\n is a framework for job scheduling and cluster resource management\n\n\nMapReduce\n is a YARN-based system for parallel processing of large data sets\n\n\n\n\nDocumentation\n\n\n\n\nHadoop v2.7.6\n\n\nUntangling Apache Hadoop YARN\n series\n\n\n\n\nAdmin\n\n\nHDFS cli\n\n\n# help\nhdfs\n\n# filesystem statistics\nhdfs dfsadmin -report\n\n# filesystem check\nhdfs fsck /\n\n\n\n\nYARN cli\n\n\n# help\nyarn\n\n# list yarn applications\nyarn application -list\n\n# list nodes\nyarn node -list\n\n# view application logs\nyarn logs -applicationId APPLICATION_ID\n\n# kill yarn application\nyarn application -kill APPLICATION_ID\n\n\n\n\nUseful paths\n\n\n# data and logs\ndevops/hadoop/.data/master/hadoop # host\n/vol/hadoop # guest\n\n# (guest) config\n/usr/local/hadoop/etc/hadoop\n\n# (hdfs) map-reduce history\n/mr-history/history/done_intermediate/hadoop\n\n# (hdfs) aggregated app logs\n/yarn/app/hadoop/logs/application_XXX\n\n\n\n\nMapReduce WordCount Job\n\n\n# build jar on the host machine\ncd devops/hadoop/example/map-reduce\n./gradlew clean build\n\ncd devops/hadoop\nvagrant ssh master\n\n# create base directory using hdfs\nhdfs dfs -mkdir -p /user/ubuntu\n\n# create example directory\nhadoop fs -mkdir -p /user/ubuntu/word-count/input\n\n# list directory\nhadoop fs -ls -h -R /\nhadoop fs -ls -h -R /user/ubuntu\n\n# create sample files\necho \"Hello World Bye World\" > file01\necho \"Hello Hadoop Goodbye Hadoop\" > file02\n\n# copy from local to hdfs\nhadoop fs -copyFromLocal file01 /user/ubuntu/word-count/input\nhadoop fs -put file02 /user/ubuntu/word-count/input\n\n# verify copied files\nhadoop fs -ls -h -R /user/ubuntu\nhadoop fs -cat /user/ubuntu/word-count/input/file01\nhadoop fs -cat /user/ubuntu/word-count/input/file02\nhadoop fs -cat /user/ubuntu/word-count/input/*\n\n# run application\nhadoop jar /vagrant/example/map-reduce/build/libs/map-reduce.jar \\\n  /user/ubuntu/word-count/input \\\n  /user/ubuntu/word-count/output\n\n# check output\nhadoop fs -cat /user/ubuntu/word-count/output/part-r-00000\n\n# delete directory to run it again\nhadoop fs -rm -R /user/ubuntu/word-count/output\n\n# run sample job in a different queue\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  wordcount \\\n  -Dmapreduce.job.queuename=root.priority_queue \\\n  /user/ubuntu/word-count/input \\\n  /user/ubuntu/word-count/output\n\n# well known WARN issue\n# https://issues.apache.org/jira/browse/HDFS-10429\n\n\n\n\nBenchmarking MapReduce with TeraSort\n\n\n# generate random data\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  teragen 1000 random-data\n\n# run terasort benchmark\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  terasort random-data sorted-data\n\n# validate data\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  teravalidate sorted-data report\n\n# useful commands\nhadoop fs -ls -h -R .\nhadoop fs -rm -r random-data\nhadoop fs -cat random-data/part-m-00000\nhadoop fs -cat sorted-data/part-r-00000\n\n\n\n\n\n\nSpark\n\n\n\n\nSpark\n is an open-source cluster-computing framework\n\n\n\n\nResources\n\n\n\n\n\n\nDocumentation\n\n\n\n\n\n\nSpark in Action\n (2016) by Petar Ze\u010devi\u0107 and Marko Bona\u0107i (Book)\n\n\n\n\n\n\nBig Data Analysis with Scala and Spark\n (Course)\n\n\n\n\n\n\nHow-to: Tune Your Apache Spark Jobs\n series\n\n\n\n\n\n\nUnderstanding Resource Allocation configurations for a Spark application\n\n\n\n\n\n\nApache Spark: Config Cheatsheet\n\n\n\n\n\n\nMastering Apache Spark\n\n\n\n\n\n\nManaging Spark Partitions with Coalesce and Repartition\n\n\n\n\n\n\nUnderstanding Apache Spark on YARN\n\n\n\n\n\n\n\n\nSpark application on YARN\n\n\n\n\n# start REPL\nspark-shell\npyspark\n\n\n\n\nInteractive Analysis example\n\n\nspark-shell\n# spark shell with yarn\nspark-shell --master yarn --deploy-mode client\n\n# view all configured parameters\nsc.getConf.getAll.foreach(x => println(s\"${x._1}: ${x._2}\"))\n\nval licenceLines = sc.textFile(\"file:/usr/local/spark/LICENSE\")\nval lineCount = licenceLines.count\nval isBsd = (line: String) => line.contains(\"BSD\")\nval bsdLines = licenceLines.filter(isBsd)\nbsdLines.count\nbsdLines.foreach(println)\n\n\n\n\nSpark Job examples\n\n\nExample local\n\n\n# run SparkPi example\nspark-submit \\\n  --class org.apache.spark.examples.SparkPi \\\n  --master local[*] \\\n  $SPARK_HOME/examples/jars/spark-examples_*.jar 10\n\n# GitHub event documentation\n# https://developer.github.com/v3/activity/events/types\n\n# build jar on the host machine\ncd devops/hadoop/example/spark\nsbt clean package\n\ncd devops/hadoop\nvagrant ssh master\n\n# sample dataset\nmkdir -p github-archive && \\\n  cd $_ && \\\n  wget http://data.githubarchive.org/2018-01-01-{0..10}.json.gz && \\\n  gunzip -k *\n# sample line\nhead -n 1 2018-01-01-0.json | jq '.'\n\n# run local job\nspark-submit \\\n  --class \"com.github.niqdev.App\" \\\n  --master local[*] \\\n  /vagrant/example/spark/target/scala-2.11/spark-github_2.11-0.1.0-SNAPSHOT.jar\n\n\n\n\nExample cluster\n\n\n# run job in YARN cluster-deploy mode\nspark-submit \\\n  --class org.apache.spark.examples.SparkPi \\\n  --master yarn \\\n  --deploy-mode cluster \\\n  --driver-memory 2g \\\n  --executor-memory 1g \\\n  --executor-cores 3 \\\n  --queue default \\\n  $SPARK_HOME/examples/jars/spark-examples*.jar \\\n  10\n\n# --conf \"spark.yarn.jars=hdfs://namenode.local:9000/user/spark/share/lib/*.jar\"\n\n\n\n\n\n\nZeppelin\n\n\n\n\nZeppelin\n is a web-based notebook that enables data-driven, interactive data analytics and collaborative documents with SQL, Scala and more\n\n\n\n\nResources\n\n\n\n\nDocumentation\n\n\n\n\nSetup\n\n\nInstall and start Zeppelin\n\n\n# access master node\nvagrant ssh master\n\n# login as root\nsudo su -\n\n# install and init\n/vagrant/script/setup_zeppelin.sh\n\n# start manually (first time only)\nsu --login hadoop /vagrant/script/bootstrap.sh zeppelin\n\n\n\n\nExamples\n\n\n\n\nLearning Spark SQL with Zeppelin\n\n\n\n\n# markdown interpreter\n%md\nhello\n\n# shell interpreter\n%sh\nhadoop fs -ls -h -R /\n\n\n\n\nCluster issue: verify to have enough memory with \nfree -m\n e.g. \nError: Cannot allocate memory\n\n\n\n\nOozie\n\n\n\n\nOozie\n is a workflow scheduler system to manage Hadoop jobs\n\n\n\n\nResources\n\n\n\n\nDocumentation\n\n\n\n\nSetup\n\n\nOptional PostgreSQL configuration\n - By default Oozie is configured to use Embedded Derby\n\n\n# access master node\nvagrant ssh master\n\n# install docker\ncurl -fsSL get.docker.com -o get-docker.sh && \\\n  chmod u+x $_ && \\\n  ./$_ && \\\n  sudo usermod -aG docker hadoop\n\n# logout and login again to verify docker installation\nexit\nvagrant ssh master\nwhoami # hadoop\ndocker ps -a\n\n# uncomment PostgreSQL configurations\nvim devops/hadoop/file/oozie/config/oozie-site.xml # from host\nvim /vagrant/file/oozie/config/oozie-site.xml # from guest\n\n# start postgres on guest machine \ndocker run \\\n  --detach \\\n  --name oozie-postgres \\\n  -p 5432:5432 \\\n  -e POSTGRES_DB=\"oozie-db\" \\\n  -e POSTGRES_USER=\"postgres\" \\\n  -e POSTGRES_PASSWORD=\"password\" \\\n  postgres\n\n# permission issue\n# https://github.com/docker-library/postgres/issues/116\n# --volume /vol/postgres:/var/lib/postgresql/data\n\n# access container\ndocker exec -it oozie-postgres bash\npsql --username=postgres\n# list all databases\n\\list\n\\connect oozie-db\n# list all tables\n\\dt\n# describe table\n\\d+ wf_jobs\n# list workflow\nselect * from wf_jobs;\n\n\n\n\nInstall and start Oozie\n\n\n# access master node\nvagrant ssh master\n\n# login as root\nsudo su -\n\n# build, install and init\n/vagrant/script/setup_oozie.sh\n\n# start oozie manually (first time only)\nsu --login hadoop /vagrant/script/bootstrap.sh oozie\n\n\n\n\nIt might take a while to build the sources\n\n\nUseful paths\n\n\n# data and logs\ndevops/hadoop/.data/master/oozie # host\n/vol/oozie # guest\n\n# (guest) config\n/usr/local/oozie/conf\n\n# (hdfs) examples\n/user/hadoop/examples\n\n\n\n\nExamples\n\n\nRun bundled examples within distribution\n\n\n# examples path\n.data/master/oozie/examples # host\n/vol/oozie/examples # guest\n\n# access master node as hadoop user\nvagrant ssh master\n\nexport OOZIE_EXAMPLE_PATH=/vol/oozie/examples\nexport OOZIE_HDFS_PATH=/user/$(whoami)/examples\n\n# open map-reduce job.properties\nvim $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties\n\n# edit the following properties\nnameNode=hdfs://namenode.local:9000 # fs.defaultFS @ core-site.xml\njobTracker=resource-manager.local:8032 # yarn.resourcemanager.address @ yarn-site.xml\nqueueName=priority_queue # or default @ fair-scheduler.xml\n\n# upload all the examples\nhadoop fs -put $OOZIE_EXAMPLE_PATH $OOZIE_HDFS_PATH\n\n# verify uploaded files\nhadoop fs -ls -h -R /user/$(whoami)\n\n# run the map-reduce workflow example\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -config $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties \\\n  -run\n\n# verify status\noozie job -oozie http://oozie.local:11000/oozie -info WORKFLOW_ID\n\n# verify result\nhadoop fs -cat $OOZIE_HDFS_PATH/output-data/map-reduce/part-00000\n\n# remove all the examples\nhadoop fs -rm -R $OOZIE_HDFS_PATH\n\n\n\n\nUseful commands\n\n\n\n\nWorkflow requires \noozie.wf.application.path\n property\n\n\nCoordinator requires \noozie.coord.application.path\n property\n\n\n\n\n# verify oozie status\noozie admin \\\n  -oozie http://oozie.local:11000/oozie \\\n  -status\n\n# verify workflow or coordinator status\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -info JOB_ID \\\n  -verbose\n\n# poll workflow or coordinator status\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -poll JOB_ID \\\n  -interval 10 \\\n  -timeout 60 \\\n  -verbose\n\n# find running coordinator\noozie jobs \\\n  -oozie http://oozie.local:11000/oozie/ \\\n  -filter status=RUNNING \\\n  -jobtype coordinator\n\n# suspend|resume|kill coordinator\noozie job \\\n  -oozie http://oozie.local:11000/oozie/ \\\n  [-suspend|-resume|-kill] \\\n  XXX-C\n\n# re-run coordinator's workflow (action)\noozie job \\\n  -oozie http://oozie.local:11000/oozie/ \\\n  -rerun XXX-C \\\n  -action 1,2,3,N\n\n# kill workflow\noozie job \\\n  -oozie http://oozie.local:11000/oozie/ \\\n  -kill \\\n  XXX-W\n\n# re-run all workflow's actions\noozie job \\\n  -oozie http://oozie.local:11000/oozie/ \\\n  -rerun \\\n  XXX-W \\\n  -Doozie.wf.rerun.failnodes=false",
            "title": "Hadoop"
        },
        {
            "location": "/hadoop/#hadoop",
            "text": "The following guide explains how to provision a Multi Node Hadoop Cluster locally and play with it. Checkout the  Vagrantfile  and the Vagrant  guide  for more details.  Resources    Documentation    Hadoop: The Definitive Guide  (2015)(4th) by Tom White (Book)    The Hadoop Ecosystem Table    Hadoop Internals",
            "title": "Hadoop"
        },
        {
            "location": "/hadoop/#setup",
            "text": "Requirements   Vagrant  VirtualBox   Directory structure  tree -a hadoop/\nhadoop/\n\u251c\u2500\u2500 .data # mounted volume\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop_rsa\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop_rsa.pub\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 master\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 mapred\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 yarn\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 namenode\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 secondary\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 spark\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 zeppelin\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 notebook\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-1\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u251c\u2500\u2500 datanode\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0     \u2514\u2500\u2500 log\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u251c\u2500\u2500 mapred\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0      \u00a0\u00a0 \u2514\u2500\u2500 yarn\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-2\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 node-3\n\u251c\u2500\u2500 example\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 map-reduce\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 spark\n\u251c\u2500\u2500 file\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hadoop\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 core-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fair-scheduler.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 hdfs-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 mapred-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 masters\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 slaves\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 yarn-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-hadoop.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 hosts\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 motd\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 oozie-env.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 oozie-site.xml\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-oozie.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 spark\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 log4j.properties\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 spark-env.sh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 profile-spark.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 ssh\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 config\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 zeppelin\n\u2502\u00a0\u00a0     \u251c\u2500\u2500 config\n\u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 zeppelin-env.sh\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 profile-zeppelin.sh\n\u251c\u2500\u2500 script\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 bootstrap.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_hadoop.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_oozie.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_spark.sh\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 setup_ubuntu.sh\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 setup_zeppelin.sh\n\u251c\u2500\u2500 Vagrantfile\n\u2514\u2500\u2500 vagrant_hadoop.sh  Import the script  source vagrant_hadoop.sh  Create and start a Multi Node Hadoop Cluster  hadoop-start  The first time it might take a while  Access the cluster via ssh, check also the  /etc/hosts  file  vagrant ssh master\nssh hadoop@172.16.0.10 -i .data/hadoop_rsa\n\n# 3 nodes\nvagrant ssh node-1\nssh hadoop@172.16.0.101 -i .data/hadoop_rsa  Destroy the cluster  hadoop-destroy  For convenience add to the host machine  cat hadoop/file/hosts | sudo tee --append /etc/hosts  Web UI links   NameNode:  http://namenode.local:50070  NameNode metrics:  http://namenode.local:50070/jmx  ResourceManager:  http://resource-manager.local:8088  Log Level:  http://resource-manager.local:8088/logLevel  Web Application Proxy Server:  http://web-proxy.local:8100/proxy/application_XXX_0000  MapReduce Job History Server:  http://history.local:19888  DataNode/NodeManager (1):  http://node-1.local:8042/node  DataNode/NodeManager (2):  http://node-2.local:8042/node  DataNode/NodeManager (3):  http://node-3.local:8042/node  Spark:  http://spark.local:4040  Spark History Server:  http://spark-history.local:18080  Zeppelin (*):  http://zeppelin.local:8080  Oozie (*):  http://oozie.local:11000   (*) Not installed by default",
            "title": "Setup"
        },
        {
            "location": "/hadoop/#hdfs-and-mapreduce",
            "text": "HDFS  is a distributed file system that provides high-throughput access to application data  YARN  is a framework for job scheduling and cluster resource management  MapReduce  is a YARN-based system for parallel processing of large data sets   Documentation   Hadoop v2.7.6  Untangling Apache Hadoop YARN  series",
            "title": "HDFS and MapReduce"
        },
        {
            "location": "/hadoop/#admin",
            "text": "HDFS cli  # help\nhdfs\n\n# filesystem statistics\nhdfs dfsadmin -report\n\n# filesystem check\nhdfs fsck /  YARN cli  # help\nyarn\n\n# list yarn applications\nyarn application -list\n\n# list nodes\nyarn node -list\n\n# view application logs\nyarn logs -applicationId APPLICATION_ID\n\n# kill yarn application\nyarn application -kill APPLICATION_ID  Useful paths  # data and logs\ndevops/hadoop/.data/master/hadoop # host\n/vol/hadoop # guest\n\n# (guest) config\n/usr/local/hadoop/etc/hadoop\n\n# (hdfs) map-reduce history\n/mr-history/history/done_intermediate/hadoop\n\n# (hdfs) aggregated app logs\n/yarn/app/hadoop/logs/application_XXX",
            "title": "Admin"
        },
        {
            "location": "/hadoop/#mapreduce-wordcount-job",
            "text": "# build jar on the host machine\ncd devops/hadoop/example/map-reduce\n./gradlew clean build\n\ncd devops/hadoop\nvagrant ssh master\n\n# create base directory using hdfs\nhdfs dfs -mkdir -p /user/ubuntu\n\n# create example directory\nhadoop fs -mkdir -p /user/ubuntu/word-count/input\n\n# list directory\nhadoop fs -ls -h -R /\nhadoop fs -ls -h -R /user/ubuntu\n\n# create sample files\necho \"Hello World Bye World\" > file01\necho \"Hello Hadoop Goodbye Hadoop\" > file02\n\n# copy from local to hdfs\nhadoop fs -copyFromLocal file01 /user/ubuntu/word-count/input\nhadoop fs -put file02 /user/ubuntu/word-count/input\n\n# verify copied files\nhadoop fs -ls -h -R /user/ubuntu\nhadoop fs -cat /user/ubuntu/word-count/input/file01\nhadoop fs -cat /user/ubuntu/word-count/input/file02\nhadoop fs -cat /user/ubuntu/word-count/input/*\n\n# run application\nhadoop jar /vagrant/example/map-reduce/build/libs/map-reduce.jar \\\n  /user/ubuntu/word-count/input \\\n  /user/ubuntu/word-count/output\n\n# check output\nhadoop fs -cat /user/ubuntu/word-count/output/part-r-00000\n\n# delete directory to run it again\nhadoop fs -rm -R /user/ubuntu/word-count/output\n\n# run sample job in a different queue\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  wordcount \\\n  -Dmapreduce.job.queuename=root.priority_queue \\\n  /user/ubuntu/word-count/input \\\n  /user/ubuntu/word-count/output\n\n# well known WARN issue\n# https://issues.apache.org/jira/browse/HDFS-10429",
            "title": "MapReduce WordCount Job"
        },
        {
            "location": "/hadoop/#benchmarking-mapreduce-with-terasort",
            "text": "# generate random data\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  teragen 1000 random-data\n\n# run terasort benchmark\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  terasort random-data sorted-data\n\n# validate data\nhadoop jar \\\n  $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-*.jar \\\n  teravalidate sorted-data report\n\n# useful commands\nhadoop fs -ls -h -R .\nhadoop fs -rm -r random-data\nhadoop fs -cat random-data/part-m-00000\nhadoop fs -cat sorted-data/part-r-00000",
            "title": "Benchmarking MapReduce with TeraSort"
        },
        {
            "location": "/hadoop/#spark",
            "text": "Spark  is an open-source cluster-computing framework   Resources    Documentation    Spark in Action  (2016) by Petar Ze\u010devi\u0107 and Marko Bona\u0107i (Book)    Big Data Analysis with Scala and Spark  (Course)    How-to: Tune Your Apache Spark Jobs  series    Understanding Resource Allocation configurations for a Spark application    Apache Spark: Config Cheatsheet    Mastering Apache Spark    Managing Spark Partitions with Coalesce and Repartition    Understanding Apache Spark on YARN     Spark application on YARN   # start REPL\nspark-shell\npyspark",
            "title": "Spark"
        },
        {
            "location": "/hadoop/#interactive-analysis-example",
            "text": "spark-shell\n# spark shell with yarn\nspark-shell --master yarn --deploy-mode client\n\n# view all configured parameters\nsc.getConf.getAll.foreach(x => println(s\"${x._1}: ${x._2}\"))\n\nval licenceLines = sc.textFile(\"file:/usr/local/spark/LICENSE\")\nval lineCount = licenceLines.count\nval isBsd = (line: String) => line.contains(\"BSD\")\nval bsdLines = licenceLines.filter(isBsd)\nbsdLines.count\nbsdLines.foreach(println)",
            "title": "Interactive Analysis example"
        },
        {
            "location": "/hadoop/#spark-job-examples",
            "text": "Example local  # run SparkPi example\nspark-submit \\\n  --class org.apache.spark.examples.SparkPi \\\n  --master local[*] \\\n  $SPARK_HOME/examples/jars/spark-examples_*.jar 10\n\n# GitHub event documentation\n# https://developer.github.com/v3/activity/events/types\n\n# build jar on the host machine\ncd devops/hadoop/example/spark\nsbt clean package\n\ncd devops/hadoop\nvagrant ssh master\n\n# sample dataset\nmkdir -p github-archive && \\\n  cd $_ && \\\n  wget http://data.githubarchive.org/2018-01-01-{0..10}.json.gz && \\\n  gunzip -k *\n# sample line\nhead -n 1 2018-01-01-0.json | jq '.'\n\n# run local job\nspark-submit \\\n  --class \"com.github.niqdev.App\" \\\n  --master local[*] \\\n  /vagrant/example/spark/target/scala-2.11/spark-github_2.11-0.1.0-SNAPSHOT.jar  Example cluster  # run job in YARN cluster-deploy mode\nspark-submit \\\n  --class org.apache.spark.examples.SparkPi \\\n  --master yarn \\\n  --deploy-mode cluster \\\n  --driver-memory 2g \\\n  --executor-memory 1g \\\n  --executor-cores 3 \\\n  --queue default \\\n  $SPARK_HOME/examples/jars/spark-examples*.jar \\\n  10\n\n# --conf \"spark.yarn.jars=hdfs://namenode.local:9000/user/spark/share/lib/*.jar\"",
            "title": "Spark Job examples"
        },
        {
            "location": "/hadoop/#zeppelin",
            "text": "Zeppelin  is a web-based notebook that enables data-driven, interactive data analytics and collaborative documents with SQL, Scala and more   Resources   Documentation",
            "title": "Zeppelin"
        },
        {
            "location": "/hadoop/#setup_1",
            "text": "Install and start Zeppelin  # access master node\nvagrant ssh master\n\n# login as root\nsudo su -\n\n# install and init\n/vagrant/script/setup_zeppelin.sh\n\n# start manually (first time only)\nsu --login hadoop /vagrant/script/bootstrap.sh zeppelin",
            "title": "Setup"
        },
        {
            "location": "/hadoop/#examples",
            "text": "Learning Spark SQL with Zeppelin   # markdown interpreter\n%md\nhello\n\n# shell interpreter\n%sh\nhadoop fs -ls -h -R /  Cluster issue: verify to have enough memory with  free -m  e.g.  Error: Cannot allocate memory",
            "title": "Examples"
        },
        {
            "location": "/hadoop/#oozie",
            "text": "Oozie  is a workflow scheduler system to manage Hadoop jobs   Resources   Documentation",
            "title": "Oozie"
        },
        {
            "location": "/hadoop/#setup_2",
            "text": "Optional PostgreSQL configuration  - By default Oozie is configured to use Embedded Derby  # access master node\nvagrant ssh master\n\n# install docker\ncurl -fsSL get.docker.com -o get-docker.sh && \\\n  chmod u+x $_ && \\\n  ./$_ && \\\n  sudo usermod -aG docker hadoop\n\n# logout and login again to verify docker installation\nexit\nvagrant ssh master\nwhoami # hadoop\ndocker ps -a\n\n# uncomment PostgreSQL configurations\nvim devops/hadoop/file/oozie/config/oozie-site.xml # from host\nvim /vagrant/file/oozie/config/oozie-site.xml # from guest\n\n# start postgres on guest machine \ndocker run \\\n  --detach \\\n  --name oozie-postgres \\\n  -p 5432:5432 \\\n  -e POSTGRES_DB=\"oozie-db\" \\\n  -e POSTGRES_USER=\"postgres\" \\\n  -e POSTGRES_PASSWORD=\"password\" \\\n  postgres\n\n# permission issue\n# https://github.com/docker-library/postgres/issues/116\n# --volume /vol/postgres:/var/lib/postgresql/data\n\n# access container\ndocker exec -it oozie-postgres bash\npsql --username=postgres\n# list all databases\n\\list\n\\connect oozie-db\n# list all tables\n\\dt\n# describe table\n\\d+ wf_jobs\n# list workflow\nselect * from wf_jobs;  Install and start Oozie  # access master node\nvagrant ssh master\n\n# login as root\nsudo su -\n\n# build, install and init\n/vagrant/script/setup_oozie.sh\n\n# start oozie manually (first time only)\nsu --login hadoop /vagrant/script/bootstrap.sh oozie  It might take a while to build the sources  Useful paths  # data and logs\ndevops/hadoop/.data/master/oozie # host\n/vol/oozie # guest\n\n# (guest) config\n/usr/local/oozie/conf\n\n# (hdfs) examples\n/user/hadoop/examples",
            "title": "Setup"
        },
        {
            "location": "/hadoop/#examples_1",
            "text": "Run bundled examples within distribution  # examples path\n.data/master/oozie/examples # host\n/vol/oozie/examples # guest\n\n# access master node as hadoop user\nvagrant ssh master\n\nexport OOZIE_EXAMPLE_PATH=/vol/oozie/examples\nexport OOZIE_HDFS_PATH=/user/$(whoami)/examples\n\n# open map-reduce job.properties\nvim $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties\n\n# edit the following properties\nnameNode=hdfs://namenode.local:9000 # fs.defaultFS @ core-site.xml\njobTracker=resource-manager.local:8032 # yarn.resourcemanager.address @ yarn-site.xml\nqueueName=priority_queue # or default @ fair-scheduler.xml\n\n# upload all the examples\nhadoop fs -put $OOZIE_EXAMPLE_PATH $OOZIE_HDFS_PATH\n\n# verify uploaded files\nhadoop fs -ls -h -R /user/$(whoami)\n\n# run the map-reduce workflow example\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -config $OOZIE_EXAMPLE_PATH/apps/map-reduce/job.properties \\\n  -run\n\n# verify status\noozie job -oozie http://oozie.local:11000/oozie -info WORKFLOW_ID\n\n# verify result\nhadoop fs -cat $OOZIE_HDFS_PATH/output-data/map-reduce/part-00000\n\n# remove all the examples\nhadoop fs -rm -R $OOZIE_HDFS_PATH",
            "title": "Examples"
        },
        {
            "location": "/hadoop/#useful-commands",
            "text": "Workflow requires  oozie.wf.application.path  property  Coordinator requires  oozie.coord.application.path  property   # verify oozie status\noozie admin \\\n  -oozie http://oozie.local:11000/oozie \\\n  -status\n\n# verify workflow or coordinator status\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -info JOB_ID \\\n  -verbose\n\n# poll workflow or coordinator status\noozie job \\\n  -oozie http://oozie.local:11000/oozie \\\n  -poll JOB_ID \\\n  -interval 10 \\\n  -timeout 60 \\\n  -verbose\n\n# find running coordinator\noozie jobs \\\n  -oozie http://oozie.local:11000/oozie/ \\\n  -filter status=RUNNING \\\n  -jobtype coordinator\n\n# suspend|resume|kill coordinator\noozie job \\\n  -oozie http://oozie.local:11000/oozie/ \\\n  [-suspend|-resume|-kill] \\\n  XXX-C\n\n# re-run coordinator's workflow (action)\noozie job \\\n  -oozie http://oozie.local:11000/oozie/ \\\n  -rerun XXX-C \\\n  -action 1,2,3,N\n\n# kill workflow\noozie job \\\n  -oozie http://oozie.local:11000/oozie/ \\\n  -kill \\\n  XXX-W\n\n# re-run all workflow's actions\noozie job \\\n  -oozie http://oozie.local:11000/oozie/ \\\n  -rerun \\\n  XXX-W \\\n  -Doozie.wf.rerun.failnodes=false",
            "title": "Useful commands"
        },
        {
            "location": "/kubernetes/",
            "text": "Kubernetes\n\n\n\n\nKubernetes\n is a system for automating deployment, scaling, and management of containerized applications\n\n\n\n\nResources\n\n\n\n\n\n\nDocumentation\n\n\n\n\n\n\nKubernetes in Action\n (2017) by Marko Luk\u0161a (Book)\n\n\n\n\n\n\nKubernetes by Example\n\n\n\n\n\n\nKubernetes comic\n\n\n\n\n\n\nRunning akka-cluster on Kubernetes\n\n\n\n\n\n\nKubernetes: The Surprisingly Affordable Platform for Personal Projects\n\n\n\n\n\n\nKubernetes from scratch to AWS with Terraform and Ansible\n\n\n\n\n\n\nSetup\n\n\nRequirements\n\n\n\n\nMinikube\n\n\nVirtualBox\n\n\n\n\nLocal cluster\n\n\n# verify installation\nminikube version\n\n# lifecycle\nminikube start --vm-driver=virtualbox\nminikube stop\nminikube delete\n\n# dashboard\nexport NO_PROXY=localhost,127.0.0.1,$(minikube ip)\nminikube dashboard\n\n# access\nminikube ssh\ndocker ps -a\n\n# reuse the minikube's built-in docker daemon\neval $(minikube docker-env)\n\n\n\n\nBasic\n\n\n# verify installation\nkubectl version\n\n# cluster info\nkubectl cluster-info\nkubectl get nodes\nkubectl describe nodes\nkubectl config view\n\n# namespace\nkubectl create namespace local\nkubectl get namespaces\nkubectl config set-context $(kubectl config current-context) --namespace=local\nkubectl config view | grep namespace\nkubectl delete namespace local\n\n\n\n\nSimple deployment\n\n\n# deploy demo app\nkubectl run kubernetes-bootcamp \\\n  --image=gcr.io/google-samples/kubernetes-bootcamp:v1 \\\n  --port=8080 \\\n  --labels='app=kubernetes-bootcamp'\n\n# update app\nkubectl set image deployments/kubernetes-bootcamp \\\n  kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2\n\n# verify update\nkubectl rollout status deployments/kubernetes-bootcamp\nkubectl rollout history deployments/kubernetes-bootcamp\n\n# undo latest deployment\nkubectl rollout undo deployments/kubernetes-bootcamp\n\n# list deployments\nkubectl get deployments\n\n# list containers inside pods\nkubectl describe pods\n\n# list pods\nkubectl get pods\n\n# filter with equality-based labels\nkubectl get pods -l app=kubernetes-bootcamp\n\n# filter with set-based labels\nkubectl get pods -l 'app in (kubernetes-bootcamp)'\n\n\n\n\nPod and Container\n\n\n# proxy cluster (open in 2nd terminal)\nkubectl proxy\n\n# pod name\nexport POD_NAME=$(kubectl get pods -l app=kubernetes-bootcamp -o go-template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}')\necho POD_NAME=$POD_NAME\n\n# verify proxy\nhttp :8001/version\nhttp :8001/api/v1/proxy/namespaces/default/pods/$POD_NAME/\n\n# view logs\nkubectl logs $POD_NAME\n\n# execute command on container\nkubectl exec $POD_NAME printenv\nkubectl exec $POD_NAME ls -- -la\n\n# access container\nkubectl exec -it $POD_NAME bash\n\n# verify label\nkubectl describe pods $POD_NAME\n\n\n\n\nService\n\n\n# list services\nkubectl get services\n\n# create service\nkubectl expose deployment/kubernetes-bootcamp \\\n  --type=\"NodePort\" \\\n  --port 8080\n\n# service info\nkubectl describe services/kubernetes-bootcamp\n\n# expose service\nexport NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}')\necho NODE_PORT=$NODE_PORT\n\n# verify service\ncurl $(minikube ip):$NODE_PORT\n\n# add 4 replicas\nkubectl scale deployments/kubernetes-bootcamp --replicas=4\n\n# info\nkubectl get pod,deployment,service\nkubectl get pods -o wide\nkubectl describe deployments/kubernetes-bootcamp\n\n# cleanup\nkubectl delete deployment,service kubernetes-bootcamp",
            "title": "Kubernetes"
        },
        {
            "location": "/kubernetes/#kubernetes",
            "text": "Kubernetes  is a system for automating deployment, scaling, and management of containerized applications   Resources    Documentation    Kubernetes in Action  (2017) by Marko Luk\u0161a (Book)    Kubernetes by Example    Kubernetes comic    Running akka-cluster on Kubernetes    Kubernetes: The Surprisingly Affordable Platform for Personal Projects    Kubernetes from scratch to AWS with Terraform and Ansible",
            "title": "Kubernetes"
        },
        {
            "location": "/kubernetes/#setup",
            "text": "Requirements   Minikube  VirtualBox   Local cluster  # verify installation\nminikube version\n\n# lifecycle\nminikube start --vm-driver=virtualbox\nminikube stop\nminikube delete\n\n# dashboard\nexport NO_PROXY=localhost,127.0.0.1,$(minikube ip)\nminikube dashboard\n\n# access\nminikube ssh\ndocker ps -a\n\n# reuse the minikube's built-in docker daemon\neval $(minikube docker-env)  Basic  # verify installation\nkubectl version\n\n# cluster info\nkubectl cluster-info\nkubectl get nodes\nkubectl describe nodes\nkubectl config view\n\n# namespace\nkubectl create namespace local\nkubectl get namespaces\nkubectl config set-context $(kubectl config current-context) --namespace=local\nkubectl config view | grep namespace\nkubectl delete namespace local  Simple deployment  # deploy demo app\nkubectl run kubernetes-bootcamp \\\n  --image=gcr.io/google-samples/kubernetes-bootcamp:v1 \\\n  --port=8080 \\\n  --labels='app=kubernetes-bootcamp'\n\n# update app\nkubectl set image deployments/kubernetes-bootcamp \\\n  kubernetes-bootcamp=jocatalin/kubernetes-bootcamp:v2\n\n# verify update\nkubectl rollout status deployments/kubernetes-bootcamp\nkubectl rollout history deployments/kubernetes-bootcamp\n\n# undo latest deployment\nkubectl rollout undo deployments/kubernetes-bootcamp\n\n# list deployments\nkubectl get deployments\n\n# list containers inside pods\nkubectl describe pods\n\n# list pods\nkubectl get pods\n\n# filter with equality-based labels\nkubectl get pods -l app=kubernetes-bootcamp\n\n# filter with set-based labels\nkubectl get pods -l 'app in (kubernetes-bootcamp)'  Pod and Container  # proxy cluster (open in 2nd terminal)\nkubectl proxy\n\n# pod name\nexport POD_NAME=$(kubectl get pods -l app=kubernetes-bootcamp -o go-template --template '{{range .items}}{{.metadata.name}}{{\"\\n\"}}{{end}}')\necho POD_NAME=$POD_NAME\n\n# verify proxy\nhttp :8001/version\nhttp :8001/api/v1/proxy/namespaces/default/pods/$POD_NAME/\n\n# view logs\nkubectl logs $POD_NAME\n\n# execute command on container\nkubectl exec $POD_NAME printenv\nkubectl exec $POD_NAME ls -- -la\n\n# access container\nkubectl exec -it $POD_NAME bash\n\n# verify label\nkubectl describe pods $POD_NAME  Service  # list services\nkubectl get services\n\n# create service\nkubectl expose deployment/kubernetes-bootcamp \\\n  --type=\"NodePort\" \\\n  --port 8080\n\n# service info\nkubectl describe services/kubernetes-bootcamp\n\n# expose service\nexport NODE_PORT=$(kubectl get services/kubernetes-bootcamp -o go-template='{{(index .spec.ports 0).nodePort}}')\necho NODE_PORT=$NODE_PORT\n\n# verify service\ncurl $(minikube ip):$NODE_PORT\n\n# add 4 replicas\nkubectl scale deployments/kubernetes-bootcamp --replicas=4\n\n# info\nkubectl get pod,deployment,service\nkubectl get pods -o wide\nkubectl describe deployments/kubernetes-bootcamp\n\n# cleanup\nkubectl delete deployment,service kubernetes-bootcamp",
            "title": "Setup"
        },
        {
            "location": "/jvm/",
            "text": "JVM\n\n\nJava Memory Management\n\n\nResources\n\n\n\n\nJava Language and Virtual Machine Specifications\n\n\nThe Java HotSpot Performance Engine Architecture\n\n\nMemory Management in the Java HotSpot Virtual Machine\n\n\nJVM Architecture 101\n\n\nInside the Java Virtual Machine\n\n\nJVM Internals\n\n\nJava Code to Byte Code\n\n\nUnderstanding JVM Internals\n\n\nUnderstanding Java Garbage Collection\n\n\nJava Decompiler\n\n\nJava Concurrency in Practice\n (Book)\n\n\n\n\n\n\nStack\n\n\nThe Stack is a First-In-Last-Out data structure managed by the JVM i.e. push to the top, pull or pop from the top. Every thread has its own Stack and data can be seen only by that thread\n\n\nEach time you call a function, Java pushes the local variables for that function onto the Stack. A copy of the value is passed to the methods. When the method returns, all the data are popped or pulled from the Stack\n\n\nWhen you reach a closing curly bracket (not only after a return) any local variables declared inside the block you are leaving is popped from the Stack and destroyed, this is how scope works\n\n\nHeap\n\n\nThe Heap allows to store data that has a longer lifetime than a single code block or function e.g. objects that need to be shared across multiple methods\n\n\nAll the memory in the JVM is mainly Heap, is massive compared with the Stack and there is only one Heap shared across all the threads and a number of Stacks (each thread has its own Stack)\n\n\nVariables\n\n\nHow variable are store in Java:\n\n\n\n\nobjects are stored physically on the Heap\n\n\nvariables are reference to objects\n\n\nlocal variables are stored on the Stack\n\n\nprimitive variables resides entirely on the Stack\n\n\n\n\nint age = 30\nString name = \"niqdev\"\n\nStack          Heap\n[  name* ] --> [ \"niqdev\" ]\n[  age   ]\n\n\n\n\nIn Java, variable can \nonly\n be passed by values i.e. a new variable is added on the Stack\n\n\nPassing by reference is \nnot\n possible, but don't confuse that when objects are passed into methods, \nthe \nreference\n of the object is passed \nby value\n\n\nfinal\n keyword means that the value can only be assigned once (can be assigned later), but we can change the value referenced by the object. Lack of const correctness in Java: \nconst\n unfortunately is a reserved keyword and cannot be used, but allows to freeze the whole state of the object internally: \nmutable states are bad!\n\n\nExample\n\n\nfinal Customer c = new Customer(\"a\")\n// or\nfinal Customer c\nc = new Customer(\"a\")\n\n// this is NOT allowed - compiler error\nc = new Customer(\"b\")\n// but this yes\nc.setName(\"b\")\n\n\n\n\nString\n\n\nAs a general rule of thumb, all objects are stored on the Heap and only references are stored in the Stack. In reality the JVM for optimization maybe store some objects also in the Stack, but this is not visibile. Strings are immutable and if \"short\" are stored in a pool on the Heap to be reused.\n\n\nWhen you create a new string with quotes e.g. \n\"hello\"\n, the JVM creates and retrieve the string from a constant pool. To create a new object every time use the \nnew String(\"hello\")\n. To force a lookup in the pool use \nintern()\n. Use \n==\n to compare the reference (address in memory)\n\n\nString s1 = \"hello\";\nString s2 = \"hello\";\nString s3 = new String(\"hello\");\nString s4 = new String(\"hello\").intern();\n\n// true - 2 strings created using quotes refer to the same object\nSystem.out.println(s1 == s2);\n// false - if you create a string using new operator\n// it's not part of the constant pool, so objects are different\nSystem.out.println(s1 == s3);\n// true - if you call intern() method Java adds current string to the string pool\n// and 2 string become the same object\nSystem.out.println(s1 == s4);\n\n\n\n\nGarbage collection\n\n\nGarbage collection\n (invented in lisp around 1959) removes object no more referenced on the Heap from the Stack. Most of the objects don't live for long, if an object survives it is likely to live forever. \nMark and sweep\n is the algorithm used:\n\n\n\n\ninstead of look for all the objects that must be removed, it looks for the object that need to be retained\n\n\nall the threads in the application are paused (stop the world event)\n\n\nfollow all the references from the Stack and mark it as alive\n\n\nfull scan on all the Heap and wipe unmarked references\n\n\nreorder contiguos memory in order to avoid fragmentation\n\n\nit's faster if more objects need to be cleaned because it looks only for what need to be retained\n\n\n\n\nGenerational garbage collection\n is a way to organize the Heap into 2 sections to try to avoid freeze the application while garbage collecting the whole Heap:\n\n\n\n\nyoung generation\n is small, so it requires only few fraction of seconds to be scanned\n\n\nold generation\n also called \nTenured\n, isn't scanned usually, only if needed i.e. when it's full and requires few seconds\n\n\nyoung generation is divided in \nEden\n, \nSurvivor0\n and \nSurvivor1\n\n\nnew objects are created in the Eden\n\n\nwhen Eden gets full, objects are moved in the Survivor space and moved amongs the two alternatively to be compacted\n\n\nmemory is reserved in the Heap for S0 and S1 even if not used\n\n\nafter an object survived 8 generations (default), movement and compaction between Survivor GC, then is stored in the old generation\n\n\n\n\nVM can change the number of generations based on the amount of memory available\n\n\n\n\n\n\nclass variables (static variables) are stored as part of the class object associated with that class and stored in the Permanent Generation (PermGen) prior Java 8 or in the MetaSpace\n\n\n\n\n\n\n\n\nRun \njvisualvm\n and add \nVisual GC\n plugin\n\n\nAny object on the Heap which cannot be reached through a reference from the Stack is \neligible for garbage collection\n. \nMemory leak\n are objects that are not free on the Heap and continue to consume memory after a program finish. Memory leaks are difficult to find and the JVM try to avoid them running the garbage collector. \nSoft leak\n happens when an object is referenced on the Stack even thought it will never be used again\n\n\nYou can not clear memory, with \nRuntime.getRuntime().gc()\n or \nSystem.gc()\n you can only suggest JVM to run garbage collection, but there is no guarentee. In general, you should never invoke \ngc()\n directly. While it is running the application is temporarily suspended and it will pause all the threads. \nfinalize()\n is invoked when an object is garbage collected, but there is absolutely no guarentee if and when it will happen.\nIs useful to check for example memory leak, as warning, if some resources were not closed properly\n\n\nPermGen and Metaspace\n\n\nPermanent Generation (Heap memory) since Java 6 contains objects that will never garbage collected:\n\n\n\n\nstring pool is in PermGen\n\n\nclass metadata are stored in PermGen\n\n\n\n\nIf the PermGen run out of space the only solution is to increase the size of memory, otherwise the app will crash. In Java 7 String Pool was moved in the old memory and therefore string can be garbage collected now. In Java 8 MetaSpace replaced PermGen as separeted memory allocated, which is not part of the Heap anymore and is the total available memory\n\n\nTuning\n\n\n# set the maximum heap size to 512 MB\n-Xmx512m\n# set the starting heap size to 150 MB\n-Xms150m\n# set the size of the PermGen to 256 MB (PermGen was removed in Java 8)\n-XX:MaxPermSize=256M\n# set the size of the young generation to 256 MB\n# young generation by default is 1/3 of the max heap size (suggestion between 1/2-1/4)\n-Xmn256m\n\n\n\n\nOracle JVM has 3 types of garbage collector, it doesn't matter how many threads, the application will be paused anyway:\n\n\n# serial uses a single thread to perform gc\n-XX:+UseSerailGC\n# parallel performs gc on young generation in parallel\n-XX:+UseParallelGC\n# mostly concurrent, closest to real time gc pausing only when marking\n# try to minimize the \"stop of the world\"\n-XX:+UseConcMarkSweepGC\n-XX:+UseG1GC\n\n\n\n\nDebugging\n\n\n# print to console when a garbage collection takes place\n-verbose:gc\n# creates a heap dump file if app crash\n-XX:HeapDumpOnOutOfMemory\n# to find default garbage collector on the machine\n-XX:+PrintCommandLineFlags\n\n\n\n\nParameters are case sensitive\n\n\nWeak and Soft references\n\n\nReferences from the Stack to the Heap\n\n\n\n\nStrong\n references are always marked as alive (default)\n\n\nSoft\n references are eligible for garbage collection only if the JVM run out of memory\n\n\nWeak\n references are always eligible for garbage collection and is up to the JVM to retain it or not\n\n\n\n\nWeakReference<T>\n and \nSoftReference<T>\n are useful for caching scenario, when a reference on the Heap is GC, then the variable in the stack became \nnull\n. In a WeakHashMap for example, the reference from the stack to the map in the Heap is strong, but the references to key/value pair are eligible for GC, in that case both key and value are removed\n\n\n\n\nPerfomance\n\n\nResources\n\n\n\n\nHdrHistogram\n\n\nUnderstanding JIT compiler\n\n\nUnderstanding Java JIT Compilation with JITWatch\n\n\nCoordinated Omission\n\n\nJava Flight Recorder\n\n\nSafepoints in HotSpot JVM\n\n\nJava Mission Control (JMC) and Flight Recorder (JFR)\n\n\nJMH\n and \nsbt-jmh\n\n\nWhat is JMX?\n and \nJolokia\n\n\n\n\nLatency\n describes the amount of time that it takes for an observed process to be completed\n\n\nThroughput\n defines the observed rate at which a process is completed\n\n\nA \nbottleneck\n refers to the slowest part of the system\n\n\nA common mistake is to rely on \naverages\n to measure the performance of a system because it is a lossy summary statistic.\nA \npercentile\n is a measurement indicating the value below which a given percentage of observations in a group of observations fall\n\n\nBenchmarks\n are a black-box kind of measurement. Benchmarks assess a whole system's performance by submitting various kinds of load as input and measuring latency and throughput as system outputs\n\n\nA \nprofiler\n enables white-box testing to help you identify bottlenecks by capturing the execution time and resource consumption of each part of your\nprogram. Most profilers instrument the code under observation, either at compile time or runtime, to inject counters and profiling components. This instrumentation imposes a runtime cost that degrades system throughput and latency\n\n\nThe \ncoordinated omission problem\n happen we you measure the time required to process a command without taking into account the time the command had to wait to be processed\n\n\nJava Flight Recorder (JFR)\n is a tool for collecting, diagnosing, and profiling data about a running Java application. It is integrated into the Java Virtual Machine and causes almost no performance overhead and is able to access data outside of \nJVM safepoints\n. Safepoints are necessary\nto coordinate global JVM activities, including stop-the-world garbage collection\n\n\nJava Mission Control (JMC)\n allows to connect to a running Java application via JMX and capture runtime information from the Flight Recorder (JFR), executing commands via JMX or displaying reports from JFR sessions\n\n\nThe \nJust-In-Time (JIT)\n compiler is a component of the Java Runtime Environment that improves the performance of Java applications at run time. Java programs consists of classes, which contain platform neutral bytecode that can be interpreted by a JVM on many different computer architectures.\nAt run time, the JVM loads the class files, determines the semantics of each individual bytecode, and performs the appropriate computation.\nThe additional processor and memory usage during interpretation means that a Java application performs more slowly than a native application.\nThe JIT compiler helps improve the performance of Java programs by compiling bytecode into native machine code at run time\n\n\nScala\n\n\n# compiles\nscalac miscellaneous/scala-example.scala -d miscellaneous/\n\n# lists compiler phases\nscalac -Xshow-phases\n\n# prints for-comprehensions desugared\nscalac -Xprint:parse miscellaneous/scala-example.scala -d miscellaneous/\n\n# prints all phases\nscalac -Xprint:all miscellaneous/scala-example.scala -d miscellaneous/\n\n# shows bytecode\njavap -v miscellaneous/Example$.class",
            "title": "JVM"
        },
        {
            "location": "/jvm/#jvm",
            "text": "",
            "title": "JVM"
        },
        {
            "location": "/jvm/#java-memory-management",
            "text": "Resources   Java Language and Virtual Machine Specifications  The Java HotSpot Performance Engine Architecture  Memory Management in the Java HotSpot Virtual Machine  JVM Architecture 101  Inside the Java Virtual Machine  JVM Internals  Java Code to Byte Code  Understanding JVM Internals  Understanding Java Garbage Collection  Java Decompiler  Java Concurrency in Practice  (Book)",
            "title": "Java Memory Management"
        },
        {
            "location": "/jvm/#stack",
            "text": "The Stack is a First-In-Last-Out data structure managed by the JVM i.e. push to the top, pull or pop from the top. Every thread has its own Stack and data can be seen only by that thread  Each time you call a function, Java pushes the local variables for that function onto the Stack. A copy of the value is passed to the methods. When the method returns, all the data are popped or pulled from the Stack  When you reach a closing curly bracket (not only after a return) any local variables declared inside the block you are leaving is popped from the Stack and destroyed, this is how scope works",
            "title": "Stack"
        },
        {
            "location": "/jvm/#heap",
            "text": "The Heap allows to store data that has a longer lifetime than a single code block or function e.g. objects that need to be shared across multiple methods  All the memory in the JVM is mainly Heap, is massive compared with the Stack and there is only one Heap shared across all the threads and a number of Stacks (each thread has its own Stack)",
            "title": "Heap"
        },
        {
            "location": "/jvm/#variables",
            "text": "How variable are store in Java:   objects are stored physically on the Heap  variables are reference to objects  local variables are stored on the Stack  primitive variables resides entirely on the Stack   int age = 30\nString name = \"niqdev\"\n\nStack          Heap\n[  name* ] --> [ \"niqdev\" ]\n[  age   ]  In Java, variable can  only  be passed by values i.e. a new variable is added on the Stack  Passing by reference is  not  possible, but don't confuse that when objects are passed into methods,  the  reference  of the object is passed  by value  final  keyword means that the value can only be assigned once (can be assigned later), but we can change the value referenced by the object. Lack of const correctness in Java:  const  unfortunately is a reserved keyword and cannot be used, but allows to freeze the whole state of the object internally:  mutable states are bad!  Example  final Customer c = new Customer(\"a\")\n// or\nfinal Customer c\nc = new Customer(\"a\")\n\n// this is NOT allowed - compiler error\nc = new Customer(\"b\")\n// but this yes\nc.setName(\"b\")",
            "title": "Variables"
        },
        {
            "location": "/jvm/#string",
            "text": "As a general rule of thumb, all objects are stored on the Heap and only references are stored in the Stack. In reality the JVM for optimization maybe store some objects also in the Stack, but this is not visibile. Strings are immutable and if \"short\" are stored in a pool on the Heap to be reused.  When you create a new string with quotes e.g.  \"hello\" , the JVM creates and retrieve the string from a constant pool. To create a new object every time use the  new String(\"hello\") . To force a lookup in the pool use  intern() . Use  ==  to compare the reference (address in memory)  String s1 = \"hello\";\nString s2 = \"hello\";\nString s3 = new String(\"hello\");\nString s4 = new String(\"hello\").intern();\n\n// true - 2 strings created using quotes refer to the same object\nSystem.out.println(s1 == s2);\n// false - if you create a string using new operator\n// it's not part of the constant pool, so objects are different\nSystem.out.println(s1 == s3);\n// true - if you call intern() method Java adds current string to the string pool\n// and 2 string become the same object\nSystem.out.println(s1 == s4);",
            "title": "String"
        },
        {
            "location": "/jvm/#garbage-collection",
            "text": "Garbage collection  (invented in lisp around 1959) removes object no more referenced on the Heap from the Stack. Most of the objects don't live for long, if an object survives it is likely to live forever.  Mark and sweep  is the algorithm used:   instead of look for all the objects that must be removed, it looks for the object that need to be retained  all the threads in the application are paused (stop the world event)  follow all the references from the Stack and mark it as alive  full scan on all the Heap and wipe unmarked references  reorder contiguos memory in order to avoid fragmentation  it's faster if more objects need to be cleaned because it looks only for what need to be retained   Generational garbage collection  is a way to organize the Heap into 2 sections to try to avoid freeze the application while garbage collecting the whole Heap:   young generation  is small, so it requires only few fraction of seconds to be scanned  old generation  also called  Tenured , isn't scanned usually, only if needed i.e. when it's full and requires few seconds  young generation is divided in  Eden ,  Survivor0  and  Survivor1  new objects are created in the Eden  when Eden gets full, objects are moved in the Survivor space and moved amongs the two alternatively to be compacted  memory is reserved in the Heap for S0 and S1 even if not used  after an object survived 8 generations (default), movement and compaction between Survivor GC, then is stored in the old generation   VM can change the number of generations based on the amount of memory available    class variables (static variables) are stored as part of the class object associated with that class and stored in the Permanent Generation (PermGen) prior Java 8 or in the MetaSpace     Run  jvisualvm  and add  Visual GC  plugin  Any object on the Heap which cannot be reached through a reference from the Stack is  eligible for garbage collection .  Memory leak  are objects that are not free on the Heap and continue to consume memory after a program finish. Memory leaks are difficult to find and the JVM try to avoid them running the garbage collector.  Soft leak  happens when an object is referenced on the Stack even thought it will never be used again  You can not clear memory, with  Runtime.getRuntime().gc()  or  System.gc()  you can only suggest JVM to run garbage collection, but there is no guarentee. In general, you should never invoke  gc()  directly. While it is running the application is temporarily suspended and it will pause all the threads.  finalize()  is invoked when an object is garbage collected, but there is absolutely no guarentee if and when it will happen.\nIs useful to check for example memory leak, as warning, if some resources were not closed properly",
            "title": "Garbage collection"
        },
        {
            "location": "/jvm/#permgen-and-metaspace",
            "text": "Permanent Generation (Heap memory) since Java 6 contains objects that will never garbage collected:   string pool is in PermGen  class metadata are stored in PermGen   If the PermGen run out of space the only solution is to increase the size of memory, otherwise the app will crash. In Java 7 String Pool was moved in the old memory and therefore string can be garbage collected now. In Java 8 MetaSpace replaced PermGen as separeted memory allocated, which is not part of the Heap anymore and is the total available memory",
            "title": "PermGen and Metaspace"
        },
        {
            "location": "/jvm/#tuning",
            "text": "# set the maximum heap size to 512 MB\n-Xmx512m\n# set the starting heap size to 150 MB\n-Xms150m\n# set the size of the PermGen to 256 MB (PermGen was removed in Java 8)\n-XX:MaxPermSize=256M\n# set the size of the young generation to 256 MB\n# young generation by default is 1/3 of the max heap size (suggestion between 1/2-1/4)\n-Xmn256m  Oracle JVM has 3 types of garbage collector, it doesn't matter how many threads, the application will be paused anyway:  # serial uses a single thread to perform gc\n-XX:+UseSerailGC\n# parallel performs gc on young generation in parallel\n-XX:+UseParallelGC\n# mostly concurrent, closest to real time gc pausing only when marking\n# try to minimize the \"stop of the world\"\n-XX:+UseConcMarkSweepGC\n-XX:+UseG1GC  Debugging  # print to console when a garbage collection takes place\n-verbose:gc\n# creates a heap dump file if app crash\n-XX:HeapDumpOnOutOfMemory\n# to find default garbage collector on the machine\n-XX:+PrintCommandLineFlags  Parameters are case sensitive",
            "title": "Tuning"
        },
        {
            "location": "/jvm/#weak-and-soft-references",
            "text": "References from the Stack to the Heap   Strong  references are always marked as alive (default)  Soft  references are eligible for garbage collection only if the JVM run out of memory  Weak  references are always eligible for garbage collection and is up to the JVM to retain it or not   WeakReference<T>  and  SoftReference<T>  are useful for caching scenario, when a reference on the Heap is GC, then the variable in the stack became  null . In a WeakHashMap for example, the reference from the stack to the map in the Heap is strong, but the references to key/value pair are eligible for GC, in that case both key and value are removed",
            "title": "Weak and Soft references"
        },
        {
            "location": "/jvm/#perfomance",
            "text": "Resources   HdrHistogram  Understanding JIT compiler  Understanding Java JIT Compilation with JITWatch  Coordinated Omission  Java Flight Recorder  Safepoints in HotSpot JVM  Java Mission Control (JMC) and Flight Recorder (JFR)  JMH  and  sbt-jmh  What is JMX?  and  Jolokia   Latency  describes the amount of time that it takes for an observed process to be completed  Throughput  defines the observed rate at which a process is completed  A  bottleneck  refers to the slowest part of the system  A common mistake is to rely on  averages  to measure the performance of a system because it is a lossy summary statistic.\nA  percentile  is a measurement indicating the value below which a given percentage of observations in a group of observations fall  Benchmarks  are a black-box kind of measurement. Benchmarks assess a whole system's performance by submitting various kinds of load as input and measuring latency and throughput as system outputs  A  profiler  enables white-box testing to help you identify bottlenecks by capturing the execution time and resource consumption of each part of your\nprogram. Most profilers instrument the code under observation, either at compile time or runtime, to inject counters and profiling components. This instrumentation imposes a runtime cost that degrades system throughput and latency  The  coordinated omission problem  happen we you measure the time required to process a command without taking into account the time the command had to wait to be processed  Java Flight Recorder (JFR)  is a tool for collecting, diagnosing, and profiling data about a running Java application. It is integrated into the Java Virtual Machine and causes almost no performance overhead and is able to access data outside of  JVM safepoints . Safepoints are necessary\nto coordinate global JVM activities, including stop-the-world garbage collection  Java Mission Control (JMC)  allows to connect to a running Java application via JMX and capture runtime information from the Flight Recorder (JFR), executing commands via JMX or displaying reports from JFR sessions  The  Just-In-Time (JIT)  compiler is a component of the Java Runtime Environment that improves the performance of Java applications at run time. Java programs consists of classes, which contain platform neutral bytecode that can be interpreted by a JVM on many different computer architectures.\nAt run time, the JVM loads the class files, determines the semantics of each individual bytecode, and performs the appropriate computation.\nThe additional processor and memory usage during interpretation means that a Java application performs more slowly than a native application.\nThe JIT compiler helps improve the performance of Java programs by compiling bytecode into native machine code at run time",
            "title": "Perfomance"
        },
        {
            "location": "/jvm/#scala",
            "text": "# compiles\nscalac miscellaneous/scala-example.scala -d miscellaneous/\n\n# lists compiler phases\nscalac -Xshow-phases\n\n# prints for-comprehensions desugared\nscalac -Xprint:parse miscellaneous/scala-example.scala -d miscellaneous/\n\n# prints all phases\nscalac -Xprint:all miscellaneous/scala-example.scala -d miscellaneous/\n\n# shows bytecode\njavap -v miscellaneous/Example$.class",
            "title": "Scala"
        },
        {
            "location": "/scala/",
            "text": "Scala\n\n\nResources\n\n\nFundamentals\n\n\n\n\nProgramming in Scala\n (2016)(3rd) by Martin Odersky, Lex Spoon, and Bill Venners (Book)\n\n\nScala High Performance Programming\n (2016) by Vincent Theron, Michael Diamant (Book)\n\n\n\n\nAkka in Action\n (2016) by Raymond Roestenburg, Rob Bakker, and Rob Williams (Book)\n\n\n\n\n\n\nTour of Scala\n\n\n\n\nTwitter Scala School\n\n\nScala Puzzlers\n\n\nS-99: Ninety-Nine Scala Problems\n\n\nScala Exercises\n\n\nScala for Project Euler\n\n\nScala Collections\n\n\nScala Collections performance characteristics\n\n\nThe Neophyte's Guide to Scala\n\n\nCheat Codes for Contravariance and Covariance\n\n\nCovariance and contravariance in Scala\n\n\nThe Scala Type System: Parameterized Types and Variances\n\n\n\n\nFP readings\n\n\n\n\nFunctional Programming for Mortals with Scalaz\n (2018) by Sam Halliday (Book)\n\n\nFunctional Programming in Scala\n (2014) by Paul Chiusano and Runar Bjarnason (Book)\n\n\nFunctional Programming, Simplified\n (2017) by Alvin Alexander (Book)\n\n\nScala with Cats\n (Book)\n\n\nThe Type Astronaut's Guide to Shapeless\n (Book)\n\n\nWhy Functional Programming Matters\n (Paper)\n\n\nThe Essence of the Iterator Pattern\n (Paper)\n\n\nApplicative programming with effects\n (Paper)\n\n\nStack Safety for Free\n (Paper)\n\n\nStackless Scala With Free Monads\n (Paper)\n\n\nType Classes as Objects and Implicits\n (paper)\n\n\nCats Infographic\n\n\n\n\nFP resources\n\n\n\n\nFunctional Programming Basics\n\n\nFunctional Programming For The Rest of Us\n\n\nThe Downfall of Imperative Programming\n\n\nParallelism and concurrency need different tools\n\n\nScala's Types of Types\n\n\nAlgebraic Data Types in Scala\n\n\nWhat the Heck are Algebraic Data Types?\n\n\nMore on Sealed Traits in Scala\n\n\nGeneralized type constraints in Scala (without a PhD)\n\n\nFirst steps with monads in Scala\n\n\nFunctors, Applicatives, And Monads In Pictures\n\n\nDemystifying the Monad in Scala\n\n\nCategory Theory for Programmers\n\n\nStackless Scala\n\n\nOverview of free monad in cats\n\n\nScalaFP: Firsthand With Scala-Cats\n\n\nScala Cats library for dummies\n\n\nAn IO monad for cats\n\n\nRethinking MonadError\n\n\nShapeless for Mortals\n (2015) by Sam Halliday (Talk)\n\n\nFree monads - what? and why?\n\n\nFree Monad examples\n\n\n\n\nTypeclass\n\n\n\n\nType classes in Scala\n\n\nReturning the \"Current\" Type in Scala\n\n\nTypeclass 101: ad hoc polymorphism in scala\n\n\nAll you don't need to know about Typeclasses\n\n\nTypeclasses 101\n\n\nScala/Haskell: A simple example of type classes\n\n\nA Small Example of the Typeclass Pattern in Scala\n\n\n\n\nPatterns\n\n\n\n\nThe Aux Pattern\n\n\nCake pattern\n\n\nMagnet pattern\n\n\n\n\nOther FP languages\n\n\n\n\nA practical introduction to functional programming\n\n\nLearn You a Haskell for Great Good!\n\n\nA Quick Tour of Haskell Syntax\n\n\nOCaml taste\n\n\n\n\nBlogs\n\n\n\n\nTypelevel\n\n\nScalac\n\n\nSignify\n\n\nScala Times\n\n\nSoftwareMill\n\n\n\n\nFAQs\n\n\n\n\nScala FAQs\n\n\n\n\nWhat is the Scala hierarchy?\n\n\n\n\nWhat is the difference between by-name and by-value parameters?\n\n\nA \nby-value\n parameter is evaluated before the method is invoked e.g. \n(x: Int)\n while a \nby-name\n parameter is not evaluated before the method is invoked, but each time the parameter is referenced inside the method e.g. \n(x: => Int)\n\n\nWhat are the differences between def val var lazy?\n\n\n\n\ndef\n defines a method\n\n\nval\n defines a fixed value, it is immmutable and eagerly initialized\n\n\nvar\n defines a variable reference, it is mutable and it should be avoided\n\n\nlazy\n only initialised when required and as late as possible (deferred evaluation), default is strict and is not recomputed like by-name parameters\n\n\n\n\nWhat are Nothing Nil None Empty Null null Unit?\n\n\n\n\nNothing\n is a trait that is the bottom subtype of every subtype of \nAny\n\n\nNil\n is an empty list that is defined as a \nList[Nothing]\n\n\nNone\n is an empty option that is defined as a \nOption[Nothing]\n\n\nNull\n is a trait and is the bottom type similiar to \nNothing\n but only for \nAnyRef\n not \nAnyVal\n\n\nnull\n is an instance of the \nNull\n trait\n\n\nUnit\n is a subtype of \nAnyVal\n, it's only value is \n()\n and it is not represented by any object in the underlying runtime system. A method with return type \nUnit\n is analogous to a Java method which is declared \nvoid\n\n\n\n\nWhat is the uniform access principal?\n\n\nThe uniform access principle states that variables, precomputed properties and parameterless functions should be accessed using the same syntax. Therefore not betraying whether they are implemented through storage or through computation. Scala supports this principle by not allowing parentheses to be placed at call sites of parameterless functions. A parameterless function definition \ndef\n can be changed to a \nval\n or vice versa, without affecting client code\n\n\nWhat referentially transparent means?\n\n\nAn expression \ne\n is \nreferentially transparent\n if, for all programs \np\n, all occurrences of \ne\n in \np\n can be replaced by the result of evaluating \ne\n without affecting the meaning of \np\n\n\nWhat is a pure function?\n\n\nA function \nf\n is \npure\n if the expression \nf(x)\n is referentially transparent for all referentially transparent \nx\n. Hence a pure function is \nmodular\n and \ncomposable\n\n\nWhat is a higher-order function?\n\n\nA \nhigher-order function\n is a function that takes other functions as arguments or returns a function as result\n\n\nWhat is recursive function?\n\n\nA \nrecursive function\n is a function which calls itself. With \nhead recursion\n, the recursive call is not the last instruction in the function.\n\n\nA \ntail recursive function\n is a special case of recursion in which the last instruction executed in the method is the recursive call. As long as the recursive call is in tail position, Scala detects and compiles it to the same sort of bytecode as would be emitted for a while loop\n\n\ndef factorial(n: Int): Int = {\n  @tailrec\n  def loop(index: Int, result: Int): Int = index match {\n    case i if i == 0 => loop(1, 1 * result)\n    case i if i < n => loop(i + 1, i * result)\n    case i => i * result\n  }\n  loop(0, 1)\n}\n\n\n\n\nWhat is a function literal?\n\n\nFunction literal\n is a synonyms for \nanonymous function\n. Because functions are just ordinary Scala objects, we say that they are \nfirst-class values\n. A function literal is syntactic sugar for an object with a method called apply\n\n\nval lessThan0 = (a: Int, b: Int) => a < b\nval lessThan1 = (a, b) => a < b\nval lessThan2 = new Function2[Int, Int, Boolean] {\n  override def apply(a: Int, b: Int): Boolean = a < b\n}\n\n\n\n\nWhat is a variadic function?\n\n\nA \nvariadic function\n accepts zero or more arguments. It provides a little syntactic sugar for creating and passing a Seq of elements explicitly. The special \n_*\n type annotation allows to pass a Seq to a variadic method\n\n\nsealed trait MyList[+A]\ncase object MyNil extends MyList[Nothing]\ncase class MyCons[+A](head: A, tail: MyList[A]) extends MyList[A]\n\nobject MyList {\n  def apply[A](list: A*): MyList[A] =\n    if (list.isEmpty) MyNil\n    else MyCons(list.head, apply(list.tail: _*))\n}\n\n// usage\nMyList(1, 2, 3, 4, 5)\n\n\n\n\nWhat is a value class?\n\n\nThe \nAnyVal\n class can be used to define a \nvalue class\n, which is optimized at compile time to avoid the allocation of an instance\n\n\nfinal case class Price(value: BigDecimal) extends AnyVal {\n  def lowerThan(p: Price): Boolean = this.value < p.value\n}\n\n\n\n\nWhat is autoboxing?\n\n\nThe JVM defines primitive types (\nboolean\n, \nbyte\n, \nchar\n, \nfloat\n, \nint\n, \nlong\n, \nshort\n and \ndouble\n) that are \nstack-allocated\n rather than \nheap-allocated\n. When a generic type is introduced, for example, \nscala.collection.immutable.List\n, the JVM references an object equivalent, instead of a primitive type. For example, an instantiated list of integers would be heap-allocated objects rather than integer primitives. The process of converting a primitive to its object equivalent is called \nboxing\n, and the reverse process is called \nunboxing\n. Boxing is a relevant concern for performance-sensitive programming because boxing involves heap allocation. In performance-sensitive code that performs numerical computations, the cost of \nboxing and unboxing\n can can create significant performance slowdowns\n\n\nWhat is the specialized annotation?\n\n\nSpecialization\n with \n@specialized\n annotation, refers to the compile-time process of generating duplicate versions of a generic trait or class that refer directly to a primitive type instead of the associated object wrapper. At runtime, the compiler-generated version of the generic class (or, as it is commonly referred to, the specialized version of the class) is instantiated. This process eliminates the runtime cost of boxing primitives, which means that you can define generic abstractions while retaining the performance of a handwritten, specialized implementation although it has some \nquirks\n\n\nWhat is the switch annotation?\n\n\nIn scenarios involving simple pattern match statements that directly match a value, using \n@switch\n annotation provides a warning at compile time if the switch can't be compiled to a tableswitch or lookupswitch which procides better performance, because it results in a branch table rather than a decision tree\n\n\nWhat is an Algebraic Data Type?\n\n\nIn type theory, regular data structures can be described in terms of sums, products and recursive types. This leads to an algebra for describing data structures (and so-called algebraic data types). Such data types are common in statically typed functional languages\n\n\nAn \nalgebraic data type\n (ADT) is just a data type defined by one or more data constructors, each of which may contain zero or more arguments. We say that the data type is the sum or union of its data constructors, and each data constructor is the product of its arguments, hence the name algebraic data type\n\n\nExample\n\n\n\n\nthese types represent a SUM type because Shape is a Circle OR a Rectangle\n\n\nCircle is a PRODUCT type because it has a radius\n\n\nRectangle is a PRODUCT type because it has a width AND a height\n\n\n\n\nsealed trait Shape\nfinal case class Circle(radius: Double) extends Shape\nfinal case class Rectangle(width: Double, height: Double) extends Shape\n\n\n\n\nHow for-comprehensions is desugared? (\ndocs\n)\n\n\n// (1) works because \"foreach\" is defined\nscala> for (i <- List(1, 2, 3)) println(i)\n1\n2\n3\n\n// (2) \"yield\" works because \"map\" is defined\nscala> for (i <- List(1, 2, 3)) yield i*2\nres2: List[Int] = List(2, 4, 6)\n\n// (3) \"if\" works because \"withFilter\" is defined\nscala> for (i <- List(1, 2, 3, 4); if i%2 == 0) yield i*2\nres3: List[Int] = List(4, 8)\n\n// (4) works because \"flatMap\" is defined\nscala> for (i <- List(1, 2, 3, 4); j <- List(3, 4, 5, 6); if i == j) yield i\nres4: List[Int] = List(3, 4)\n\n\n\n\nWhat is a Typeclass?\n\n\nA Typeclass is a programming pattern that allow to extend existing libraries with new functionality, without using traditional inheritance and without altering the original library source code using a combination of ad-hoc polymorphism, parametric polymorphism (type parameters) and implicits\n\n\nWhat is a Monoid?\n\n\nA Monoid is an algebraic type with 2 laws, a binary operation over that type, satisfying \nassociativity\n and an \nidentity\n element\n\n\n\n\nassociative e.g \na + (b + c) == (a + b) + c\n\n\nidentity e.g. for sum is 0, for product is 1, for string is \"\"\n\n\n\n\ntrait Monoid[A] {\n  // associativity\n  // op(op(x, y), z) == op(x, op(y, z))\n  def op(x: A, y: A): A\n\n  // identity\n  // op(x, zero) == op(zero, x) == x\n  def zero: A\n}\n\n// example\nval stringMonoid = new Monoid[String] {\n  override def op(x: String, y: String): String = x + y\n  override def zero: String = \"\"\n}\n\n\n\n\nMonoids have an intimate connection with lists and arguments of the same type, it doesn't matter if we choose \nfoldLeft\n or \nfoldRight\n when folding with a monoid because the laws of associativity and identity hold, hence this allows parallel computation\n\n\nThe real power of monoids comes from the fact that they compose, this means, for example, that if types A and B are monoids, then the tuple type (A, B) is also a monoid (called their product)\n\n\nscala> List(\"first\", \"second\", \"third\").foldLeft(stringMonoid.zero)(stringMonoid.op)\nscala> List(\"first\", \"second\", \"third\").foldRight(stringMonoid.zero)(stringMonoid.op)\nres: String = firstsecondthird\n\n\n\n\nWhat is a Semigroup?\n\n\nA Semigroup is just the \ncombine\n part of a Monoid. While many semigroups are also monoids, there are some data types for which we cannot define an empty element e.g. non-empty sequences and positive integers\n\n\ntrait Semigroup[A] {\n  // or op\n  def combine(x: A, y: A): A\n}\n\ntrait Monoid[A] extends Semigroup[A] {\n  // or zero\n  def empty: A\n}\n\n\n\n\nWhat is a Functor?\n\n\nInformally, a Functor is anything with a \nmap\n method\n\n\n// F is a higher-order type constructor or a higher-kinded type\ntrait Functor[F[_]] {\n  def map[A, B](fa: F[A])(f: A => B): F[B]\n}\n\n\n\n\nWhat is a Monad?\n\n\nInformally, a Monad is anything with a constructor and a flatMap method. A Monad is a mechanism for \nsequencing computations\n, all monads are functors but the opposite is not true.\n\n\nA Monad is an implementation of one of the minimal sets of monadic combinators, satisfying the laws of associativity and identity\n\n\n\n\nunit and flatMap\n\n\nunit and compose\n\n\nunit, map and join\n\n\n\n\nwhere the above are defined\n\n\ndef unit[A](a: => A): F[A]\ndef map[A, B](ma: F[A])(f: A => B): F[B]\ndef flatMap[A, B](ma: F[A])(f: A => F[B]): F[B]\ndef compose[A, B, C](f: A => F[B], g: B => F[C]): A => F[C]\ndef join[A](mma: F[F[A]]): F[A]\n\n// Identity: compose(unit, f) = f = compose(f, unit)\n// Associativity: compose(compose(f, g), h) = compose(f, compose(g, h))\n\n\n\n\nA Monad provide a context for introducing and binding variables and performing variable substitution\n\n\nobject Monad {\n  case class Id[A](value: A) {\n    def map[B](f: A => B): Id[B] =\n      Id(f(value))\n    def flatMap[B](f: A => Id[B]): Id[B] =\n      f(value)\n  }\n\n  val idMonad: Monad[Id] = new Monad[Id] {\n    override def unit[A](a: => A): Id[A] =\n      Id(a)\n\n    override def flatMap[A, B](ma: Id[A])(f: A => Id[B]): Id[B] =\n      ma.flatMap(f)\n  }\n}\n\nMonad.Id(\"hello \").flatMap(a => Monad.Id(\"world\").flatMap(b => Monad.Id(a + b)))\n\nfor {\n  a <- Monad.Id(\"hello \")\n  b <- Monad.Id(\"world\")\n} yield a + b\n\nres: Monad.Id[String] = Id(hello world)\n\n\n\n\nWhat is a Semigroupal?\n\n\nA Semigroupal is a type class that allows to combine contexts. In contrast to flatMap, which imposes a strict order, Semigroupal parameters are independent of one another, which gives more freedom with respect to monads\n\n\ntrait Semigroupal[F[_]] {\n  def product[A, B](fa: F[A], fb: F[B]): F[(A, B)]\n}\n\n\n\n\nWhat is an Applicative?\n\n\n\n\nall applicatives are functors\n\n\nall applicatives are a semigroupal\n\n\nall monads are applicative functors, viceversa is not true\n\n\n\n\n// cats definition\ntrait Apply[F[_]] extends Semigroupal[F] with Functor[F] {\n  def ap[A, B](ff: F[A => B])(fa: F[A]): F[B]\n  def product[A, B](fa: F[A], fb: F[B]): F[(A, B)] =\n    ap(map(fa)(a => (b: B) => (a, b)))(fb)\n}\n\ntrait Applicative[F[_]] extends Apply[F] {\n  def pure[A](a: A): F[A]\n}\n\n// red book definition\ntrait Applicative[F[_]] extends Functor[F] {\n  // primitive combinators\n  def map2[A, B, C](fa: F[A], fb: F[B])(f: (A, B) => C): F[C]\n  def unit[A](a: => A): F[A]\n\n  // derived combinators\n  def map[A, B](fa: F[A])(f: A => B): F[B] =\n    map2(fa, unit(()))((a, _) => f(a))\n  def traverse[A, B](as: List[A])(f: A => F[B]): F[List[B]] =\n    as.foldRight(unit(List[B]()))((a, fbs) => map2(f(a), fbs)(_ :: _))\n}",
            "title": "Scala"
        },
        {
            "location": "/scala/#scala",
            "text": "",
            "title": "Scala"
        },
        {
            "location": "/scala/#resources",
            "text": "Fundamentals   Programming in Scala  (2016)(3rd) by Martin Odersky, Lex Spoon, and Bill Venners (Book)  Scala High Performance Programming  (2016) by Vincent Theron, Michael Diamant (Book)   Akka in Action  (2016) by Raymond Roestenburg, Rob Bakker, and Rob Williams (Book)    Tour of Scala   Twitter Scala School  Scala Puzzlers  S-99: Ninety-Nine Scala Problems  Scala Exercises  Scala for Project Euler  Scala Collections  Scala Collections performance characteristics  The Neophyte's Guide to Scala  Cheat Codes for Contravariance and Covariance  Covariance and contravariance in Scala  The Scala Type System: Parameterized Types and Variances   FP readings   Functional Programming for Mortals with Scalaz  (2018) by Sam Halliday (Book)  Functional Programming in Scala  (2014) by Paul Chiusano and Runar Bjarnason (Book)  Functional Programming, Simplified  (2017) by Alvin Alexander (Book)  Scala with Cats  (Book)  The Type Astronaut's Guide to Shapeless  (Book)  Why Functional Programming Matters  (Paper)  The Essence of the Iterator Pattern  (Paper)  Applicative programming with effects  (Paper)  Stack Safety for Free  (Paper)  Stackless Scala With Free Monads  (Paper)  Type Classes as Objects and Implicits  (paper)  Cats Infographic   FP resources   Functional Programming Basics  Functional Programming For The Rest of Us  The Downfall of Imperative Programming  Parallelism and concurrency need different tools  Scala's Types of Types  Algebraic Data Types in Scala  What the Heck are Algebraic Data Types?  More on Sealed Traits in Scala  Generalized type constraints in Scala (without a PhD)  First steps with monads in Scala  Functors, Applicatives, And Monads In Pictures  Demystifying the Monad in Scala  Category Theory for Programmers  Stackless Scala  Overview of free monad in cats  ScalaFP: Firsthand With Scala-Cats  Scala Cats library for dummies  An IO monad for cats  Rethinking MonadError  Shapeless for Mortals  (2015) by Sam Halliday (Talk)  Free monads - what? and why?  Free Monad examples   Typeclass   Type classes in Scala  Returning the \"Current\" Type in Scala  Typeclass 101: ad hoc polymorphism in scala  All you don't need to know about Typeclasses  Typeclasses 101  Scala/Haskell: A simple example of type classes  A Small Example of the Typeclass Pattern in Scala   Patterns   The Aux Pattern  Cake pattern  Magnet pattern   Other FP languages   A practical introduction to functional programming  Learn You a Haskell for Great Good!  A Quick Tour of Haskell Syntax  OCaml taste   Blogs   Typelevel  Scalac  Signify  Scala Times  SoftwareMill",
            "title": "Resources"
        },
        {
            "location": "/scala/#faqs",
            "text": "Scala FAQs   What is the Scala hierarchy?   What is the difference between by-name and by-value parameters?  A  by-value  parameter is evaluated before the method is invoked e.g.  (x: Int)  while a  by-name  parameter is not evaluated before the method is invoked, but each time the parameter is referenced inside the method e.g.  (x: => Int)  What are the differences between def val var lazy?   def  defines a method  val  defines a fixed value, it is immmutable and eagerly initialized  var  defines a variable reference, it is mutable and it should be avoided  lazy  only initialised when required and as late as possible (deferred evaluation), default is strict and is not recomputed like by-name parameters   What are Nothing Nil None Empty Null null Unit?   Nothing  is a trait that is the bottom subtype of every subtype of  Any  Nil  is an empty list that is defined as a  List[Nothing]  None  is an empty option that is defined as a  Option[Nothing]  Null  is a trait and is the bottom type similiar to  Nothing  but only for  AnyRef  not  AnyVal  null  is an instance of the  Null  trait  Unit  is a subtype of  AnyVal , it's only value is  ()  and it is not represented by any object in the underlying runtime system. A method with return type  Unit  is analogous to a Java method which is declared  void   What is the uniform access principal?  The uniform access principle states that variables, precomputed properties and parameterless functions should be accessed using the same syntax. Therefore not betraying whether they are implemented through storage or through computation. Scala supports this principle by not allowing parentheses to be placed at call sites of parameterless functions. A parameterless function definition  def  can be changed to a  val  or vice versa, without affecting client code  What referentially transparent means?  An expression  e  is  referentially transparent  if, for all programs  p , all occurrences of  e  in  p  can be replaced by the result of evaluating  e  without affecting the meaning of  p  What is a pure function?  A function  f  is  pure  if the expression  f(x)  is referentially transparent for all referentially transparent  x . Hence a pure function is  modular  and  composable  What is a higher-order function?  A  higher-order function  is a function that takes other functions as arguments or returns a function as result  What is recursive function?  A  recursive function  is a function which calls itself. With  head recursion , the recursive call is not the last instruction in the function.  A  tail recursive function  is a special case of recursion in which the last instruction executed in the method is the recursive call. As long as the recursive call is in tail position, Scala detects and compiles it to the same sort of bytecode as would be emitted for a while loop  def factorial(n: Int): Int = {\n  @tailrec\n  def loop(index: Int, result: Int): Int = index match {\n    case i if i == 0 => loop(1, 1 * result)\n    case i if i < n => loop(i + 1, i * result)\n    case i => i * result\n  }\n  loop(0, 1)\n}  What is a function literal?  Function literal  is a synonyms for  anonymous function . Because functions are just ordinary Scala objects, we say that they are  first-class values . A function literal is syntactic sugar for an object with a method called apply  val lessThan0 = (a: Int, b: Int) => a < b\nval lessThan1 = (a, b) => a < b\nval lessThan2 = new Function2[Int, Int, Boolean] {\n  override def apply(a: Int, b: Int): Boolean = a < b\n}  What is a variadic function?  A  variadic function  accepts zero or more arguments. It provides a little syntactic sugar for creating and passing a Seq of elements explicitly. The special  _*  type annotation allows to pass a Seq to a variadic method  sealed trait MyList[+A]\ncase object MyNil extends MyList[Nothing]\ncase class MyCons[+A](head: A, tail: MyList[A]) extends MyList[A]\n\nobject MyList {\n  def apply[A](list: A*): MyList[A] =\n    if (list.isEmpty) MyNil\n    else MyCons(list.head, apply(list.tail: _*))\n}\n\n// usage\nMyList(1, 2, 3, 4, 5)  What is a value class?  The  AnyVal  class can be used to define a  value class , which is optimized at compile time to avoid the allocation of an instance  final case class Price(value: BigDecimal) extends AnyVal {\n  def lowerThan(p: Price): Boolean = this.value < p.value\n}  What is autoboxing?  The JVM defines primitive types ( boolean ,  byte ,  char ,  float ,  int ,  long ,  short  and  double ) that are  stack-allocated  rather than  heap-allocated . When a generic type is introduced, for example,  scala.collection.immutable.List , the JVM references an object equivalent, instead of a primitive type. For example, an instantiated list of integers would be heap-allocated objects rather than integer primitives. The process of converting a primitive to its object equivalent is called  boxing , and the reverse process is called  unboxing . Boxing is a relevant concern for performance-sensitive programming because boxing involves heap allocation. In performance-sensitive code that performs numerical computations, the cost of  boxing and unboxing  can can create significant performance slowdowns  What is the specialized annotation?  Specialization  with  @specialized  annotation, refers to the compile-time process of generating duplicate versions of a generic trait or class that refer directly to a primitive type instead of the associated object wrapper. At runtime, the compiler-generated version of the generic class (or, as it is commonly referred to, the specialized version of the class) is instantiated. This process eliminates the runtime cost of boxing primitives, which means that you can define generic abstractions while retaining the performance of a handwritten, specialized implementation although it has some  quirks  What is the switch annotation?  In scenarios involving simple pattern match statements that directly match a value, using  @switch  annotation provides a warning at compile time if the switch can't be compiled to a tableswitch or lookupswitch which procides better performance, because it results in a branch table rather than a decision tree  What is an Algebraic Data Type?  In type theory, regular data structures can be described in terms of sums, products and recursive types. This leads to an algebra for describing data structures (and so-called algebraic data types). Such data types are common in statically typed functional languages  An  algebraic data type  (ADT) is just a data type defined by one or more data constructors, each of which may contain zero or more arguments. We say that the data type is the sum or union of its data constructors, and each data constructor is the product of its arguments, hence the name algebraic data type  Example   these types represent a SUM type because Shape is a Circle OR a Rectangle  Circle is a PRODUCT type because it has a radius  Rectangle is a PRODUCT type because it has a width AND a height   sealed trait Shape\nfinal case class Circle(radius: Double) extends Shape\nfinal case class Rectangle(width: Double, height: Double) extends Shape  How for-comprehensions is desugared? ( docs )  // (1) works because \"foreach\" is defined\nscala> for (i <- List(1, 2, 3)) println(i)\n1\n2\n3\n\n// (2) \"yield\" works because \"map\" is defined\nscala> for (i <- List(1, 2, 3)) yield i*2\nres2: List[Int] = List(2, 4, 6)\n\n// (3) \"if\" works because \"withFilter\" is defined\nscala> for (i <- List(1, 2, 3, 4); if i%2 == 0) yield i*2\nres3: List[Int] = List(4, 8)\n\n// (4) works because \"flatMap\" is defined\nscala> for (i <- List(1, 2, 3, 4); j <- List(3, 4, 5, 6); if i == j) yield i\nres4: List[Int] = List(3, 4)  What is a Typeclass?  A Typeclass is a programming pattern that allow to extend existing libraries with new functionality, without using traditional inheritance and without altering the original library source code using a combination of ad-hoc polymorphism, parametric polymorphism (type parameters) and implicits  What is a Monoid?  A Monoid is an algebraic type with 2 laws, a binary operation over that type, satisfying  associativity  and an  identity  element   associative e.g  a + (b + c) == (a + b) + c  identity e.g. for sum is 0, for product is 1, for string is \"\"   trait Monoid[A] {\n  // associativity\n  // op(op(x, y), z) == op(x, op(y, z))\n  def op(x: A, y: A): A\n\n  // identity\n  // op(x, zero) == op(zero, x) == x\n  def zero: A\n}\n\n// example\nval stringMonoid = new Monoid[String] {\n  override def op(x: String, y: String): String = x + y\n  override def zero: String = \"\"\n}  Monoids have an intimate connection with lists and arguments of the same type, it doesn't matter if we choose  foldLeft  or  foldRight  when folding with a monoid because the laws of associativity and identity hold, hence this allows parallel computation  The real power of monoids comes from the fact that they compose, this means, for example, that if types A and B are monoids, then the tuple type (A, B) is also a monoid (called their product)  scala> List(\"first\", \"second\", \"third\").foldLeft(stringMonoid.zero)(stringMonoid.op)\nscala> List(\"first\", \"second\", \"third\").foldRight(stringMonoid.zero)(stringMonoid.op)\nres: String = firstsecondthird  What is a Semigroup?  A Semigroup is just the  combine  part of a Monoid. While many semigroups are also monoids, there are some data types for which we cannot define an empty element e.g. non-empty sequences and positive integers  trait Semigroup[A] {\n  // or op\n  def combine(x: A, y: A): A\n}\n\ntrait Monoid[A] extends Semigroup[A] {\n  // or zero\n  def empty: A\n}  What is a Functor?  Informally, a Functor is anything with a  map  method  // F is a higher-order type constructor or a higher-kinded type\ntrait Functor[F[_]] {\n  def map[A, B](fa: F[A])(f: A => B): F[B]\n}  What is a Monad?  Informally, a Monad is anything with a constructor and a flatMap method. A Monad is a mechanism for  sequencing computations , all monads are functors but the opposite is not true.  A Monad is an implementation of one of the minimal sets of monadic combinators, satisfying the laws of associativity and identity   unit and flatMap  unit and compose  unit, map and join   where the above are defined  def unit[A](a: => A): F[A]\ndef map[A, B](ma: F[A])(f: A => B): F[B]\ndef flatMap[A, B](ma: F[A])(f: A => F[B]): F[B]\ndef compose[A, B, C](f: A => F[B], g: B => F[C]): A => F[C]\ndef join[A](mma: F[F[A]]): F[A]\n\n// Identity: compose(unit, f) = f = compose(f, unit)\n// Associativity: compose(compose(f, g), h) = compose(f, compose(g, h))  A Monad provide a context for introducing and binding variables and performing variable substitution  object Monad {\n  case class Id[A](value: A) {\n    def map[B](f: A => B): Id[B] =\n      Id(f(value))\n    def flatMap[B](f: A => Id[B]): Id[B] =\n      f(value)\n  }\n\n  val idMonad: Monad[Id] = new Monad[Id] {\n    override def unit[A](a: => A): Id[A] =\n      Id(a)\n\n    override def flatMap[A, B](ma: Id[A])(f: A => Id[B]): Id[B] =\n      ma.flatMap(f)\n  }\n}\n\nMonad.Id(\"hello \").flatMap(a => Monad.Id(\"world\").flatMap(b => Monad.Id(a + b)))\n\nfor {\n  a <- Monad.Id(\"hello \")\n  b <- Monad.Id(\"world\")\n} yield a + b\n\nres: Monad.Id[String] = Id(hello world)  What is a Semigroupal?  A Semigroupal is a type class that allows to combine contexts. In contrast to flatMap, which imposes a strict order, Semigroupal parameters are independent of one another, which gives more freedom with respect to monads  trait Semigroupal[F[_]] {\n  def product[A, B](fa: F[A], fb: F[B]): F[(A, B)]\n}  What is an Applicative?   all applicatives are functors  all applicatives are a semigroupal  all monads are applicative functors, viceversa is not true   // cats definition\ntrait Apply[F[_]] extends Semigroupal[F] with Functor[F] {\n  def ap[A, B](ff: F[A => B])(fa: F[A]): F[B]\n  def product[A, B](fa: F[A], fb: F[B]): F[(A, B)] =\n    ap(map(fa)(a => (b: B) => (a, b)))(fb)\n}\n\ntrait Applicative[F[_]] extends Apply[F] {\n  def pure[A](a: A): F[A]\n}\n\n// red book definition\ntrait Applicative[F[_]] extends Functor[F] {\n  // primitive combinators\n  def map2[A, B, C](fa: F[A], fb: F[B])(f: (A, B) => C): F[C]\n  def unit[A](a: => A): F[A]\n\n  // derived combinators\n  def map[A, B](fa: F[A])(f: A => B): F[B] =\n    map2(fa, unit(()))((a, _) => f(a))\n  def traverse[A, B](as: List[A])(f: A => F[B]): F[List[B]] =\n    as.foldRight(unit(List[B]()))((a, fbs) => map2(f(a), fbs)(_ :: _))\n}",
            "title": "FAQs"
        },
        {
            "location": "/readings/",
            "text": "Papers\n\n\n\n\n\n\nThe Google File System\n\n\n\n\n\n\nMapReduce: Simplified Data Processing on Large Clusters\n\n\n\n\n\n\nRaft: In Search of an Understandable Consensus Algorithm\n\n\n\n\n\n\nPaxos Made Simple\n\n\n\n\n\n\nZab: A simple totally ordered broadcast protocol\n\n\n\n\n\n\nThe Chubby lock service for loosely-coupled distributed systems\n\n\n\n\n\n\nSpanner: Google's Globally-Distributed Database\n\n\n\n\n\n\nDynamo: Amazon\u2019s Highly Available Key-value Store\n\n\n\n\n\n\nHyperLogLog in Practice\n\n\n\n\n\n\nDapper, a Large-Scale Distributed Systems Tracing Infrastructure\n\n\n\n\n\n\nLarge-scale cluster management at Google with Borg\n\n\n\n\n\n\nLinearizability: A Correctness Condition for Concurrent Objects\n\n\n\n\n\n\nHarvest, Yield, and Scalable Tolerant Systems\n\n\n\n\n\n\nLife beyond Distributed Transactions\n\n\n\n\n\n\nThe \u03d5 Accrual Failure Detector\n\n\n\n\n\n\nConflict-free Replicated Data Types\n\n\n\n\n\n\nFLP - Impossibility of Distributed Consensus with One Faulty Process\n\n\n\n\n\n\nSEDA: An Architecture for Well-Conditioned, Scalable Internet Services\n\n\n\n\n\n\nMerkle Hash Tree based Techniques for Data Integrity of Outsourced Data\n\n\n\n\n\n\nWhat Every Programmer Should Know About Memory\n\n\n\n\n\n\nStatistically Rigorous Java Performance Evaluation\n\n\n\n\n\n\nPregel: A System for Large-Scale Graph Processing\n\n\n\n\n\n\nHashed and Hierarchical Timing Wheels\n\n\n\n\n\n\n\n\nArticles and Resources\n\n\n\n\n\n\nThere is No Now\n\n\n\n\n\n\nJepsen\n\n\n\n\n\n\nThe world beyond batch: Streaming 101\n\n\n\n\n\n\nExploring CQRS and Event Sourcing\n\n\n\n\n\n\nVersioning in an Event Sourced System\n\n\n\n\n\n\nCAP Theorem\n\n\n\n\n\n\nBrewer's CAP Theorem\n\n\n\n\n\n\nCAP Twelve Years Later: How the \"Rules\" Have Changed\n\n\n\n\n\n\n\nPlease stop calling databases CP or AP\n\n\n\n\n\n\n\n\nBooks\n\n\n\n\n\n\nDesigning Data-Intensive Applications\n (2017) by Martin Kleppmann\n\n\n\n\n\n\nAddison-Wesley Professional Computing Series",
            "title": "Readings"
        },
        {
            "location": "/readings/#papers",
            "text": "The Google File System    MapReduce: Simplified Data Processing on Large Clusters    Raft: In Search of an Understandable Consensus Algorithm    Paxos Made Simple    Zab: A simple totally ordered broadcast protocol    The Chubby lock service for loosely-coupled distributed systems    Spanner: Google's Globally-Distributed Database    Dynamo: Amazon\u2019s Highly Available Key-value Store    HyperLogLog in Practice    Dapper, a Large-Scale Distributed Systems Tracing Infrastructure    Large-scale cluster management at Google with Borg    Linearizability: A Correctness Condition for Concurrent Objects    Harvest, Yield, and Scalable Tolerant Systems    Life beyond Distributed Transactions    The \u03d5 Accrual Failure Detector    Conflict-free Replicated Data Types    FLP - Impossibility of Distributed Consensus with One Faulty Process    SEDA: An Architecture for Well-Conditioned, Scalable Internet Services    Merkle Hash Tree based Techniques for Data Integrity of Outsourced Data    What Every Programmer Should Know About Memory    Statistically Rigorous Java Performance Evaluation    Pregel: A System for Large-Scale Graph Processing    Hashed and Hierarchical Timing Wheels",
            "title": "Papers"
        },
        {
            "location": "/readings/#articles-and-resources",
            "text": "There is No Now    Jepsen    The world beyond batch: Streaming 101    Exploring CQRS and Event Sourcing    Versioning in an Event Sourced System    CAP Theorem    Brewer's CAP Theorem    CAP Twelve Years Later: How the \"Rules\" Have Changed    Please stop calling databases CP or AP",
            "title": "Articles and Resources"
        },
        {
            "location": "/readings/#books",
            "text": "Designing Data-Intensive Applications  (2017) by Martin Kleppmann    Addison-Wesley Professional Computing Series",
            "title": "Books"
        },
        {
            "location": "/other/",
            "text": "Online tools\n\n\n\n\nRegular Expressions\n\n\nRegular Expression Visualization Site\n\n\nCurrent Millis\n\n\nJSFiddle\n\n\nScalaFiddle\n\n\nJSON Formatter\n\n\nBeautify JavaScript or HTML\n\n\ndevdocs\n\n\n\n\n\n\nVagrant\n\n\n\n\nVagrant\n is a tool for building and managing virtual machine environments in a single workflow\n\n\n\n\nResources\n\n\n\n\nDocumentation\n\n\nVirtualBox\n\n\n\n\nSetup project creating a Vagrantfile\n\n\nvagrant init\n\n\n\n\nBoot and connect to the default virtual machine\n\n\nvagrant up\nvagrant status\nvagrant ssh\n\n\n\n\nUseful commands\n\n\n# shut down gracefully\nvagrant halt\n\n# reload (halt + up) + re-provision\nvagrant reload --provision\n\n# update box\nvagrant box update\nvagrant box list\n\n# delete virtual machine without prompt\nvagrant destory -f\n\n\n\n\n\n\nMkDocs\n\n\n\n\nMkDocs\n is a static site generator\n\n\n\n\nResources\n\n\n\n\nDocumentation\n\n\n\n\nInstall\n\n\npip install mkdocs\nsudo -H pip3 install mkdocs\n\n\n\n\nUseful commands\n\n\n# setup in current directory\nmkdocs new .\n\n# start dev server with hot reload @ http://127.0.0.1:8000\nmkdocs serve\n\n# build static site\nmkdocs build --clean\n\n# deploy to github\nmkdocs gh-deploy\n\n\n\n\n\n\nSDKMAN!\n\n\n\n\nSDKMAN!\n is a tool for managing parallel versions of multiple Software Development Kits on most Unix based systems\n\n\n\n\nResources\n\n\n\n\nDocumentation\n\n\n\n\nSetup\n\n\ncurl -s \"https://get.sdkman.io\" | bash\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\nsdk version\n\n\n\n\nGradle\n\n\n# setup\nsdk list gradle\nsdk install gradle 4.4.1\ngradle -version\n\n# create Gradle project\nmkdir -p PROJECT_NAME && cd $_\ngradle init --type java-library\n\n./gradlew clean build\n\n\n\n\nScala\n\n\n# setup sbt\nsdk list sbt\nsdk install sbt\nsbt sbtVersion\nsbt about\n\n# setup scala\nsdk list scala\nsdk install scala 2.11.8\nscala -version\n\n# sample project\nsbt new sbt/scala-seed.g8\n\n\n\n\n\n\nGiter8\n\n\n\n\nGiter8\n is a command line tool to generate files and directories from templates published on GitHub or any other git repository\n\n\n\n\nResources\n\n\n\n\nDocumentation\n\n\nTemplates\n\n\n\n\nSetup\n\n\n# install conscript\ncurl https://raw.githubusercontent.com/foundweekends/conscript/master/setup.sh | sh\nsource ~/.bashrc\n\n# install g8\ncs foundweekends/giter8\n\n\n\n\nExample\n\n\n# interactive\ng8 sbt/scala-seed.g8\n# non-interactive\ng8 sbt/scala-seed.g8 --name=my-new-website\n\n\n\n\n\n\nSnap\n\n\nResources\n\n\n\n\nDocumentation\n\n\n\n\nUseful commands\n\n\n# search\nsnap find gimp\n\n# info\nsnap info gimp\n\n# install\nsnap install gimp\n\n# list installed app\nsnap list\n\n\n\n\n\n\nPython\n\n\nResources\n\n\n\n\npip\n\n\nvirtualenv\n\n\nWhat is the difference between virtualenv | pyenv | virtualenvwrapper | venv ?\n\n\n\n\nSetup\n\n\n# search\napt-get update && apt-cache search python | grep python2\n\n# setup python\napt-get install -y python2.7\napt-get install -y python3\n\n# install pip + setuptools\ncurl https://bootstrap.pypa.io/get-pip.py | python2.7 -\ncurl https://bootstrap.pypa.io/get-pip.py | python3 -\napt install -y python-pip\napt install -y python3-pip\n\n# upgrade pip\npip install -U pip\n\n# install virtualenv globally \npip install virtualenv\n\n\n\n\nvirtualenv\n\n\n# create virtualenv\nvirtualenv venv\nvirtualenv -p python3 venv\nvirtualenv -p $(which python3) venv\n\n# activate virtualenv\nsource venv/bin/activate\n\n# verify virtualenv\nwhich python\npython --version\n\n# deactivate virtualenv\ndeactivate\n\n\n\n\npip\n\n\n# search package\npip search <package>\n\n# install new package\npip install <package>\n\n# update requirements with new packages\npip freeze > requirements.txt\n\n# install all requirements\npip install -r requirements.txt\n\n\n\n\nOther\n\n\n# generate rc file\npylint --generate-rcfile > .pylintrc\n\n# create module\ntouch app/{__init__,main}.py\n\n\n\n\n\n\nGit\n\n\nResources\n\n\n\n\ngit - the simple guide\n\n\ngit notes (1)\n\n\ngit notes (2)\n\n\n\n\n\n\nMercurial\n\n\nResources\n\n\n\n\nA Guide to Branching in Mercurial\n\n\n\n\n# changes since last commit\nhg st\n\n# verify current branch\nhg branch\n\n# lists all branches\nhg branches\n\n# checkout default branch\nhg up default\n\n# pull latest changes\nhg pull -u\n\n# create new branch\nhg branch \"branch-name\"\n\n# track new file\nhg add .\n\n# track new files and untrack removed files\nhg addremove\n\n# commit all tracked files\nhg commit -m \"my-comment\"\n\n# commit specific files\nhg commit FILE_1 FILE_2 -m \"my-comment\"\n\n# commit and track/untrack files (i.e. addremove)\nhg commit -A -m \"my-comment-with-addremove\"\n\n# rename last unpushed commit message\nhg commit -m \"bad-commit-message\"\nhg commit --amend -m \"good-commit-message\"\n\n# discard untracked files\nhg purge\n\n# discard uncommitted local changes\nhg up -C\n\n# discard local uncommitted branch\nhg strip \"branch-name\"\n\n# push commits in all branches\nhg push\n\n# push commits in current branch\nhg push -b .\n\n# create a new branch and push commits in current branch (first time only)\nhg push -b . --new-branch\n\n# lists unpushed commit\nhg outgoing\n\n# change head to specific revision\nhg up -r 12345\n\n# merge default branch on current branch\nhg up default\nhg pull -u\nhg status\nhg up CURRENT-BRANCH\nhg merge default\nhg diff\n\n# remove all resolved conflicts\nrm **/*.orig\n\n# list stashes\nhg shelve --list\n\n# stash\nhg shelve -n \"my-draft\"\n\n# unstash\nhg unshelve \"my-draft\"\n\n# revert/undo last unpushed commit\nhg strip -r -1 --keep\nhg strip --keep --rev .\n\n# solve conflicts manually and then mark it as merged\nhg resolve -m FILE-NAME\n\n# lists commits\nhg log\nhg ls\n\n# pretty log\nhg history --graph --limit 10",
            "title": "Other"
        },
        {
            "location": "/other/#online-tools",
            "text": "Regular Expressions  Regular Expression Visualization Site  Current Millis  JSFiddle  ScalaFiddle  JSON Formatter  Beautify JavaScript or HTML  devdocs",
            "title": "Online tools"
        },
        {
            "location": "/other/#vagrant",
            "text": "Vagrant  is a tool for building and managing virtual machine environments in a single workflow   Resources   Documentation  VirtualBox   Setup project creating a Vagrantfile  vagrant init  Boot and connect to the default virtual machine  vagrant up\nvagrant status\nvagrant ssh  Useful commands  # shut down gracefully\nvagrant halt\n\n# reload (halt + up) + re-provision\nvagrant reload --provision\n\n# update box\nvagrant box update\nvagrant box list\n\n# delete virtual machine without prompt\nvagrant destory -f",
            "title": "Vagrant"
        },
        {
            "location": "/other/#mkdocs",
            "text": "MkDocs  is a static site generator   Resources   Documentation   Install  pip install mkdocs\nsudo -H pip3 install mkdocs  Useful commands  # setup in current directory\nmkdocs new .\n\n# start dev server with hot reload @ http://127.0.0.1:8000\nmkdocs serve\n\n# build static site\nmkdocs build --clean\n\n# deploy to github\nmkdocs gh-deploy",
            "title": "MkDocs"
        },
        {
            "location": "/other/#sdkman",
            "text": "SDKMAN!  is a tool for managing parallel versions of multiple Software Development Kits on most Unix based systems   Resources   Documentation   Setup  curl -s \"https://get.sdkman.io\" | bash\nsource \"$HOME/.sdkman/bin/sdkman-init.sh\"\nsdk version  Gradle  # setup\nsdk list gradle\nsdk install gradle 4.4.1\ngradle -version\n\n# create Gradle project\nmkdir -p PROJECT_NAME && cd $_\ngradle init --type java-library\n\n./gradlew clean build  Scala  # setup sbt\nsdk list sbt\nsdk install sbt\nsbt sbtVersion\nsbt about\n\n# setup scala\nsdk list scala\nsdk install scala 2.11.8\nscala -version\n\n# sample project\nsbt new sbt/scala-seed.g8",
            "title": "SDKMAN!"
        },
        {
            "location": "/other/#giter8",
            "text": "Giter8  is a command line tool to generate files and directories from templates published on GitHub or any other git repository   Resources   Documentation  Templates   Setup  # install conscript\ncurl https://raw.githubusercontent.com/foundweekends/conscript/master/setup.sh | sh\nsource ~/.bashrc\n\n# install g8\ncs foundweekends/giter8  Example  # interactive\ng8 sbt/scala-seed.g8\n# non-interactive\ng8 sbt/scala-seed.g8 --name=my-new-website",
            "title": "Giter8"
        },
        {
            "location": "/other/#snap",
            "text": "Resources   Documentation   Useful commands  # search\nsnap find gimp\n\n# info\nsnap info gimp\n\n# install\nsnap install gimp\n\n# list installed app\nsnap list",
            "title": "Snap"
        },
        {
            "location": "/other/#python",
            "text": "Resources   pip  virtualenv  What is the difference between virtualenv | pyenv | virtualenvwrapper | venv ?   Setup  # search\napt-get update && apt-cache search python | grep python2\n\n# setup python\napt-get install -y python2.7\napt-get install -y python3\n\n# install pip + setuptools\ncurl https://bootstrap.pypa.io/get-pip.py | python2.7 -\ncurl https://bootstrap.pypa.io/get-pip.py | python3 -\napt install -y python-pip\napt install -y python3-pip\n\n# upgrade pip\npip install -U pip\n\n# install virtualenv globally \npip install virtualenv  virtualenv  # create virtualenv\nvirtualenv venv\nvirtualenv -p python3 venv\nvirtualenv -p $(which python3) venv\n\n# activate virtualenv\nsource venv/bin/activate\n\n# verify virtualenv\nwhich python\npython --version\n\n# deactivate virtualenv\ndeactivate  pip  # search package\npip search <package>\n\n# install new package\npip install <package>\n\n# update requirements with new packages\npip freeze > requirements.txt\n\n# install all requirements\npip install -r requirements.txt  Other  # generate rc file\npylint --generate-rcfile > .pylintrc\n\n# create module\ntouch app/{__init__,main}.py",
            "title": "Python"
        },
        {
            "location": "/other/#git",
            "text": "Resources   git - the simple guide  git notes (1)  git notes (2)",
            "title": "Git"
        },
        {
            "location": "/other/#mercurial",
            "text": "Resources   A Guide to Branching in Mercurial   # changes since last commit\nhg st\n\n# verify current branch\nhg branch\n\n# lists all branches\nhg branches\n\n# checkout default branch\nhg up default\n\n# pull latest changes\nhg pull -u\n\n# create new branch\nhg branch \"branch-name\"\n\n# track new file\nhg add .\n\n# track new files and untrack removed files\nhg addremove\n\n# commit all tracked files\nhg commit -m \"my-comment\"\n\n# commit specific files\nhg commit FILE_1 FILE_2 -m \"my-comment\"\n\n# commit and track/untrack files (i.e. addremove)\nhg commit -A -m \"my-comment-with-addremove\"\n\n# rename last unpushed commit message\nhg commit -m \"bad-commit-message\"\nhg commit --amend -m \"good-commit-message\"\n\n# discard untracked files\nhg purge\n\n# discard uncommitted local changes\nhg up -C\n\n# discard local uncommitted branch\nhg strip \"branch-name\"\n\n# push commits in all branches\nhg push\n\n# push commits in current branch\nhg push -b .\n\n# create a new branch and push commits in current branch (first time only)\nhg push -b . --new-branch\n\n# lists unpushed commit\nhg outgoing\n\n# change head to specific revision\nhg up -r 12345\n\n# merge default branch on current branch\nhg up default\nhg pull -u\nhg status\nhg up CURRENT-BRANCH\nhg merge default\nhg diff\n\n# remove all resolved conflicts\nrm **/*.orig\n\n# list stashes\nhg shelve --list\n\n# stash\nhg shelve -n \"my-draft\"\n\n# unstash\nhg unshelve \"my-draft\"\n\n# revert/undo last unpushed commit\nhg strip -r -1 --keep\nhg strip --keep --rev .\n\n# solve conflicts manually and then mark it as merged\nhg resolve -m FILE-NAME\n\n# lists commits\nhg log\nhg ls\n\n# pretty log\nhg history --graph --limit 10",
            "title": "Mercurial"
        }
    ]
}